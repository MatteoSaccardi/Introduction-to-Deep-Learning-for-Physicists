{"cells":[{"cell_type":"markdown","metadata":{"id":"zPTNy_tHUrrI"},"source":["<h1><center> MLP MNIST Training </center></h1>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N_fSAUpAZj9E"},"source":["### Cristiano De Nobili - My Contacts\n","For any questions or doubts you can find my contacts here:\n","\n","<p align=\"center\">\n","\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Instagram_logo_2016.svg/2048px-Instagram_logo_2016.svg.png\" width=\"20\">](https://www.instagram.com/denocris/?hl=it)\n","[<img src=\"https://1.bp.blogspot.com/-Rwqcet_SHbk/T8_acMUmlmI/AAAAAAAAGgw/KD_fx__8Q4w/s1600/Twitter+bird.png\" width=\"30\">](https://twitter.com/denocris) \n","[<img src=\"https://loghi-famosi.com/wp-content/uploads/2020/04/Linkedin-Simbolo.png\" width=\"40\">](https://www.linkedin.com/in/cristiano-de-nobili/)     \n","\n","</p>\n","\n","or here (https://denocris.com).\n","\n","### Useful Links\n","\n","All notebooks can be found [here!](https://drive.google.com/drive/folders/1i3cNfzWZTNXfvkFVVIIDXjRDdSa9L9Dv?usp=sharing)\n","\n","Introductory slides [here!](https://www.canva.com/design/DAEa5hLfuWg/-L2EFFfZLVuiDkmg4KiKkQ/view?utm_content=DAEa5hLfuWg&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink)\n","\n","Collection of references: [here!](https://denocris.notion.site/Deep-Learning-References-0c5af2dc5c8d40baba19f1328d596fff)\n"]},{"cell_type":"markdown","metadata":{"id":"rychVYL5J_-O"},"source":["### Tools used \n","\n","PyTorch, NumPy, sklearn. To perform grid search we will use the Optuna optimization library.\n","\n","### Notebook Outline\n","\n","* information theory background;\n","* write and train a MLP on MNIST using hold-out validation;\n","* implement grid search with Optuna;\n","* learn k-fold cross-validation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p1WdF9dANBDu"},"source":["## Packages and GPU Settings"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"M99bQapvPh-e"},"outputs":[],"source":["%%capture\n","!pip install -q tensorboard\n","!pip install -q optuna "]},{"cell_type":"markdown","metadata":{},"source":["**capture** avoids the printing of installation package garbage and warnings\\\n","**optuna** is a package to optimize hyperparameters (grid search)\\\n","**tensorboard** is a package to visualize data"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchsummary in /home/matteo/.local/lib/python3.8/site-packages (1.5.1)\n"]}],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"53Fac-e3rNUw"},"outputs":[],"source":["import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import transforms\n","from torchsummary import summary\n","\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import random\n","import optuna"]},{"cell_type":"markdown","metadata":{"id":"SG9z4Rz_HCPS"},"source":["GPU info e device set up"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1641155537841,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"MjmlP7ZObnce","outputId":"3859c89e-e643-4c5a-929c-cc2b6a7bf65a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: nvidia-smi: comando non trovato\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"Ozj-Ar_7b_55"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["use_cuda = torch.cuda.is_available()\n","print(use_cuda)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"]},{"cell_type":"markdown","metadata":{"id":"uOyMhZgTHinw"},"source":["## Information Theory Background \n"]},{"cell_type":"markdown","metadata":{"id":"a_h2KilRH39t"},"source":["Basic notions of Information Theory are essential for the understanding of many machine/deep learning mechanisms as we will see in a while."]},{"cell_type":"markdown","metadata":{"id":"CtrqdhkMID39"},"source":["### Self-Information\n","\n","Every event takes with it an amount of self-information. The idea behind self-information goes as follows\n","\n","* if an event always occurs, we associate it with a smaller amount of information. It will not suprise us!\n","* On the other side, a rare event is associated with a huge amount of information. It will suprise us!\n","\n","I am not surprise to see the sunrise every morning (likely event). Instead,  I would be really suprised if tomorrow the Sun will not rise (unlikely event). This amount of surprise or self-information of the event $x$ is quantified by\n","\n","$$I(x) = - \\log p(x),$$\n","\n","where $p(x)$ is the probability of the event $x$. If $p(x)=1$, then self-info is zero. A rare event instead has a huge surpise factor.\n","\n","### Shannon Entropy \n","\n","Here the original paper by Claude Shannon (1948): [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf).\n","\n","In terms of self-info, Shannon Entropy is the average self-information (expected value) over all possible values of X.\n","The entropy for a probability $p(x)$ distribution is\n","\n","$$ S = - \\sum_i p(x_i) \\log p(x_i),$$\n","\n","where we assume we know the probability $p$ for each outcome $i$ and $ \\sum_i p(x_i)=1$. If we use $log_2$ for our calculation we can interpret entropy as *the minimum number of bits it would take us to encode our information*.\n","\n","For continous variables, we can use the integral form\n","\n","$$ S = - \\int  p(x) \\log p(x) \\, dx,$$\n","\n","where now $p(x)$ is taking the role of a probability density function (PDF). Take in mind that a broad probability density has higher entropy than a narrowed one (think about Gaussian distribution vs delta Dirac, which has $S=0$).\n","\n","In both discrete and continous formulation, we are computing the expectation (i.e. average) of the negative log-probability (i.e. self-info) which is the theoretical minimum encoding size of the information from the event $x$. The same formula is usually written as\n","\n","$$S = \\mathbb E _{\\, x \\sim p} \\left[ -\\log p(x) \\right],$$\n","\n","where $x \\sim p$ means that we calculate the expectation with the probability distribution $p$."]},{"cell_type":"markdown","metadata":{"id":"8BEow_oxYSje"},"source":["Let's give an example! \n","\n","<!---\n","  REMIND to change open with uc\n"," https://drive.google.com/open?id=1Y52T3Z4dwRU4Rq5L5bEVYh0d3kU0aVB8\n","--->\n","\n","  <center>  <img src=https://drive.google.com/uc?id=1GaAeK8xIZCVDRb-oHQNUzRuoOprFh1eS \" width=\"700\">  </center> "]},{"cell_type":"markdown","metadata":{"id":"K5E4znWyYWDT"},"source":["Let us say we have to pass a message about what drink Cristiano will order during an event. In general, Cristiano loves [Midori Sour](https://drizly.com/midori-sour/r-b972d5282bec6fe8) , Daiquiri, Spritz and Wine.\n","\n","On Monday, Cristiano loves to listen Jazz and the probability distribution of his choice is: \n","\n","$$P(\\text Midori ) =  P(\\text Daiquiri ) = P(\\text Spritz ) = P(\\text Wine ) = 0.25,$$\n","\n","while the corresponding entropy\n","\n","$$S = - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} = 2$$\n","\n","On Wednesday, he usually meets with some friends after work: \n","\n","$$P(\\text Midori ) = 0.125,\\;  P(\\text Daiquiri ) =0.125,\\;  P(\\text Spritz ) = 0.5,\\; P(\\text  Wine ) = 0.25,$$\n","\n","while the corresponding entropy\n","\n","$$S = - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} = 1.75$$\n","\n","\n","On Thursday, he often goes to an event where cocktail attire dress code is required\n","\n","$$P(\\text Midori ) = 0.95,\\;  P(\\text Daiquiri ) =0.02,\\;  P(\\text Spritz ) = 0.018,\\; P(\\text  Wine ) = 0.012,$$\n","\n","and the corresponding entropy\n","\n","$$S = - 0.95 \\log 0.95 - 0.02 \\log 0.02 - 0.018 \\log 0.018 - 0.012 \\log 0.012 = 0.364$$"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":734,"status":"ok","timestamp":1599731724144,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"KReLhZDvICt1","outputId":"3d38218f-ef94-4b5b-cbd3-1b05a3645d78"},"outputs":[{"name":"stdout","output_type":"stream","text":["On Monday, high entropy:  2.0\n","On Wednesday, medium entropy:  1.75\n","On Thursday, low entropy:  0.36407300467232967\n"]}],"source":["# On Monday, all drinks have equal probability to be chose\n","entropy_1 = -0.25*np.log2(0.25)-0.25*np.log2(0.25)-0.25*np.log2(0.25)-0.25*np.log2(0.25)\n","print('On Monday, high entropy: ', entropy_1)\n","\n","# On Wednesday, some are more probable than others\n","entropy_2 = -0.5*np.log2(0.5)-0.25*np.log2(0.25)-0.125*np.log2(0.125)-0.125*np.log2(0.125)\n","print('On Wednesday, medium entropy: ', entropy_2)\n","\n","# On Thursday, one drink is by far the most probable\n","entropy_3 = -0.95*np.log2(0.95)-0.02*np.log2(0.02)-0.018*np.log2(0.018)-0.012*np.log2(0.012)\n","print('On Thursday, low entropy: ', entropy_3)"]},{"cell_type":"markdown","metadata":{"id":"CZh-58l2Qwvu"},"source":["If entropy is high (encoding size $log_2 p(x)$ is big on average), it means we have many message types with small and almost equal probabilities. Hence, every time a new message arrives, you would expect a different type than previous messages. You may see it as a disorder or uncertainty or unpredictability.\n","\n","On the contrary, when a message has much smaller probability than other messages, it appears as a surprise because on average you would expect other more frequently sent message types. Moreover, a rare message type has more information than more frequent message types because it eliminates a lot of other probabilities and tells us more specific information.\n","\n","In the drink scenario, by sending “Wine” on thursday which happens 1.2% of the times, we are reducing the uncertainty by 98.8% of the probability distribution (“Midori, Daiquiri, Spritz”) provided we had no information before. If we were sending “Midori” (95%) instead, we would be reducing the uncertainty by 5% only.\n","\n","If the entropy is high (ex: fair coin), the average encoding size is significant which means each message tends to have more (specific) information. Again, this is why high entropy is associated with disorder, uncertainty, surprise, unpredictability, amount of information. The more random a message is, the more information will be gained from decoding the message.\n","\n","Low entropy (ex: sunrise) means that most of the times we are receiving the more predictable information which means less disorder, less uncertainty, less surprise, more predictability and less (specific) information. This is the Thursday case."]},{"cell_type":"markdown","metadata":{"id":"Fu6ZmWp5UGDa"},"source":["### Cross Entropy\n","\n","It is one of the most used loss functions in machine learning, as it is a measure of the \"distance\" (even though it is not symmetric) between twod distributions.\n","\n","Suppose to have two distributions, the true one $p(x)$ and the estimated $q(x)$. In the language of neural networks, $p(x)$ would be the grond truth (labels in one hot-encoding) and $q(x)$ the outcome of the net, i.e. the one that your machine learning algorithm is trying to match. Cross entropy is a mathematical tool for comparing two probability distributions $p(x)$ and $q(x)$ and it is expressed by the formula \n","\n","$$ H (p,q) = - \\int p(x) \\log q(x)\\,dx.$$\n","\n","If $\\log$ is in base $2$, then cross entropy measures the number of bits you will need encoding symbols from $p$ using the wrong distribution $q$. Subtracting to cross entropy the entropy of $p$, you are counting the cost in terms of bits of using the wrong distribution $q$ (this somehow will be KL-divergence). An important property of cross-entropy is that its value is minimum (and corresponds to $H(p)$) when $p(x)=q(x)$. That is the reason why, during training we want to minimize its value. This corresponds to force the estimated distribution $q(x)$ to be close to the true one $p(x)$."]},{"cell_type":"markdown","metadata":{"id":"60tieif3uc-l"},"source":["### Kullback-Leibler Divergence\n","\n","KL-divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution $h$ we add into the game our approximating distribution $g$. Then we look at the difference of the log values for each\n","\n","$$D_{KL}(h || g) =  \\sum_i h(x_i) (\\log h(x) - \\log g(x)) = \\sum_i h(x_i) \\log \\frac{h(x)}{g(x)}$$ \n","\n","from which\n","\n","$$H(h, g) =  H(h) + D_{KL}(h || g).$$ \n","\n","\n","KL-divergence is the expectation of the log-difference between the probability of data in the original distribution $h$ with the approximating distribution $g$. Again, if we think in terms of $\\log_2$ we can interpret this as how many bits of information we expect to lose when we choose an approximation $g$ of our original ditribution $h$. \n","\n","In the variational autoencoder loss function, the KL-divergence is used to force the distribution of latent variables $q(z | x)$ to be a normal distribution $n(z)$ so that we can sample latent variables from the normal distribution. As such, the KL-divergence is included in the loss function to improve the similarity between the distribution of latent variables and the normal distribution. More about **KL** can be found [here](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) and about **cross-entropy** [here](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)."]},{"cell_type":"markdown","metadata":{"id":"0hbIeSgD_YUw"},"source":["## Training, Validation and Test Sets\n","\n","* **parameters:** weights and biases of a DNN which transform the input before applying the activation function. Each layer has its own set of parameters. The parameters are adjusted through backpropagation to minimize the loss function or in other words are learned during the training process;\n","\n","* **hyperparameters:** unlike parameters, their values are not adapted by the learning algorithm itself. They can be viewed as settings that can be used to control the behaviour of the algorithm. Examples are number of layers, type of architecture, batch size, learning rate, etc...;\n","\n","* **training set:** a set of examples used for learning. It affects parameters and in particular they are optimized according to this set;\n","\n","* **validation set:** a set of examples not seen by the model during training. It is like a mini-test set that provides feedback to the model during training on how well the current weights generalize beyond the training set. It does not impact or adjust weights directly, but providing information about overfitting it can indirectly impact weights if some regularization techniques are applied. In addition, validation set is widely used to tune hyperparameters;\n","\n","* **test set:** a set of examples used at the end of training and validation to assess the predictive power of your model."]},{"cell_type":"markdown","metadata":{"id":"MHSBvKR3QNTV"},"source":["## A Complete Training of a MLP Model on MNIST Dataset\n","\n","In this section, we want to show a complete train of a MLP network . We will show the following pipelines\n","\n","* Import, preprocess and properly normalize training and test data;\n","* Build up the MLP and initialize its weights;\n","* Train and evaluate the model using standard hold-out validation;\n","* Tune hyperparameters with Optuna (one of the many libraries to do GridSearch);\n","* Train and evaluate the model using K-Fold Cross-Validation."]},{"cell_type":"markdown","metadata":{"id":"ZBKPCzq6T781"},"source":["## Train MLP Model with hold-out validation"]},{"cell_type":"markdown","metadata":{"id":"9AE9tPDSt6KK"},"source":["### Import and Preprocess Data"]},{"cell_type":"markdown","metadata":{"id":"hD8f7nPP_zs0"},"source":["MNIST is a dataset for handwritten digit recognition.\n","\n","* The dataset is composed of 60,000 grayscale images\n","  * by default, the dataset is already split into a training set of 50,000 images, while the remaining 10,000 images make up the test/validation set\n","* Each image is composed of 28x28 pixel\n","* Only one digit is present in each image\n","  * thus, we will be classifying digits from 0 to 9 (10 classes)\n","* The digit is centered within the image\n","\n","From **torchvision** we import the MNIST dataset "]},{"cell_type":"code","execution_count":68,"metadata":{"id":"5_MazLV_qDmi"},"outputs":[],"source":["%%capture\n","#Download data\n","train_set = MNIST('./data', train=True, download=True)\n","test_set = MNIST('./data', train=False, download=True)"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1641155589533,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"4JYVj4sWrWv4","outputId":"4a1ca433-0f22-481c-bdc5-582f22bf830c"},"outputs":[{"data":{"text/plain":["(60000, 28, 28)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["train_set.data.numpy().shape"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/plain":["array([5, 0, 4, ..., 5, 6, 8])"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["train_set.targets.numpy()"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1641155590805,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"PYW3X_OjjrVt","outputId":"eb1c8b70-a145-4e03-a199-411dfc15df9c"},"outputs":[{"data":{"text/plain":["(28, 28)"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["# First image in dataset (from targets, we expect it to be 5)\n","img = train_set.data.numpy()[0]\n","img.shape"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1641155592180,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"rCuQNDuWoKSA","outputId":"e5e465d3-9cf4-4a00-fe18-cbc545e9e95a"},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7ff21a68ca30>"]},"execution_count":72,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.imshow(img, interpolation=\"nearest\", cmap=\"gray_r\")"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1641155596536,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"ykqVWzhgoE8t","outputId":"fe8d4dda-49ba-46a7-c96c-a7bdeb32b0dd"},"outputs":[{"data":{"text/plain":["(5, 28, 28)"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["imgs = train_set.data.numpy()[0:5]\n","imgs.shape"]},{"cell_type":"markdown","metadata":{"id":"VErobQDOsuie"},"source":["Applying a reshape we can view more than one sample"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1641155600584,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"sVUuKi_hoAvG","outputId":"ad93e7b0-6a60-4842-dc4d-0e4abf812959"},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7ff219df98b0>"]},"execution_count":74,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# use this trick\n","multi_img_reshaped = imgs.reshape(5*28, 28)\n","plt.imshow(multi_img_reshaped, cmap=\"gray\") #cmap=\"gray_r\" for black digits"]},{"cell_type":"markdown","metadata":{"id":"pzPVNUawtNt8"},"source":["Normally, digital pixels have values from $0$ to $255$. On the other side, neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. Therefore it is a good practice to normalize input to small values. We will apply the centering procedure, that consist of dividing first all pixel values by the highest value $255$. Then, we subtract the mean value and dividing by the standard deviation.\n","\n","As a first step, let us compute mean and standard deviation of MNIST training dataset:"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680,"status":"ok","timestamp":1641155607419,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"U7oqJm4LuTB7","outputId":"f4a3ff44-888c-434f-d51a-7137886a4fdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train mean and std: 0.130660  0.308108\n","Test mean and std: 0.132515  0.310480\n"]}],"source":["train_set_array = train_set.data.numpy() / 255.0\n","test_set_array = test_set.data.numpy() / 255.0\n","\n","print('Train mean and std: %f  %f' %(train_set_array.mean(), train_set_array.std()))\n","print('Test mean and std: %f  %f' %(test_set_array.mean(), test_set_array.std()))"]},{"cell_type":"markdown","metadata":{"id":"SF-lJXJctxKO"},"source":["Now we go on with three steps:\n","\n","* Set random seeds;\n","* Apply **transforms** ([Official Docs](https://pytorch.org/vision/stable/transforms.html)) which allows a set of common image transformations that can be composed;\n","* Use `DataLoader`, that combines a dataset and a sampler, and provides an iterable over the given dataset. It handles the dataset in mini-batch for Stochastic Gradient Descent."]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1641155635002,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"6_4rNQ74t9FF","outputId":"eb3312d5-00f9-409d-d4c1-b35dca548f02"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7ff21ceec510>"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["# set the seed: built-in python, numpy, and pytorch\n","seed = 172\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed) # works for all devices (CPU and GPU)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"fQj-ZUoUvjTC"},"outputs":[],"source":["batch_size = 16\n","\n","transform = transforms.Compose([transforms.ToTensor(), \n","                                        transforms.Normalize( (0.1307,), (0.3081,))])\n","\n","# DataLoader: combines a dataset and a sampler, and provides an iterable over the given dataset.\n","train_loader = DataLoader(MNIST(root = './data', train = True, transform = transform, download=True), batch_size=batch_size , shuffle=False)\n","# Splitting the test images (tot 10k) in valid and test set.\n","valid_loader_tmp, test_loader_tmp = random_split(MNIST(root = './data', train = False, transform = transform, download=True), [7000, 3000]) \n","\n","valid_loader = DataLoader(valid_loader_tmp, batch_size=batch_size , shuffle=False)\n","test_loader = DataLoader(test_loader_tmp, batch_size=batch_size , shuffle=False)"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1641155639747,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"N2FRkLfBA6yW","outputId":"2804aabb-89cc-4e56-da7f-612d3519e3f1"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# check for seed = 172\n","imgs_check = next(iter(valid_loader))\n","np.array_equal(imgs_check[1].numpy() , np.array([1, 4, 6, 5, 9, 7, 3, 4, 7, 0, 2, 1, 9, 7, 6, 4]))"]},{"cell_type":"markdown","metadata":{"id":"nxdgzVqr0Zgg"},"source":["**A good practice** is to normalize the test set using the training normalization parameters (mean and std).\n","\n","We want testing data points to represent real-world data that the network has never seen. If we take the mean and variance of the whole dataset we will be introducing future information into the training explanatory variables (i.e. the mean and variance).\n","\n","Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.\n","\n","To better understand imagine now that we have trained our model and we are on a production where new data keep coming for prediction. We might not get them in mass, but one by one such as in an API call. We do not have the mean and standard deviation of those new data. We only have the mean and std during the training process.  To sum up, the goal is to be as close as possible to real problems. Therefore, during training we should not use any knowledge we get from the test data."]},{"cell_type":"markdown","metadata":{"id":"5ryJmjq3amVy"},"source":["### MLP Model\n","\n","The idea is to introduce a few concepts, build up a pipeline and be as clear as possible. "]},{"cell_type":"code","execution_count":79,"metadata":{"id":"H6UteiKb27Fc"},"outputs":[],"source":["# write here the class for your MLP\n","class MLP(nn.Module):\n","    def __init__(self, dropout_rate = .2):\n","        super().__init__()\n","\n","\n","        self.flat = torch.nn.Flatten() # X comes in as a n x 1 x 28 x 28 -> we need n 784-size vectors (or, a n x 784 matrix). flatten does this\n","        self.layer1 = torch.nn.Linear(784, 64) # 784 = 28 * 28\n","        self.layer2 = torch.nn.Linear(64, 32)\n","        self.layer3 = torch.nn.Linear(32, 24)\n","        self.layer4 = torch.nn.Linear(24, 10)\n","        \n","        self.droput = nn.Dropout2d(p = dropout_rate)\n","\n","\n","    def forward(self, X): \n","        out = self.flat(X)\n","        out = self.layer1(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.layer2(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.layer3(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.droput(out)\n","        logits = self.layer4(out)\n","        # out = torch.nn.functional.softmax(out) -> this is wrong since we will use NegativeLogLikelihood Loss\n","        # softmax transforms a vector into a simplex (vector of positive nums summing to 1): \n","        # [1,2,3,4] -> [0.1, 0.2, 0.3, 0.4]\n","        # instead we need the logsoftmax\n","        #out = torch.nn.functional.log_softmax(out)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"UUEiwOtSqFx1"},"source":["### Weights Initialization"]},{"cell_type":"markdown","metadata":{"id":"dimObwP6bUgb"},"source":["We will now initialize weights (and biases) using Xavier initialization. If you are more interested in understanding its details, you can find here the original paper by [Xavier and Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n","\n","There are several ways to initialize parmeters. However, the most naively ones might cause vanishing gradient effects during training. Researchers found safer initializations. One of them is Xavier initialization.\n","It sets a layer’s weights to values chosen from a random uniform distribution that is bounded between\n","\n","\n","$$\\pm \\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}, $$\n","\n","where $n_i$ is the number of incoming network connections, or *fan-in*, to the layer, and $n_{i+1}$ is the number of outgoing network connections from that layer, also known as the *fan-out*. We are going to use it for weights initialization (`nn.init.xavier_uniform_(m.weight)`). For biases initialization instead we will adopt a slightly different approach, just to show you how you can play with initializations."]},{"cell_type":"code","execution_count":80,"metadata":{"id":"eUUdTKP7fhFo"},"outputs":[],"source":["def weight_init(m):\n","    torch.manual_seed(seed) \n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n","            bound = 1 / np.sqrt(fan_in)\n","            nn.init.uniform_(m.bias, -bound, bound)"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8581,"status":"ok","timestamp":1641155753358,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"22InO0WGtZu_","outputId":"ace35660-be6f-4c11-9e2a-c1794c23f65c"},"outputs":[{"data":{"text/plain":["MLP(\n","  (flat): Flatten(start_dim=1, end_dim=-1)\n","  (layer1): Linear(in_features=784, out_features=64, bias=True)\n","  (layer2): Linear(in_features=64, out_features=32, bias=True)\n","  (layer3): Linear(in_features=32, out_features=24, bias=True)\n","  (layer4): Linear(in_features=24, out_features=10, bias=True)\n","  (droput): Dropout2d(p=0.2, inplace=False)\n",")"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["model = MLP(dropout_rate = 0.2)\n","#lenet = MLP(dropout_rate = 0.000001)\n","\n","# apply weight init\n","model.apply(weight_init)\n","\n","# put the model on the device\n","model.to(device) \n"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1641155757937,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"74o3mkz0vfYR","outputId":"dd596361-8975-4525-ac4a-62af7a442597"},"outputs":[{"name":"stdout","output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Flatten: 1-1                           [-1, 784]                 --\n","├─Linear: 1-2                            [-1, 64]                  50,240\n","├─Linear: 1-3                            [-1, 32]                  2,080\n","├─Linear: 1-4                            [-1, 24]                  792\n","├─Dropout2d: 1-5                         [-1, 24]                  --\n","├─Linear: 1-6                            [-1, 10]                  250\n","==========================================================================================\n","Total params: 53,362\n","Trainable params: 53,362\n","Non-trainable params: 0\n","Total mult-adds (M): 0.05\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.20\n","Estimated Total Size (MB): 0.21\n","==========================================================================================\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Flatten: 1-1                           [-1, 784]                 --\n","├─Linear: 1-2                            [-1, 64]                  50,240\n","├─Linear: 1-3                            [-1, 32]                  2,080\n","├─Linear: 1-4                            [-1, 24]                  792\n","├─Dropout2d: 1-5                         [-1, 24]                  --\n","├─Linear: 1-6                            [-1, 10]                  250\n","==========================================================================================\n","Total params: 53,362\n","Trainable params: 53,362\n","Non-trainable params: 0\n","Total mult-adds (M): 0.05\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.20\n","Estimated Total Size (MB): 0.21\n","=========================================================================================="]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["# check that the model is working\n","summary(model, (1, 28, 28))"]},{"cell_type":"markdown","metadata":{"id":"cl5aSZSDdC0W"},"source":["To keep under control all steps"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1641155761218,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"vDDrKnst15wX","outputId":"2186c187-f9cd-43e3-820c-d935d4802530"},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[-0.0426, -0.0491,  0.0672,  ..., -0.0125,  0.0540,  0.0738],\n","        [-0.0727,  0.0535, -0.0265,  ..., -0.0688,  0.0169,  0.0542],\n","        [-0.0692,  0.0162,  0.0654,  ..., -0.0331, -0.0827,  0.0830],\n","        ...,\n","        [ 0.0176,  0.0817,  0.0369,  ...,  0.0798, -0.0513,  0.0467],\n","        [-0.0117, -0.0026,  0.0757,  ...,  0.0619,  0.0366,  0.0650],\n","        [-0.0025,  0.0541, -0.0683,  ..., -0.0203, -0.0608, -0.0005]],\n","       requires_grad=True)"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["torch.save(model.state_dict(), './weights_init')\n","\n","list(model.parameters())[0]"]},{"cell_type":"markdown","metadata":{"id":"ROpfooUCqNrT"},"source":["### Training and Evaluation\n","\n","Let us first define \n","\n","* a function that computes accuracy from logits (applying a softmax function);\n","\n","* a function that computes evaluation.\n","\n","Logits are the unnormalized final scores (predictions) of our model. We must apply softmax to it to get a probability distribution over our classes. If you remind, the LeNet() class returns logits\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1)"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["torch.argmax(torch.tensor([0.2,0.7,0.1])).long() #index of max"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.8000)"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["(torch.tensor([0,1,2,5,0]) == torch.tensor([0,1,2,5,9])).float().mean()"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1.6038e-28, 1.0000e+00, 3.6251e-34]])"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["softmax = nn.Softmax(dim=1)\n","softmax(torch.tensor([[25, 89, 12]]).float())"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"--ZryiNvxyth"},"outputs":[],"source":["def get_accuracy_from_logits(logits, labels):\n","    softmax = nn.Softmax(dim=1)\n","    argmax = torch.argmax(softmax(logits.float()), dim=1)\n","    pred_class = argmax.long()\n","    acc = (pred_class == labels.long()).float().mean()\n","    return acc\n","\n","def evaluate(net, crit, dataloader, device):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for data, labels in dataloader:\n","            data, labels = data.to(device), labels.to(device)\n","            logits = net(data)\n","            mean_loss += crit(logits, labels).item()\n","            mean_acc += get_accuracy_from_logits(logits, labels)\n","            count += 1\n","\n","    return mean_acc / count, mean_loss / count"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1641156325691,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"TnmsFxJppv1n","outputId":"1d0559af-909b-4d28-af8f-626e9394db1b"},"outputs":[{"data":{"text/plain":["3750"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["len(train_loader) # 3750*batch_size"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"data":{"text/plain":["[(0, 'a'), (1, 'b'), (2, 'c')]"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["list(enumerate(['a','b','c']))"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"1hotozlhni1N"},"outputs":[],"source":["# The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard.\n","writer = SummaryWriter() # it will output to ./runs/ directory by default\n","\n","def training(model, train_loader, valid_loader, optim, crit, epochs, device, model_save = True):\n","    # set up the model on training phase\n","    model.train()\n","    best_acc = 0\n","    tot_len = len(train_loader)\n","    for epoch in range(1, epochs + 1):\n","        for batch_idx, (data, labels) in enumerate(train_loader):\n","            # transfer data on the device\n","            data, labels = data.to(device), labels.to(device)\n","            #Clear gradients  \n","            # Since the backward() function accumulates gradients, and you do not want to mix up \n","            # gradients between different batches, you have to zero them out at the start of a new batch.\n","            optim.zero_grad() # same as model.zero_grad()\n","            logits = model(data)\n","            loss = crit(logits, labels)\n","            loss.backward()\n","            optim.step()\n","            if batch_idx % 200 == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx * len(data), len(train_loader.dataset),\n","                    100. * batch_idx / len(train_loader), loss.item()))\n","                writer.add_scalars('Loss', {'train': loss.item()}, batch_idx + tot_len*(epoch-1))\n","\n","            if batch_idx % 500 == 0:\n","                _, valid_loss = evaluate(model, crit, valid_loader, device)\n","                writer.add_scalars('Loss', {'valid': valid_loss}, batch_idx + tot_len*(epoch-1))\n","                # set again the model to train mode\n","                model.train()\n","    \n","        valid_acc, _ = evaluate(model, crit, valid_loader, device)\n","        #writer.add_scalar('Accuracy', valid_acc, tot_len + tot_len*(epoch-1))\n","        writer.add_scalars('Acc', {'valid': valid_acc}, tot_len + tot_len*(epoch-1))\n","\n","        if valid_acc > best_acc:\n","          print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, valid_acc))\n","          best_acc = valid_acc\n","          if model_save:\n","            torch.save(model.state_dict(), 'Models/lenet_{}.pt'.format(epoch))\n","\n","    return best_acc\n","    \n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, labels).item() # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"dcNjSHBbEiI0"},"outputs":[],"source":["#!rm -rf runs\n","#!rm -rf Models"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"4kENFWEdZKsh"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: impossibile creare la directory \"Models\": File già esistente\n"]}],"source":["epochs = 3\n","!mkdir Models\n"]},{"cell_type":"markdown","metadata":{"id":"61YjipGhv2za"},"source":["Let us train the MLP model"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80926,"status":"ok","timestamp":1641156410319,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"xjjff52rZJGE","outputId":"e179e194-376d-455f-ce11-7894eaf6e8f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.449013\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.899300\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.125380\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.895625\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.930898\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.727148\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.524453\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.501038\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.458741\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.419532\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.458280\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.413066\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.569719\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.678050\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.201645\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.660281\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.317721\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.253289\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.243792\n","Best validation accuracy improved from 0 to 0.9133846759796143, saving model...\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.082635\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.562042\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.037769\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.401888\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.325431\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.405016\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.237254\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.255650\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.505837\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.206522\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.215856\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.146534\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.310299\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.395965\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.129972\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.407144\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.097688\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.273850\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.097164\n","Best validation accuracy improved from 0.9133846759796143 to 0.9347888231277466, saving model...\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.032672\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.360286\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.020790\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.257541\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.132179\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.195924\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.108224\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.202874\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.142341\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.140258\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.175472\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.185807\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.423950\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.093800\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.069852\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.310870\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.179176\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.145399\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.073777\n","Best validation accuracy improved from 0.9347888231277466 to 0.9446346759796143, saving model...\n"]}],"source":["# define loss function\n","criterion = nn.CrossEntropyLoss() # The input is expected to contain raw, unnormalized scores for each class (logits)\n","\n","# define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) \n","\n","kwargs = {'optim': optimizer, 'crit': criterion, \n","          'epochs': epochs, 'device': device}\n","\n","training(model, train_loader, valid_loader, **kwargs)\n","\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"W6QyPq_6H6YO"},"source":["Let us now evaluate the model on the test set"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":569,"status":"ok","timestamp":1641148473803,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"_pBd_Wz83GlS","outputId":"94687a59-0b0e-4057-efbf-ebcd6c767558"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set: Loss: 0.0099, Accuracy: 2855/3000 (95.2%)\n","\n"]}],"source":["# Model with Batch Normalization active\n","test(model, device = device, test_loader=test_loader)"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"vKQKAqaI3KGu"},"outputs":[{"data":{"text/plain":["tensor([-5.2999e-02, -5.9541e-02,  5.6823e-02,  6.9551e-02,  4.7808e-02,\n","         3.9124e-02,  6.1959e-02, -9.3706e-02, -2.6536e-02, -7.2665e-02,\n","        -5.9170e-02,  7.0832e-02, -8.8911e-02, -2.1119e-02, -3.9852e-02,\n","        -4.2732e-02, -5.5919e-02, -6.9296e-02,  1.6370e-02, -3.3428e-02,\n","         5.2197e-02, -8.6305e-02, -4.1576e-02, -1.3248e-02, -7.8504e-02,\n","        -5.9105e-02,  4.8924e-02, -4.6244e-02,  1.8635e-02, -8.1541e-02,\n","        -6.1727e-02,  3.7628e-02,  1.5242e-04, -3.4926e-02,  7.0440e-02,\n","        -9.1827e-02,  2.6532e-02,  2.1177e-02,  1.9411e-02,  1.5659e-02,\n","        -9.9251e-03,  3.5104e-02, -6.1669e-02, -9.2484e-02,  6.1959e-02,\n","         3.1407e-02, -9.3905e-03,  1.6538e-02, -2.3221e-02,  1.7549e-02,\n","        -8.8138e-02,  6.2875e-02, -6.9184e-02, -3.4463e-02,  6.8159e-02,\n","         6.0280e-02,  2.6693e-02,  3.1251e-02,  1.2800e-02, -7.4549e-02,\n","        -3.9663e-02,  1.1368e-03, -3.6220e-02, -6.8453e-02,  6.4799e-02,\n","        -3.2134e-02,  4.5843e-02,  1.0128e-02, -2.7381e-02,  3.0797e-02,\n","         2.3505e-02, -1.8297e-02, -1.0122e-01, -2.5377e-02,  3.9571e-02,\n","        -9.3821e-02,  5.1324e-02, -5.7411e-02, -8.1600e-02, -6.7334e-02,\n","         1.2452e-02, -5.3856e-02, -8.1249e-02, -4.9002e-02, -8.0710e-02,\n","        -3.1059e-02, -3.6794e-02,  3.5586e-02,  1.1958e-02,  5.2673e-02,\n","         6.1340e-04, -1.1921e-02, -5.6329e-02,  4.1690e-02,  5.9623e-02,\n","        -8.0621e-02,  2.1959e-03, -2.9238e-03, -6.1468e-02, -3.2780e-03,\n","        -5.7961e-02, -8.3087e-02, -1.1557e-01, -2.7755e-02, -2.5193e-02,\n","        -6.0026e-02, -3.1484e-02, -4.9674e-03,  2.2475e-02, -2.6364e-02,\n","        -5.5732e-02, -7.5531e-02, -4.5844e-02, -7.7241e-02, -7.3881e-02,\n","         5.2058e-02, -5.9167e-02,  6.3882e-02, -1.5607e-02,  6.9414e-02,\n","        -7.0890e-02,  5.9852e-02, -2.6546e-02,  4.4535e-02, -8.0858e-02,\n","        -3.9726e-02, -2.7119e-02,  2.0020e-02, -6.3110e-02, -5.1456e-02,\n","        -2.8159e-02, -8.3262e-02, -7.6868e-02,  6.1925e-02, -1.0117e-02,\n","         5.8840e-02, -2.0306e-02,  2.4082e-02,  5.4790e-02, -4.4406e-02,\n","         1.8046e-02,  3.8844e-02,  5.4075e-03,  1.6020e-02,  2.8079e-02,\n","        -3.8439e-02,  7.0041e-02, -7.2020e-02,  2.5972e-02,  2.3764e-02,\n","        -1.0658e-03,  3.6438e-02,  8.1912e-02,  8.5516e-02, -3.2691e-02,\n","         7.7401e-02, -2.6427e-02,  6.0066e-02,  4.3554e-02, -1.1611e-02,\n","         3.4393e-02,  4.7618e-02, -5.0310e-02, -2.8776e-02, -2.8591e-02,\n","        -6.0988e-02, -2.7964e-02,  4.5852e-02, -7.8569e-02, -2.4013e-02,\n","        -1.1012e-02, -7.1371e-02,  4.2412e-02,  4.9759e-02, -7.6721e-02,\n","        -3.5074e-04,  7.9836e-02, -1.4168e-02, -2.9186e-02, -5.6889e-02,\n","         8.6815e-02,  8.3833e-02,  9.2797e-02,  4.2504e-02,  6.8298e-02,\n","        -1.2031e-02, -2.4487e-02, -2.4050e-02, -5.5029e-02, -3.1077e-02,\n","        -3.6709e-02,  1.2462e-02, -1.2385e-03,  1.0451e-02,  3.3435e-03,\n","         1.5593e-02, -8.9933e-02, -1.8883e-02, -6.6781e-02,  6.9672e-02,\n","         1.8937e-02, -6.2898e-02, -2.2509e-02,  3.1699e-02, -2.3458e-02,\n","        -2.3430e-02,  1.8887e-02,  1.4648e-02, -4.1377e-02,  5.9080e-02,\n","         3.8112e-02,  2.5104e-02,  6.3612e-02, -1.6156e-04,  8.6754e-02,\n","         1.1016e-01, -6.5234e-02,  5.7038e-02, -1.2709e-02,  4.7022e-02,\n","        -1.1956e-02, -9.4053e-03,  3.7277e-03,  2.1038e-02, -4.8984e-02,\n","        -2.5697e-02, -8.0774e-02, -9.2271e-02,  6.6607e-02, -4.1070e-02,\n","         8.9219e-02,  8.7265e-02,  4.1835e-02, -3.9530e-03,  8.6549e-02,\n","         8.1847e-02,  5.7904e-02,  2.4754e-02,  4.6489e-02, -4.5151e-02,\n","         8.2161e-02, -1.7417e-02,  5.6578e-02,  4.5979e-02,  1.7550e-03,\n","         6.1705e-02, -1.5573e-02, -5.2984e-02,  5.0538e-02, -7.5490e-02,\n","        -2.0922e-02,  5.3581e-02,  5.1877e-02, -7.3144e-02, -3.8961e-03,\n","         6.4425e-02,  3.9323e-02,  7.1094e-03,  1.2435e-01,  5.9450e-02,\n","         4.5170e-02,  3.4696e-02, -2.7034e-02,  6.5634e-02,  5.5612e-02,\n","         6.3229e-02, -9.0245e-03,  1.1055e-02,  1.8169e-02, -4.0995e-02,\n","         3.8507e-02,  3.9678e-02, -2.8026e-03,  1.2603e-02,  1.0217e-01,\n","         8.8101e-02, -3.6598e-02, -1.5035e-02, -4.9676e-02, -7.4722e-02,\n","        -3.4815e-02, -2.7814e-02,  4.3574e-02,  5.8751e-02,  1.4528e-02,\n","        -1.7298e-02,  1.8670e-02,  9.3090e-02,  1.0302e-01, -4.8392e-02,\n","         3.7152e-02, -3.7861e-03, -4.5665e-02,  2.6433e-02, -3.8038e-02,\n","         2.4997e-02, -8.9867e-03,  6.9479e-02,  1.9957e-02,  7.6267e-02,\n","         2.1191e-02,  4.3940e-02,  2.1882e-02,  5.0044e-02,  1.2831e-02,\n","         5.4283e-02,  2.0284e-02, -8.7477e-02, -1.1034e-02, -5.4162e-02,\n","        -1.4147e-02,  5.2645e-03,  1.2756e-02,  1.0094e-01,  7.6806e-02,\n","         8.2307e-02,  3.1309e-02,  9.7115e-02,  8.7473e-02,  6.2505e-02,\n","        -9.6779e-03, -3.3560e-02, -8.4394e-03,  3.5135e-02, -7.9316e-02,\n","        -5.9614e-03, -3.6930e-02,  2.0489e-02,  1.1428e-01, -2.0262e-02,\n","         1.0121e-01, -9.8530e-03, -7.7578e-03, -6.0921e-02,  1.8205e-03,\n","        -2.2645e-02, -5.4368e-02, -3.5890e-02,  1.2276e-02,  8.0740e-02,\n","         4.1402e-03,  1.4497e-02,  4.6281e-02, -2.9339e-02,  9.6084e-04,\n","        -2.2574e-02,  5.1881e-02, -5.9418e-02, -4.3542e-02,  6.4234e-02,\n","         4.2170e-02,  8.9061e-03,  8.4193e-02,  8.3086e-02,  4.8755e-02,\n","        -4.2715e-02,  6.9222e-02,  4.5014e-02,  9.5591e-02,  8.1612e-02,\n","         4.6901e-02,  4.7322e-02,  5.3500e-02,  3.3627e-02, -4.7585e-02,\n","        -9.0903e-02,  7.2688e-02, -3.5571e-02, -1.0922e-02,  3.6758e-02,\n","        -5.6236e-02, -2.1700e-02, -8.2291e-02, -6.8499e-02, -1.6269e-03,\n","         1.8433e-02, -1.1858e-02,  1.4578e-02, -1.2298e-02,  9.2681e-02,\n","        -6.4913e-02,  2.9754e-02, -6.0266e-03,  1.6231e-02,  7.3000e-02,\n","         5.0920e-02,  3.2671e-02, -3.9885e-03, -1.1871e-02, -7.5338e-02,\n","        -7.7787e-02, -6.4871e-02,  4.8411e-02, -9.4882e-03, -1.9958e-02,\n","        -8.8403e-02,  1.0024e-02, -8.1362e-02, -9.6272e-02, -7.3227e-02,\n","        -7.3367e-02, -1.1292e-01, -5.6173e-02, -1.9072e-03, -8.8663e-03,\n","        -4.5040e-02,  5.3190e-02,  6.8158e-02,  7.7453e-03,  1.3713e-02,\n","        -2.6467e-02,  2.2789e-02, -6.2822e-02, -3.6044e-02,  5.1443e-02,\n","        -7.4593e-02, -8.3932e-02,  1.9228e-02,  5.8870e-02, -6.0164e-02,\n","         5.7268e-02, -5.8638e-03,  3.2192e-02,  5.2674e-02, -5.1375e-02,\n","         3.3989e-02, -5.1964e-02,  2.0003e-02,  3.5509e-02,  5.6588e-02,\n","        -6.0390e-02, -3.6312e-02, -7.4741e-03,  6.8998e-02,  8.3155e-02,\n","         4.2810e-02,  6.2333e-02, -5.6356e-02,  4.7709e-03, -9.2754e-02,\n","        -9.5742e-02, -6.9017e-03,  6.8320e-02, -4.6579e-02,  2.9189e-02,\n","        -8.0729e-02,  7.2447e-02,  5.0865e-03, -6.6508e-02, -7.3269e-02,\n","         2.0224e-02, -7.7080e-02, -2.5234e-02, -1.3237e-02,  2.6981e-02,\n","        -1.4681e-02,  6.2804e-02,  8.9856e-04, -7.4841e-04,  5.7813e-02,\n","         9.0281e-02,  3.3994e-02, -2.4114e-02, -6.6482e-02,  2.8875e-02,\n","         1.4631e-02, -2.3784e-02, -1.4103e-02, -2.2848e-02, -1.0344e-01,\n","         2.7911e-02, -3.5467e-03, -2.8093e-02, -8.0958e-02,  1.2654e-03,\n","        -6.3918e-02, -5.1623e-02, -2.5857e-02,  3.3924e-02, -5.3249e-02,\n","        -5.6817e-02, -8.6497e-02, -9.3077e-02,  1.9763e-02,  3.2343e-02,\n","         8.2415e-02,  5.5781e-02,  8.6679e-02,  2.8103e-02, -2.2877e-02,\n","        -3.1278e-02,  2.9397e-02, -9.4901e-02,  1.7061e-02, -8.7850e-02,\n","        -6.5526e-02, -6.5971e-03, -8.2699e-02,  5.1897e-02, -7.6129e-02,\n","        -3.6621e-03,  5.1322e-02, -2.5882e-02, -7.7384e-02,  4.8710e-02,\n","         4.6111e-02,  3.1130e-02, -2.5910e-02,  3.0768e-02,  1.2331e-02,\n","        -6.1677e-02, -4.5107e-03,  5.5598e-02,  5.5417e-02,  4.2179e-02,\n","         2.9606e-03,  5.9350e-03,  1.4883e-05,  1.0832e-02,  1.2574e-02,\n","        -1.4924e-03,  1.7512e-02,  1.2298e-02, -2.0053e-02,  1.9413e-02,\n","         5.5678e-02, -3.4841e-02, -1.4651e-02,  2.1804e-02,  2.9885e-02,\n","        -3.0685e-02, -1.2778e-02,  1.9175e-03, -7.7854e-03, -2.0999e-02,\n","        -3.7817e-02,  8.4627e-03, -2.9582e-02, -6.9372e-02,  7.3442e-02,\n","         2.4227e-02, -2.6936e-02, -1.1330e-02,  2.8933e-02, -2.1067e-03,\n","        -3.6025e-02, -6.3523e-02, -1.0636e-01, -8.9277e-03,  2.9422e-02,\n","        -4.0171e-02, -1.5053e-02, -5.6697e-02, -7.2267e-02,  2.4443e-03,\n","        -1.2151e-03, -1.1986e-02, -5.0388e-02, -7.1034e-02, -2.3149e-02,\n","        -1.8844e-02, -3.4440e-02,  5.3766e-02, -4.9566e-02, -3.1105e-02,\n","         5.6860e-02, -1.3330e-02,  4.8957e-03, -5.4794e-02, -5.2405e-02,\n","         7.8116e-02, -8.1046e-02,  6.9407e-02, -5.0894e-02, -9.4818e-02,\n","        -6.7637e-02, -4.3600e-02,  9.5291e-03,  7.5035e-02,  6.6159e-02,\n","         9.7108e-02, -2.1348e-02, -2.7962e-02,  2.6991e-02, -5.8151e-02,\n","        -2.5597e-02, -5.3108e-02, -6.6572e-02, -8.3638e-02, -1.1441e-02,\n","        -8.2708e-02, -8.4870e-02, -4.3688e-02, -1.4463e-02,  3.7872e-02,\n","         2.0119e-02, -1.2576e-02, -4.6232e-02,  5.9953e-02,  4.3193e-02,\n","        -1.6813e-02, -5.7805e-02,  6.2170e-02,  5.8481e-02,  2.3994e-02,\n","         7.7212e-02, -6.1058e-02,  2.0316e-02, -1.8661e-02, -4.8512e-02,\n","         4.4549e-03, -2.0362e-02, -5.6005e-02, -1.8456e-02, -3.2180e-02,\n","         4.3684e-02, -4.0717e-02,  6.6722e-02,  4.2292e-02, -2.6533e-02,\n","         3.7301e-02,  2.0796e-02, -5.1244e-03,  4.2754e-02, -5.2702e-03,\n","         5.4834e-02, -4.8955e-02,  4.4229e-02, -1.8814e-03,  9.5781e-02,\n","        -4.5973e-02, -4.5326e-02,  1.8132e-02, -4.5674e-03,  8.5274e-02,\n","         7.5310e-02, -9.3332e-03,  2.7797e-02,  8.3345e-02,  3.2535e-02,\n","         1.9287e-02,  2.0213e-03, -8.1059e-02,  5.9525e-02, -8.5737e-02,\n","        -2.7207e-02, -8.6984e-02, -6.7522e-03,  1.3964e-03, -5.8707e-02,\n","        -4.8911e-02, -2.4419e-03, -1.9712e-02, -1.4303e-02,  1.1070e-01,\n","         5.3962e-02,  1.4406e-01,  9.9476e-02,  4.2503e-02, -1.5297e-02,\n","         3.0928e-02,  2.5916e-02, -1.6531e-02,  7.8973e-02,  6.1324e-02,\n","        -3.0006e-02, -7.6461e-02, -5.9184e-02,  3.5682e-02, -1.3569e-02,\n","         3.4134e-02,  1.0829e-02, -3.7218e-03, -7.7004e-02,  4.9301e-02,\n","        -6.5194e-02, -1.0333e-01,  3.4135e-02,  7.1852e-03,  8.8285e-04,\n","        -3.9209e-02,  2.7726e-02,  1.8652e-02,  2.9835e-02,  5.1726e-03,\n","        -3.3047e-02,  1.0766e-01,  7.9666e-02,  6.0705e-02, -7.0142e-02,\n","        -8.1991e-04,  6.0888e-02, -4.5708e-03,  2.1360e-02, -5.1522e-02,\n","         7.1022e-02,  7.2580e-03,  3.5833e-02, -1.1249e-02, -2.9841e-02,\n","         6.5251e-02, -4.5256e-02,  5.0353e-02, -2.4113e-02, -5.1798e-02,\n","         1.2834e-02,  1.4560e-02,  4.2610e-02, -5.3911e-02, -2.5839e-02,\n","        -5.2638e-02, -5.4423e-02,  8.6498e-02, -6.8512e-02, -3.5831e-02,\n","         4.5628e-02,  4.9692e-03, -3.2418e-02,  5.5346e-02, -1.6431e-02,\n","        -2.1638e-02,  1.1346e-02,  5.8558e-02,  3.5031e-02, -8.2277e-02,\n","         6.1295e-02, -1.9989e-03, -8.6525e-02, -6.4788e-02, -3.9850e-02,\n","         6.7351e-02, -8.4825e-02, -2.2490e-02, -4.1206e-03,  1.8563e-02,\n","         6.4148e-02,  7.0666e-02, -1.4512e-02, -8.7265e-02,  5.0296e-02,\n","         5.4868e-02, -6.1294e-02,  4.5020e-03, -3.6502e-02, -4.8352e-02,\n","         5.6536e-02, -1.7791e-02,  5.3630e-02,  6.7294e-02,  3.8688e-03,\n","         1.7994e-02,  1.3438e-02, -3.4551e-02, -8.6665e-02,  6.9081e-02,\n","         1.5142e-02,  5.2309e-02,  4.8611e-02, -4.1892e-02,  6.4316e-02,\n","        -1.5176e-02, -1.3720e-02, -1.0564e-02,  2.8336e-02, -1.3277e-02,\n","         2.7021e-03,  3.8389e-03, -6.5641e-02,  2.5375e-02, -8.0680e-02,\n","        -3.8210e-02,  5.5402e-02, -7.2398e-02, -8.0679e-02, -5.9902e-02,\n","        -7.2703e-02,  4.0783e-02, -3.4965e-02, -6.7695e-02, -8.0605e-02,\n","         4.0639e-02, -2.2877e-02,  4.3596e-02,  6.3393e-02],\n","       grad_fn=<SelectBackward0>)"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["list(model.parameters())[0][0]"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1641148551648,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"9m8rzjnKmU9P","outputId":"33200abc-bbc8-4be5-9c59-1a4c1d60465c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["model_ = MLP(dropout_rate = 0.2)\n","#model_.cuda()\n","model_.load_state_dict(torch.load('./weights_init'))"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"Qydnfz95mfrS"},"outputs":[{"data":{"text/plain":["tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["list(model_.parameters())[0][0]"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"DoSdmZ-BblFi"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 2259), started 1 day, 17:36:18 ago. (Use '!kill 2259' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-318933b35a3ebdda\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-318933b35a3ebdda\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Start tensorboard.\n","%load_ext tensorboard\n","%tensorboard --logdir runs/"]},{"cell_type":"markdown","metadata":{"id":"2VEH5WiYLYjB"},"source":["## Hyperparameters Tuning with Optuna\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zJJo9vkBl2Qv"},"source":["Hyperparameter optimization is an important issue in machine learning. Parameters are learned during training, as we have seen so far. Hyperparameters are instead setup before training and they do not take part to backpropagation. Different hyperparameter setups can evolve in different performances. Therefore it is important to explore the landscape of hyperparameter combinations and find out the best one according to a choosen metric.\n","\n","[Optuna](https://optuna.org/) is an open source hyperparameter optimization framework to automate hyperparameter search. Optuna is framework agnostic. You can use it with any machine learning or deep learning framework.\n","\n","The main objects of an Optuna pipeline are\n","\n","* a **trial** corresponds to a single execution of the objective function and it is internally instantiated upon each invocation of the function;\n","\n","* **suggest methods** are called inside the objective function to obtain parameters for a trial;\n","\n","* a **study object** is created to start the optimization.\n","\n","The main idea is to do a lot of small trainings and see their performance to understand the best choice for hyperparameters that will be later applied for the larger training."]},{"cell_type":"code","execution_count":99,"metadata":{"id":"SXgEz_vuPAQp"},"outputs":[],"source":["# let us define our objective function\n","def train_mnist(trial):\n","\n","  # configuration\n","  cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","          'n_epochs' : 1,\n","          'seed' : 0, #sometimes also the seed is treated as a hyperparameter\n","          'model_save' : False,\n","          'lr' : trial.suggest_loguniform('lr', 1e-4, 1e-2),          \n","          'dropout': trial.suggest_discrete_uniform('dropout', 0.1, 0.3, 0.5),\n","          'optimizer': trial.suggest_categorical('optimizer',[torch.optim.SGD, torch.optim.Adam]),\n","          'batch_norm': trial.suggest_categorical('batch_norm',[True, False]), #after each layer, re-normalize images\n","          }\n","\n","  torch.manual_seed(cfg['seed'])\n","  # model\n","  model = MLP(dropout_rate = cfg['dropout']).to(device)\n","  # apply weight init\n","  model.apply(weight_init)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = cfg['optimizer'](model.parameters(), lr=cfg['lr'])\n","\n","  best_valid_acc = training(model, train_loader, valid_loader, optimizer, criterion, \n","                  epochs = cfg['n_epochs'], device = cfg['device'], model_save = cfg['model_save'])\n","\n","  return best_valid_acc"]},{"cell_type":"markdown","metadata":{"id":"RKpYTX-vEW6U"},"source":["To determine the hyperparameter values to be used in a trial, we must define a sampler. We will use `optuna.samplers.TPESampler()` that is based on Tree-structured Parzen Estimator algorithm (it makes use of Gaussian Mixture Models, more details [here](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler)). There are [several samplers](https://optuna.readthedocs.io/en/stable/reference/samplers.html), for instance also the most basic ones like `optuna.samplers.GridSampler` which suggests all combinations of parameters in the given search space during the study.\n","\n","It means that there are some methods to optimize the choice of hyperparameters in the landscape without having to try all different combinations, which could be an issue with a large number of hyperparameters."]},{"cell_type":"code","execution_count":100,"metadata":{"id":"BoZYv0C3jCj4"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:24:06,583]\u001b[0m A new study created in memory with name: no-name-8daad48b-79c3-4b45-a7be-f1bd64935bce\u001b[0m\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.400244\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.539339\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.499991\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.517063\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.466884\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.378365\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.317955\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.283356\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.141896\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.216037\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.145608\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.552999\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.417999\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.232610\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.481063\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.160139\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.508085\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.253235\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:24:42,451]\u001b[0m Trial 0 finished with value: 0.9198059439659119 and parameters: {'lr': 0.000151363448864468, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 0 with value: 0.9198059439659119.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9198059439659119, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.750429\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.321132\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.156487\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.535379\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.170803\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.089809\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.336929\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.228790\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.470848\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.056128\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.376404\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.197922\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.211433\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.127316\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.134431\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.099375\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.471330\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.099779\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:25:18,903]\u001b[0m Trial 1 finished with value: 0.9375 and parameters: {'lr': 0.0028541577749143453, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9375, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.820084\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.716220\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.623483\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.792563\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.628427\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.524142\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.366638\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.321197\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.133297\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.195827\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.319549\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.514977\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.472526\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.294571\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.466597\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.251218\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.435545\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.277994\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:25:51,762]\u001b[0m Trial 2 finished with value: 0.9128139019012451 and parameters: {'lr': 0.0034941803007518257, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9128139019012451, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.891878\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.764834\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.640768\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.821174\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.643692\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.539317\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.385500\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.323390\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.146666\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.201720\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.344950\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.509911\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.485488\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.306382\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.507465\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.268583\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.490198\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.300686\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:26:25,039]\u001b[0m Trial 3 finished with value: 0.9099600315093994 and parameters: {'lr': 0.0032767446647566016, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9099600315093994, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.454022\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.957827\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.898387\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.878587\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.916594\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.631270\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.506176\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.284660\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.088629\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.066763\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.081795\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.869032\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.151980\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.042064\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.766852\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.842765\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.153137\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.841476\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:26:58,229]\u001b[0m Trial 4 finished with value: 0.808504581451416 and parameters: {'lr': 0.0005113687026901631, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.808504581451416, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.090077\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.367341\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.396503\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.172304\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.444754\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.602132\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.350893\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.228862\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.416934\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.207167\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.596852\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.231120\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.500272\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.386347\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.114082\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.273666\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.202831\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.056849\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:27:34,430]\u001b[0m Trial 5 finished with value: 0.8671517968177795 and parameters: {'lr': 0.008338312815350314, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.8671517968177795, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.786675\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.694794\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.626590\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.763152\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.599680\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.529461\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.345409\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.324860\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.107426\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.187248\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.300636\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.487492\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.453054\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.265068\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.403128\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.258891\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.431619\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.257336\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:28:07,726]\u001b[0m Trial 6 finished with value: 0.9135273694992065 and parameters: {'lr': 0.0036716786924814644, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9135273694992065, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.589637\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.679887\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.556858\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.597204\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.501029\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.414011\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.325287\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.308196\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.137058\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.208845\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.171126\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.590844\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.445486\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.253050\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.405897\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.178462\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.447527\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.247423\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:28:44,017]\u001b[0m Trial 7 finished with value: 0.9215182662010193 and parameters: {'lr': 0.00013069605236461175, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9215182662010193, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.082777\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.428096\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.413250\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.484210\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.426225\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.385024\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.263881\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.267669\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.159615\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.174257\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.107958\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.530710\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.340642\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.227182\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.258771\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.139167\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.360362\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.168716\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:29:20,061]\u001b[0m Trial 8 finished with value: 0.9287956357002258 and parameters: {'lr': 0.00021468570899726226, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9287956357002258, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.448142\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.911467\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.832020\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.807837\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.817362\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.520973\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.336119\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.147577\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.951432\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.949124\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.947663\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.778790\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.041244\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.902283\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.699885\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.758306\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.081712\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.782550\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:29:53,543]\u001b[0m Trial 9 finished with value: 0.826626718044281 and parameters: {'lr': 0.0005969148182847768, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.826626718044281, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.685842\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.226727\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.177911\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.395603\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.301876\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.140895\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.210131\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.145116\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.476505\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.247478\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.288384\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.159547\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.159671\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.509614\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.180460\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.068211\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.162390\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.030667\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:30:29,780]\u001b[0m Trial 10 finished with value: 0.9292237162590027 and parameters: {'lr': 0.001334414654267651, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9292237162590027, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.711581\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.073568\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.223176\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.533315\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.225707\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.158881\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.196421\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.369393\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.333169\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175167\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.306152\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.216313\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.045289\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.156831\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.102598\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.196488\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.119956\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.029873\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:31:06,039]\u001b[0m Trial 11 finished with value: 0.9333618879318237 and parameters: {'lr': 0.0015240122103371992, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9375.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9333618879318237, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.741107\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.178380\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.277825\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.563628\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.227543\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.161136\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.149107\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.125192\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.463066\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162893\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.351322\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.333321\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.219147\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.146346\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.208897\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.141351\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.260061\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.040216\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:31:42,015]\u001b[0m Trial 12 finished with value: 0.9376426935195923 and parameters: {'lr': 0.0014943285436459864, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 12 with value: 0.9376426935195923.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9376426935195923, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.888338\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.123029\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.092285\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.381033\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.295240\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.201767\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.118254\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.131900\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.490476\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.163862\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.397258\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.304897\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.176823\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.078252\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.359547\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.124583\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.471372\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.038610\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:32:18,308]\u001b[0m Trial 13 finished with value: 0.9402111768722534 and parameters: {'lr': 0.0020395787073124237, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 13 with value: 0.9402111768722534.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9402111768722534, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/home/matteo/.local/lib/python3.8/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.526620\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.123850\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.215962\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.303867\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.201047\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.311273\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.219652\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.137511\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.380539\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.073491\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.308845\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.411232\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.104730\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.179938\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.179547\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.164880\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.263048\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.015073\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-25 10:32:54,656]\u001b[0m Trial 14 finished with value: 0.941495418548584 and parameters: {'lr': 0.0007313410911473901, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 14 with value: 0.941495418548584.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.941495418548584, saving model...\n"]},{"data":{"text/plain":["['./optuna_report.pkl']"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","\n","# a sampler has the responsibility to determine the parameter values to be evaluated in a trial.\n","sampler = optuna.samplers.TPESampler()\n","# You can also try \n","# sampler = optuna.samplers.GridSampler()\n","    \n","study = optuna.create_study(sampler=sampler, direction='maximize')\n","study.optimize(func=train_mnist, n_trials=15)\n","\n","# persist an arbitrary Python object into one file.\n","joblib.dump(study, './optuna_report.pkl')"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /home/matteo/.local/lib/python3.8/site-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/matteo/.local/lib/python3.8/site-packages (from pandas) (1.22.1)\n","Requirement already satisfied: pytz>=2020.1 in /home/matteo/.local/lib/python3.8/site-packages (from pandas) (2021.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /home/matteo/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n"]}],"source":["!pip install pandas\n","import pandas"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: joblib in /home/matteo/.local/lib/python3.8/site-packages (1.1.0)\n"]}],"source":["!pip install joblib\n","import joblib"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":1780,"status":"ok","timestamp":1599665015980,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"XR7f5Y9gfxMz","outputId":"7267f7e4-32a2-4632-f1a9-0dbca7ff5efb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>value</th>\n","      <th>duration</th>\n","      <th>params_batch_norm</th>\n","      <th>params_dropout</th>\n","      <th>params_lr</th>\n","      <th>params_optimizer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.919806</td>\n","      <td>0 days 00:00:35.864565</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000151</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.937500</td>\n","      <td>0 days 00:00:36.450741</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.002854</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.912814</td>\n","      <td>0 days 00:00:32.857596</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.003494</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.909960</td>\n","      <td>0 days 00:00:33.276012</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.003277</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.808505</td>\n","      <td>0 days 00:00:33.188476</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000511</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.867152</td>\n","      <td>0 days 00:00:36.195337</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.008338</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.913527</td>\n","      <td>0 days 00:00:33.292750</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.003672</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.921518</td>\n","      <td>0 days 00:00:36.289386</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000131</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.928796</td>\n","      <td>0 days 00:00:36.041090</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000215</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.826627</td>\n","      <td>0 days 00:00:33.480093</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000597</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.929224</td>\n","      <td>0 days 00:00:36.235855</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.001334</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.933362</td>\n","      <td>0 days 00:00:36.257713</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.001524</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>0.937643</td>\n","      <td>0 days 00:00:35.974238</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.001494</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>0.940211</td>\n","      <td>0 days 00:00:36.290622</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.002040</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>0.941495</td>\n","      <td>0 days 00:00:36.346092</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000731</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    number     value               duration  params_batch_norm  \\\n","0        0  0.919806 0 days 00:00:35.864565               True   \n","1        1  0.937500 0 days 00:00:36.450741               True   \n","2        2  0.912814 0 days 00:00:32.857596               True   \n","3        3  0.909960 0 days 00:00:33.276012              False   \n","4        4  0.808505 0 days 00:00:33.188476              False   \n","5        5  0.867152 0 days 00:00:36.195337              False   \n","6        6  0.913527 0 days 00:00:33.292750               True   \n","7        7  0.921518 0 days 00:00:36.289386               True   \n","8        8  0.928796 0 days 00:00:36.041090              False   \n","9        9  0.826627 0 days 00:00:33.480093               True   \n","10      10  0.929224 0 days 00:00:36.235855               True   \n","11      11  0.933362 0 days 00:00:36.257713               True   \n","12      12  0.937643 0 days 00:00:35.974238               True   \n","13      13  0.940211 0 days 00:00:36.290622               True   \n","14      14  0.941495 0 days 00:00:36.346092               True   \n","\n","    params_dropout  params_lr                 params_optimizer  \n","0              0.1   0.000151  <class 'torch.optim.adam.Adam'>  \n","1              0.1   0.002854  <class 'torch.optim.adam.Adam'>  \n","2              0.1   0.003494    <class 'torch.optim.sgd.SGD'>  \n","3              0.1   0.003277    <class 'torch.optim.sgd.SGD'>  \n","4              0.1   0.000511    <class 'torch.optim.sgd.SGD'>  \n","5              0.1   0.008338  <class 'torch.optim.adam.Adam'>  \n","6              0.1   0.003672    <class 'torch.optim.sgd.SGD'>  \n","7              0.1   0.000131  <class 'torch.optim.adam.Adam'>  \n","8              0.1   0.000215  <class 'torch.optim.adam.Adam'>  \n","9              0.1   0.000597    <class 'torch.optim.sgd.SGD'>  \n","10             0.1   0.001334  <class 'torch.optim.adam.Adam'>  \n","11             0.1   0.001524  <class 'torch.optim.adam.Adam'>  \n","12             0.1   0.001494  <class 'torch.optim.adam.Adam'>  \n","13             0.1   0.002040  <class 'torch.optim.adam.Adam'>  \n","14             0.1   0.000731  <class 'torch.optim.adam.Adam'>  "]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["# load the saved study\n","study = joblib.load('./optuna_report.pkl')\n","# convert in dataframe\n","df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n","df"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":1154,"status":"ok","timestamp":1599665083110,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"dSJzu-9PgZCV","outputId":"4d0cef9f-0239-41b2-f761-c0f4cceddf3e"},"outputs":[{"data":{"text/plain":["FrozenTrial(number=14, values=[0.941495418548584], datetime_start=datetime.datetime(2022, 2, 25, 10, 32, 18, 310302), datetime_complete=datetime.datetime(2022, 2, 25, 10, 32, 54, 656394), params={'lr': 0.0007313410911473901, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}, distributions={'lr': LogUniformDistribution(high=0.01, low=0.0001), 'dropout': DiscreteUniformDistribution(high=0.1, low=0.1, q=0.5), 'optimizer': CategoricalDistribution(choices=(<class 'torch.optim.sgd.SGD'>, <class 'torch.optim.adam.Adam'>)), 'batch_norm': CategoricalDistribution(choices=(True, False))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=14, state=TrialState.COMPLETE, value=None)"]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["study.best_trial"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /home/matteo/.local/lib/python3.8/site-packages (2.10.0)\n","Requirement already satisfied: scipy!=1.4.0 in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (1.8.0)\n","Requirement already satisfied: colorlog in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (6.6.0)\n","Requirement already satisfied: numpy in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (1.22.1)\n","Requirement already satisfied: cmaes>=0.8.2 in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (0.8.2)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (1.4.31)\n","Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna) (5.3.1)\n","Requirement already satisfied: cliff in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (3.10.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from optuna) (20.3)\n","Requirement already satisfied: tqdm in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (4.62.3)\n","Requirement already satisfied: alembic in /home/matteo/.local/lib/python3.8/site-packages (from optuna) (1.7.6)\n","Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in /home/matteo/.local/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Requirement already satisfied: pyparsing>=2.1.0 in /usr/lib/python3/dist-packages (from cliff->optuna) (2.4.6)\n","Requirement already satisfied: cmd2>=1.0.0 in /home/matteo/.local/lib/python3.8/site-packages (from cliff->optuna) (2.3.3)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /home/matteo/.local/lib/python3.8/site-packages (from cliff->optuna) (3.1.1)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/matteo/.local/lib/python3.8/site-packages (from cliff->optuna) (5.8.1)\n","Requirement already satisfied: autopage>=0.4.0 in /home/matteo/.local/lib/python3.8/site-packages (from cliff->optuna) (0.5.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /home/matteo/.local/lib/python3.8/site-packages (from cliff->optuna) (3.5.0)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/matteo/.local/lib/python3.8/site-packages (from alembic->optuna) (5.4.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /home/matteo/.local/lib/python3.8/site-packages (from alembic->optuna) (4.10.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic->optuna) (1.1.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /home/matteo/.local/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /home/matteo/.local/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: pyperclip>=1.6 in /home/matteo/.local/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n","Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/matteo/.local/lib/python3.8/site-packages (from importlib-resources; python_version < \"3.9\"->alembic->optuna) (3.7.0)\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["import optuna\n","import plotly"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":3068,"status":"ok","timestamp":1599665101897,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"jVpuIr6mgZMv","outputId":"7e2217ee-0292-4da4-bd96-f2e185337cb2"},"outputs":[{"ename":"ImportError","evalue":"Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m try_import() \u001b[39mas\u001b[39;00m _imports:  \u001b[39m# NOQA\u001b[39;00m\n\u001b[0;32m----> <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py?line=6'>7</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m  \u001b[39m# NOQA\u001b[39;00m\n\u001b[1;32m      <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py?line=7'>8</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mplotly\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m plotly_version\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m/home/matteo/Documenti/PhD/Corsi di Dottorato/Introduction to Deep Learning for Physicists/3_mlp_mnist_training.ipynb Cell 79'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/Documenti/PhD/Corsi%20di%20Dottorato/Introduction%20to%20Deep%20Learning%20for%20Physicists/3_mlp_mnist_training.ipynb#ch0000074?line=0'>1</a>\u001b[0m \u001b[39m#optuna.visualization.plot_contour(study, params=['lr','optimizer'])\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matteo/Documenti/PhD/Corsi%20di%20Dottorato/Introduction%20to%20Deep%20Learning%20for%20Physicists/3_mlp_mnist_training.ipynb#ch0000074?line=1'>2</a>\u001b[0m optuna\u001b[39m.\u001b[39;49mvisualization\u001b[39m.\u001b[39;49mplot_optimization_history(study)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py:73\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_optimization_history\u001b[39m(\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=22'>23</a>\u001b[0m     study: Study,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=23'>24</a>\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=24'>25</a>\u001b[0m     target: Optional[Callable[[FrozenTrial], \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=25'>26</a>\u001b[0m     target_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mObjective Value\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=26'>27</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgo.Figure\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=27'>28</a>\u001b[0m     \u001b[39m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=28'>29</a>\u001b[0m \n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=29'>30</a>\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=69'>70</a>\u001b[0m \u001b[39m            optimization.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=70'>71</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=72'>73</a>\u001b[0m     _imports\u001b[39m.\u001b[39;49mcheck()\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=73'>74</a>\u001b[0m     _check_plot_args(study, target, target_name)\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_optimization_history_plot(study, target, target_name)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/_imports.py:86\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/_imports.py?line=83'>84</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deferred \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.8/site-packages/optuna/_imports.py?line=84'>85</a>\u001b[0m     exc_value, message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deferred\n\u001b[0;32m---> <a href='file:///~/.local/lib/python3.8/site-packages/optuna/_imports.py?line=85'>86</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc_value\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."]}],"source":["#optuna.visualization.plot_contour(study, params=['lr','optimizer'])\n","optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"markdown","metadata":{},"source":["We can also add a \"green parameter\" to have a better understanding of performances with respect to the choice of hyperparameters and their carbon footprint, to better decide which model to use."]},{"cell_type":"markdown","metadata":{"id":"jB704nfU-eiB"},"source":["## K-Fold Cross Validation"]},{"cell_type":"markdown","metadata":{"id":"ZPyYQxI1x8hB"},"source":["So far, we have used holdout set for model validation that consists of splitting once and for all the whole dataset in training and validation sets. This method is widely used but can suffer from the lucky split phenomenon.\n","\n","It is a good practice in machine learning to overwhelm hold-out validation and exploit cross-validation. This means that multiple rounds of train/validation are performed using different data partitions. Validation results are then averaged over the rounds to give a fair estimate of the model predictive performances. This process allows to reduce variance and avoids the lucky split phonomenon.\n","\n","Let see how it works. Cross-validation is always performed on training set (Think about MNIST dataset, that has 60k samples in training set and 10K in test set). In case of k-fold cross validation, say number of samples in training set is 100 and you have taken k = 5, then train set is equally divided in 5 equal parts: p1, p2, p3, p4, p5.\n","Now, in first iteration/fold p1 will be left out and remaining 4 parts (p2, p3, p4, p5) will be used for training the algorithm and p1 to validate it. Once algorithm is trained this trained model will be validated on p1 from this you will get error/accuracy metric. Now in 2nd iteration/fold p2 will be left out and again algorithm will be trained on remaining 4 parts( p1, p3, p4, p5) and once algorithm is trained it gets validated on p2. It continues till all 5 iterations are over. At last you will get average train and validated error/accuracy metrics from cross validation exercise. And this is how cross-validation works and is used.\n","\n","<center>  <img src=\"https://drive.google.com/uc?export=view&id=1qRpYnHSzc1lhutp-F3ehpEGMSSa18Eza\"width=\"800\">  </center> \n","\n","\n","The best scenario is that our accuracy is similar in all our folds, say 92.0, 91.5, 92.0, 92.5 and 91.8. This means that our algorithm (and our data) is consistent and we can be confident that by training it on all the data set and deploy it in production will lead to similar performance.\n","However, we could end up in a slightly different scenario, say 92.0, 44.0, 91.5, 92.5 and 91.8. These results look very strange. It looks like one of our folds is from a different distribution, we have to go back and make sure that our data is what we think it is.\n","\n","The worst scenario we can end up in is when we have considerable variation in our results, say 80, 44, 99, 60 and 87. Here it looks like that our algorithm or our data (or both) is no consistent, it could be that our algorithm is unable to learn, or our data is very complicated.\n"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"pR3ds_7kCPko"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor(), \n","                                        transforms.Normalize( (0.1307,), (0.3081,))])\n","\n","train_dataset = MNIST(root = './data', train = True, transform = transform, download=True)"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"p3MsWG5l6CHH"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","kfold = KFold(n_splits=5)\n","\n","def cross_validation(network, optim, crit, epochs, device, batch_size = 16, model_save = False):\n","    final_acc = []\n","    for fold, (train_index, valid_index) in enumerate(kfold.split(train_dataset.data, train_dataset.targets)):\n","        # reset weights to init values at each fold\n","        network.load_state_dict(torch.load('./weights_init'))\n","        print(list(network.parameters())[0][0])\n","        \n","        ### Dividing data into folds\n","        train_fold = torch.utils.data.Subset(train_dataset, train_index)\n","        valid_fold = torch.utils.data.Subset(train_dataset, valid_index)\n","\n","        train_loader_fold = DataLoader(train_fold, batch_size = batch_size, shuffle = False)\n","        valid_loader_fold = DataLoader(valid_fold, batch_size = batch_size, shuffle = False)\n","\n","        #print(len(train_loader_fold))\n","        #print(len(valid_loader_fold))\n","\n","        kwargs = {'optim': optim, 'crit': crit, \n","          'epochs': epochs, 'device': device, 'model_save': model_save}\n","        \n","        valid_acc = training(network, train_loader_fold, valid_loader_fold, **kwargs)\n","        final_acc.append(valid_acc.to('cpu'))\n","        #print(final_acc)\n","\n","        print('\\nFold number {} , Valid Accuracy: {}\\n'.format(fold + 1 , valid_acc))\n","    #print(final_acc)\n","    print('\\nFinal Accuracy Mean: {} , Final Accuracy Std: {}\\n'.format(np.array(final_acc).mean() , np.array(final_acc).std()))  \n","    return np.array(final_acc).mean(), np.array(final_acc).std()"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"HvRbV5CRGdQ8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.591111\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.783140\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.284075\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.358121\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.450302\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.153719\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.934324\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.151172\n","Best validation accuracy improved from 0 to 0.9348333477973938, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.280789\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.243501\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.138408\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.217637\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.211842\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.100034\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.591741\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.128091\n","Best validation accuracy improved from 0.9348333477973938 to 0.9474166631698608, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.214643\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.129086\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.159412\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.111194\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.045590\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.041908\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.342181\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.101167\n","Best validation accuracy improved from 0.9474166631698608 to 0.9568333625793457, saving model...\n","\n","Fold number 1 , Valid Accuracy: 0.9568333625793457\n","\n","tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.519601\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.417233\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.371081\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.352094\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.332727\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.145913\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.069862\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.183367\n","Best validation accuracy improved from 0 to 0.921500027179718, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.114670\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.252688\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.264273\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.117479\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.076482\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.064991\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.760640\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.179998\n","Best validation accuracy improved from 0.921500027179718 to 0.9458333253860474, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.111626\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.061335\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.222403\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.064257\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.063468\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.024126\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.774413\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.108287\n","Best validation accuracy improved from 0.9458333253860474 to 0.956166684627533, saving model...\n","\n","Fold number 2 , Valid Accuracy: 0.956166684627533\n","\n","tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.519480\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.472636\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.398023\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.357292\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.205801\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.171614\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.775991\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.195779\n","Best validation accuracy improved from 0 to 0.9364166855812073, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.100749\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.222072\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.095326\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.189186\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.094375\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.095599\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.606596\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.157558\n","Best validation accuracy improved from 0.9364166855812073 to 0.9449166655540466, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.086478\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.289276\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.143553\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.171627\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.054408\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.036686\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.669210\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.092799\n","Best validation accuracy improved from 0.9449166655540466 to 0.9572499990463257, saving model...\n","\n","Fold number 3 , Valid Accuracy: 0.9572499990463257\n","\n","tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.364529\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.605899\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.343401\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.407727\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.204252\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.436441\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.077784\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.176176\n","Best validation accuracy improved from 0 to 0.9307500123977661, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.194213\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.108296\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.126553\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.185601\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.152481\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.433005\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.685935\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.171355\n","Best validation accuracy improved from 0.9307500123977661 to 0.9480000138282776, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.090254\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.067505\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.086905\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.070278\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.067501\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.265497\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.603708\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.100428\n","Best validation accuracy improved from 0.9480000138282776 to 0.9523333311080933, saving model...\n","\n","Fold number 4 , Valid Accuracy: 0.9523333311080933\n","\n","tensor([-4.2584e-02, -4.9126e-02,  6.7238e-02,  7.9967e-02,  5.8223e-02,\n","         4.9539e-02,  7.2374e-02, -8.3290e-02, -1.6120e-02, -6.2249e-02,\n","        -4.8754e-02,  8.1247e-02, -7.8496e-02, -1.0715e-02, -2.9448e-02,\n","        -3.2317e-02, -4.5504e-02, -5.8880e-02,  2.6785e-02, -2.3013e-02,\n","         6.2612e-02, -7.5890e-02, -3.1161e-02, -2.8328e-03, -6.8089e-02,\n","        -4.8689e-02,  5.9339e-02, -3.5828e-02,  2.9050e-02, -7.1126e-02,\n","        -5.1311e-02,  4.8043e-02,  1.0567e-02, -2.4514e-02,  8.0874e-02,\n","        -8.1541e-02,  3.6894e-02,  3.1967e-02,  3.0852e-02,  2.6882e-02,\n","         8.5259e-04,  4.4804e-02, -5.1139e-02, -7.9749e-02,  7.2724e-02,\n","         4.1615e-02,  1.7808e-03,  2.8067e-02, -1.2003e-02,  2.8030e-02,\n","        -7.7755e-02,  7.3268e-02, -5.8769e-02, -2.4047e-02,  7.8574e-02,\n","         7.0696e-02,  3.7109e-02,  4.1667e-02,  2.3221e-02, -6.4034e-02,\n","        -2.9213e-02,  1.1525e-02, -2.6206e-02, -6.0651e-02,  7.2394e-02,\n","        -2.3528e-02,  5.7279e-02,  2.1279e-02, -1.6197e-02,  4.6683e-02,\n","         4.2622e-02,  7.4196e-03, -7.3610e-02, -6.5199e-04,  6.1671e-02,\n","        -7.1180e-02,  6.6570e-02, -4.5188e-02, -7.1280e-02, -5.7223e-02,\n","         2.3265e-02, -4.3128e-02, -7.0834e-02, -3.8586e-02, -7.0294e-02,\n","        -2.0643e-02, -2.6278e-02,  4.6375e-02,  2.2457e-02,  6.2188e-02,\n","         9.4837e-03, -5.2565e-03, -4.8359e-02,  5.5778e-02,  7.0440e-02,\n","        -6.5997e-02,  2.7935e-02,  3.1646e-02, -2.0175e-02,  3.9664e-02,\n","        -9.4439e-03, -3.5028e-02, -8.3334e-02, -5.7247e-03, -1.2265e-02,\n","        -5.3543e-02, -2.7656e-02,  3.4142e-04,  3.0353e-02, -1.8128e-02,\n","        -4.5847e-02, -6.5116e-02, -3.5429e-02, -6.6829e-02, -6.3199e-02,\n","         6.2662e-02, -4.8817e-02,  7.1041e-02, -6.6587e-03,  7.9286e-02,\n","        -5.7528e-02,  8.1189e-02, -4.0672e-03,  6.1334e-02, -7.2879e-02,\n","        -3.0507e-02, -1.4099e-02,  4.2745e-02, -5.1911e-02, -4.2097e-02,\n","        -2.3125e-02, -7.9695e-02, -7.5381e-02,  7.8707e-02,  1.6182e-02,\n","         7.4873e-02, -1.8936e-02,  2.8358e-02,  6.2122e-02, -3.4474e-02,\n","         2.8462e-02,  4.9259e-02,  1.5997e-02,  2.6241e-02,  3.8229e-02,\n","        -2.7859e-02,  8.1160e-02, -5.9705e-02,  4.7182e-02,  3.4932e-02,\n","        -9.1478e-03,  8.9772e-03,  6.3256e-02,  7.2112e-02, -4.5859e-02,\n","         7.0781e-02, -4.1043e-02,  3.9020e-02,  2.5099e-02, -4.4734e-02,\n","         1.1708e-02,  5.6562e-02, -1.8960e-02, -9.1102e-04, -2.4687e-02,\n","        -6.3178e-02, -2.0572e-02,  5.5839e-02, -6.8154e-02, -1.3601e-02,\n","        -6.2407e-04, -6.2406e-02,  5.3819e-02,  5.8745e-02, -7.2789e-02,\n","        -2.2346e-03,  6.5299e-02, -5.2713e-02, -6.3692e-02, -7.2688e-02,\n","         7.5811e-02,  7.8314e-02,  7.6593e-02,  1.1523e-02,  3.4879e-02,\n","        -4.2989e-02, -5.2370e-02, -6.8231e-02, -8.3344e-02, -3.3736e-02,\n","        -2.0074e-02,  2.2290e-02, -8.3119e-03,  6.8177e-03,  1.1403e-02,\n","         2.4815e-02, -7.9519e-02, -8.5964e-03, -5.5587e-02,  7.9976e-02,\n","         3.1174e-02, -6.5101e-02, -4.5197e-02, -6.4969e-03, -8.1634e-02,\n","        -7.0433e-02, -5.5457e-03,  1.0761e-02, -4.6224e-02,  4.4621e-02,\n","         9.6195e-03, -2.4121e-02,  2.3777e-02, -2.9581e-02,  4.8740e-02,\n","         8.3687e-02, -6.7656e-02,  6.9991e-02, -6.6111e-04,  4.7957e-02,\n","        -2.3984e-02, -6.8445e-03,  1.3762e-02,  3.0606e-02, -3.8754e-02,\n","        -1.5802e-02, -7.0155e-02, -8.2095e-02,  6.9464e-02, -6.1991e-02,\n","         4.7381e-02,  3.3291e-02, -1.1615e-02, -3.2441e-02,  6.9290e-02,\n","         7.6090e-02,  3.9793e-02, -2.7137e-03,  2.5118e-02, -6.9923e-02,\n","         5.3136e-02, -5.2170e-02,  9.9739e-03,  1.2612e-02,  9.9019e-03,\n","         7.7430e-02, -1.5249e-02, -6.0526e-02,  4.0836e-02, -7.2641e-02,\n","        -1.4988e-02,  6.2568e-02,  6.2253e-02, -6.2930e-02,  4.1948e-03,\n","         7.6008e-02,  3.4724e-02, -2.0834e-02,  8.1667e-02,  2.3950e-02,\n","         1.2295e-02,  9.3912e-03, -5.0349e-02,  4.8425e-02,  4.0097e-02,\n","         5.8492e-02, -1.0808e-03,  2.9497e-02,  1.4104e-02, -7.1658e-02,\n","         4.2408e-03,  8.0898e-03, -1.2244e-02,  6.1577e-03,  7.9639e-02,\n","         5.9734e-02, -4.8148e-02, -1.3459e-02, -4.6433e-02, -6.3354e-02,\n","        -2.4440e-02, -1.7828e-02,  5.1356e-02,  6.5653e-02,  1.9183e-03,\n","        -5.4733e-02, -2.1162e-02,  5.7730e-02,  7.1384e-02, -6.8338e-02,\n","         1.0679e-02, -2.6410e-02, -6.6854e-02,  1.4669e-02, -2.2156e-02,\n","         5.9110e-02,  1.1591e-03,  6.0720e-02, -1.6519e-02,  2.8170e-02,\n","        -1.0499e-02,  5.6657e-03, -2.9179e-02,  4.7828e-03,  2.8768e-03,\n","         6.2095e-02,  2.5906e-02, -7.8252e-02, -6.3888e-04, -4.4184e-02,\n","        -6.9132e-03,  6.7920e-03, -6.9685e-03,  6.0217e-02,  4.2983e-02,\n","         4.3410e-02,  2.9537e-03,  8.3969e-02,  7.6426e-02,  4.7763e-02,\n","        -3.5119e-02, -6.4283e-02, -1.6571e-02,  5.2273e-02, -7.7477e-02,\n","        -2.0735e-02, -7.9016e-02, -2.4850e-02,  8.1317e-02, -6.2626e-02,\n","         5.1058e-02, -4.3967e-02, -3.0649e-02, -6.5372e-02,  1.8417e-03,\n","        -1.2859e-02, -4.3961e-02, -2.6425e-02,  1.9272e-02,  7.7888e-02,\n","        -7.4441e-03, -1.5109e-02,  3.0953e-02, -4.9928e-02, -2.5824e-03,\n","        -1.3771e-02,  7.5287e-02, -5.1507e-02, -5.2587e-02,  5.2403e-02,\n","         4.1679e-02,  8.7064e-03,  7.8232e-02,  7.2514e-02,  2.5489e-02,\n","        -7.2865e-02,  3.6459e-02,  6.3726e-03,  6.7099e-02,  6.1061e-02,\n","         3.5156e-02,  4.6634e-02,  5.6406e-02,  4.3749e-02, -3.7165e-02,\n","        -8.1053e-02,  8.1461e-02, -3.1415e-02, -1.2956e-03,  3.6398e-02,\n","        -4.2266e-02, -5.3773e-03, -5.9079e-02, -4.2380e-02,  3.5439e-02,\n","         3.7094e-02, -1.6739e-02,  2.6182e-03, -2.1738e-02,  8.1290e-02,\n","        -6.1793e-02,  4.6242e-02, -1.4836e-03,  1.1151e-02,  5.9030e-02,\n","         4.3016e-02,  2.4628e-02, -4.4693e-03, -9.0099e-03, -7.7545e-02,\n","        -7.3807e-02, -5.7161e-02,  5.8773e-02,  8.5544e-04, -9.1826e-03,\n","        -7.5186e-02,  3.8621e-02, -5.0836e-02, -5.8846e-02, -2.6634e-02,\n","        -2.1550e-02, -7.5557e-02, -3.4470e-02, -5.0018e-03, -2.0126e-02,\n","        -6.2878e-02,  3.4454e-02,  6.6206e-02,  2.0848e-02,  3.1419e-02,\n","         8.2108e-03,  4.3709e-02, -5.1134e-02, -2.8555e-02,  6.5117e-02,\n","        -6.3946e-02, -7.1933e-02,  1.8472e-02,  6.3359e-02, -4.9972e-02,\n","         6.7665e-02,  4.5839e-03,  4.2598e-02,  6.9347e-02, -2.0878e-02,\n","         7.6105e-02, -3.2363e-03,  6.1935e-02,  7.3761e-02,  7.7795e-02,\n","        -6.3123e-02, -5.6414e-02, -3.6161e-02,  2.7189e-02,  4.2947e-02,\n","         2.4967e-02,  6.1607e-02, -3.9522e-02,  4.8459e-02, -6.1682e-02,\n","        -6.8634e-02,  8.0670e-03,  8.3982e-02, -2.7583e-02,  4.0038e-02,\n","        -7.4306e-02,  8.2560e-02,  1.4760e-02, -5.6099e-02, -6.2684e-02,\n","         2.9883e-02, -6.1658e-02,  3.4726e-03,  2.8778e-02,  6.7532e-02,\n","         9.8921e-03,  7.2624e-02, -1.2777e-02, -3.1363e-02,  2.3292e-02,\n","         3.0394e-02, -2.3968e-02, -6.1217e-02, -7.7566e-02,  2.1721e-02,\n","         3.1975e-02,  1.6815e-02,  2.9494e-02,  1.3951e-02, -7.7606e-02,\n","         5.3551e-02,  2.0262e-02, -5.7480e-03, -6.6765e-02,  1.0141e-02,\n","        -5.4580e-02, -4.1208e-02, -1.5377e-02,  4.3893e-02, -3.9962e-02,\n","        -2.9657e-02, -5.2231e-02, -6.7670e-02,  3.6412e-02,  3.5641e-02,\n","         6.8373e-02,  4.4387e-02,  5.5760e-02, -4.1388e-02, -7.2030e-02,\n","        -3.7016e-02,  4.5959e-02, -7.9084e-02,  5.2152e-02, -5.5527e-02,\n","        -4.0075e-02,  1.9941e-02, -5.2852e-02,  7.9927e-02, -5.0089e-02,\n","         1.3913e-02,  6.8542e-02, -1.6098e-02, -6.7242e-02,  5.9175e-02,\n","         5.6556e-02,  4.2203e-02, -1.0601e-02,  6.3358e-02,  3.8920e-02,\n","        -5.1229e-02,  1.5434e-02,  7.3291e-02,  6.1820e-02,  3.3197e-02,\n","        -2.7966e-02, -4.0191e-02, -2.2664e-02,  3.0188e-02,  4.6198e-02,\n","         3.3766e-02,  5.1933e-02,  3.1745e-02, -1.1767e-02,  2.8249e-02,\n","         8.2001e-02, -1.9603e-02, -8.0194e-03,  3.0291e-02,  4.4265e-02,\n","        -2.1625e-02, -2.5865e-03,  1.2335e-02,  2.5508e-03, -8.5573e-03,\n","        -1.6211e-02,  4.5978e-02, -7.5991e-03, -7.1708e-02,  7.6919e-02,\n","         3.6929e-02, -2.3474e-02, -1.5962e-02,  2.6552e-02, -1.5266e-02,\n","        -4.5554e-02, -4.2228e-02, -7.8359e-02,  1.7261e-02,  5.5646e-02,\n","        -2.3070e-02, -2.2337e-02, -6.6260e-02, -6.0420e-02,  1.9018e-03,\n","        -7.0690e-03, -5.2632e-03, -3.9829e-02, -6.2983e-02, -1.2837e-02,\n","        -8.4289e-03, -2.4311e-02,  6.5791e-02, -2.9674e-02, -2.1348e-05,\n","         7.7896e-02, -1.7608e-02, -1.1894e-02, -5.9099e-02, -5.5669e-02,\n","         7.8477e-02, -5.7093e-02,  7.6940e-02, -5.4495e-02, -6.6810e-02,\n","        -5.2483e-02, -2.2616e-02,  3.2847e-02,  8.3392e-02,  4.4658e-02,\n","         7.6976e-02, -2.7055e-02, -3.5262e-02,  2.9261e-02, -4.6856e-02,\n","        -1.2952e-02, -4.2341e-02, -5.6167e-02, -7.3214e-02, -9.5352e-04,\n","        -7.2430e-02, -7.2300e-02, -1.5100e-02,  1.1775e-02,  3.5604e-02,\n","        -2.5344e-03, -3.0634e-02, -6.0324e-02,  6.6998e-02,  6.1179e-02,\n","        -2.0943e-02, -7.5090e-02,  6.8682e-02,  5.9035e-02,  2.1743e-02,\n","         7.7679e-02, -7.9989e-02, -1.3372e-02, -3.8626e-02, -5.0817e-02,\n","        -2.2587e-03, -1.1749e-02, -4.4088e-02, -6.9089e-03, -2.0854e-02,\n","         5.4099e-02, -3.0293e-02,  7.7136e-02,  5.1912e-02, -1.4798e-02,\n","         6.6239e-02,  4.3070e-02, -9.9125e-03,  1.4182e-02, -4.4315e-02,\n","         3.5854e-02, -6.5996e-02,  1.3821e-02, -4.9843e-02,  5.7525e-02,\n","        -7.4607e-02, -8.0003e-02, -7.1075e-03, -2.7014e-02,  5.0645e-02,\n","         4.6072e-02, -2.6695e-02,  2.0414e-02,  7.8898e-02,  3.8013e-02,\n","         2.5061e-02,  1.1329e-02, -6.9474e-02,  6.9940e-02, -7.5322e-02,\n","        -1.6792e-02, -7.7157e-02,  5.8281e-03,  3.2934e-02, -1.5292e-02,\n","        -3.7501e-02, -2.0854e-02, -6.5764e-02, -5.3730e-02,  7.6567e-02,\n","        -5.1757e-03,  7.1382e-02,  4.0700e-02, -9.7637e-03, -7.6930e-02,\n","        -1.2094e-02,  1.3007e-03, -4.4852e-02,  5.2981e-02,  3.8163e-02,\n","        -4.5685e-02, -8.3187e-02, -5.8737e-02,  4.5660e-02, -3.4251e-03,\n","         4.6328e-02,  2.1245e-02,  6.6938e-03, -6.6589e-02,  5.9793e-02,\n","        -5.1512e-02, -8.3193e-02,  7.4778e-02,  3.6468e-02,  5.5328e-03,\n","        -7.1988e-02, -2.1900e-02, -2.4671e-02, -3.0293e-02, -6.4208e-02,\n","        -8.1219e-02,  6.5251e-02,  3.7973e-02,  3.4007e-02, -7.7248e-02,\n","        -2.6542e-02,  2.9929e-02, -2.3391e-02,  1.2806e-02, -4.9843e-02,\n","         7.6843e-02,  1.5065e-02,  4.6219e-02, -5.9145e-04, -1.9425e-02,\n","         7.5667e-02, -3.4841e-02,  6.0747e-02, -1.2594e-02, -3.8601e-02,\n","         3.2431e-02,  2.7345e-02,  5.1520e-02, -5.2889e-02, -3.6816e-02,\n","        -7.2273e-02, -7.1222e-02,  7.1256e-02, -7.6048e-02, -5.3428e-02,\n","         2.2135e-02, -2.6478e-02, -6.7430e-02,  3.0721e-02, -3.9976e-02,\n","        -3.4472e-02,  1.4671e-02,  7.1059e-02,  4.8465e-02, -7.1259e-02,\n","         7.2041e-02,  8.6009e-03, -7.6110e-02, -5.4372e-02, -2.9435e-02,\n","         7.7766e-02, -7.4417e-02, -1.1403e-02,  8.5320e-03,  3.3124e-02,\n","         7.8008e-02,  8.1386e-02, -7.7497e-03, -8.2230e-02,  5.5302e-02,\n","         6.0546e-02, -6.3736e-02,  9.9093e-04, -3.6861e-02, -5.6759e-02,\n","         4.8713e-02, -2.2766e-02,  5.8807e-02,  7.4583e-02,  1.2244e-02,\n","         2.9311e-02,  2.3893e-02, -2.4173e-02, -7.6250e-02,  7.9497e-02,\n","         2.5558e-02,  6.2725e-02,  5.9027e-02, -3.1477e-02,  7.4732e-02,\n","        -4.7044e-03, -3.3739e-03, -1.7573e-04,  3.8366e-02, -3.5313e-03,\n","         1.2747e-02,  1.4109e-02, -5.5246e-02,  3.5671e-02, -7.1293e-02,\n","        -2.8066e-02,  6.5291e-02, -6.2819e-02, -7.0667e-02, -5.0647e-02,\n","        -6.2653e-02,  5.1748e-02, -2.3871e-02, -5.6334e-02, -6.9892e-02,\n","         5.1055e-02, -1.2462e-02,  5.4011e-02,  7.3809e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.420280\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.506465\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.425320\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.449819\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.285993\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.478484\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.544653\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.340567\n","Best validation accuracy improved from 0 to 0.940583348274231, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.080047\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.311933\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.175867\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.254534\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.235601\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.097689\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.260389\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.223796\n","Best validation accuracy improved from 0.940583348274231 to 0.9594166874885559, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.064497\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.194878\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.146903\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.310539\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.161100\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.151856\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.070012\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.181580\n","Best validation accuracy improved from 0.9594166874885559 to 0.9614999890327454, saving model...\n","\n","Fold number 5 , Valid Accuracy: 0.9614999890327454\n","\n","\n","Final Accuracy Mean: 0.9568167924880981 , Final Accuracy Std: 0.0029204224701970816\n","\n"]},{"data":{"text/plain":["(0.9568168, 0.0029204225)"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["model = MLP(dropout_rate = 0.2)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.00066) \n","criterion = nn.CrossEntropyLoss()\n","\n","kwargs = {'optim': optimizer, 'crit': criterion, \n","          'epochs': 3, 'device': device, 'batch_size': 32, 'model_save': False}\n","\n","cross_validation(model, **kwargs)"]},{"cell_type":"markdown","metadata":{"id":"o4KEtFhgobSR"},"source":["Cross validation is finished and now it is time to test the model on the test dataset"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":1107,"status":"ok","timestamp":1599735287231,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"bE30oneCl2Jp","outputId":"c7b9fa0c-e7f1-486c-9f5c-bf16eae4bea9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set: Loss: 0.0073, Accuracy: 2892/3000 (96.4%)\n","\n"]}],"source":["test(model=model, device=device, test_loader=test_loader)\n"]},{"cell_type":"markdown","metadata":{"id":"DJu6BGRjjJa-"},"source":["Since we are dealing with a multi-class classification (10 classes), let us check"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"MLbuRknip0gf"},"outputs":[],"source":["def confusion_matrix_data(model, device, test_loader):\n","    model.eval()\n","\n","    all_preds = np.array([])\n","    all_labels = np.array([])\n","    with torch.no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            output = model(data)\n","            preds = output.argmax(dim=1, keepdim=True) \n","            # transfer to cpu and reshape preds from [[1],[2],[3]] to [1,2,3]\n","            preds = preds.to('cpu').detach().numpy().reshape(-1)\n","            labels = labels.to('cpu').detach().numpy()\n","\n","            all_preds = np.concatenate((all_preds, preds), axis=0)\n","            all_labels = np.concatenate((all_labels, labels), axis=0)\n","\n","    return all_preds, all_labels"]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"elapsed":1129,"status":"ok","timestamp":1599752087482,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"x9eqkWGmcvJa","outputId":"0ca34127-7e2e-46d7-a267-351132f06640"},"outputs":[{"data":{"text/plain":["array([[301,   0,   2,   0,   1,   0,   1,   0,   0,   1],\n","       [  0, 348,   0,   0,   1,   0,   0,   5,   0,   1],\n","       [  0,   0, 303,   3,   2,   0,   0,   5,   0,   1],\n","       [  0,   0,   1, 274,   0,   2,   0,   2,   3,   3],\n","       [  0,   0,   0,   0, 270,   0,   1,   1,   1,   0],\n","       [  1,   0,   0,   2,   0, 269,   2,   0,   1,   1],\n","       [  1,   3,   0,   0,   3,   0, 299,   0,   2,   0],\n","       [  1,   0,   3,   2,   1,   1,   1, 262,   3,   3],\n","       [  1,   5,   1,   3,   2,   2,   1,   1, 264,   4],\n","       [  1,   0,   0,   0,  14,   0,   0,   5,   0, 302]])"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import metrics\n","\n","preds, labels = confusion_matrix_data(model=model, device=device, test_loader=test_loader)\n","\n","cm = metrics.confusion_matrix(preds, labels)\n","\n","cm"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":596},"executionInfo":{"elapsed":2567,"status":"ok","timestamp":1599752309772,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"WpclKO10k4Uz","outputId":"b09bd774-2875-4e6b-c17b-51a73446a492"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.98      0.98      0.98       306\n","         1.0       0.98      0.98      0.98       356\n","         2.0       0.96      0.98      0.97       310\n","         3.0       0.96      0.96      0.96       284\n","         4.0       0.99      0.92      0.95       294\n","         5.0       0.97      0.98      0.98       274\n","         6.0       0.97      0.98      0.98       305\n","         7.0       0.95      0.93      0.94       281\n","         8.0       0.93      0.96      0.95       274\n","         9.0       0.94      0.96      0.95       316\n","\n","    accuracy                           0.96      3000\n","   macro avg       0.96      0.96      0.96      3000\n","weighted avg       0.96      0.96      0.96      3000\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import seaborn as sns #library for plotting\n","\n","## Plot multi-class metrics and confusion matrix \n","classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","print(metrics.classification_report(labels, preds))\n","\n","fig, ax = plt.subplots()\n","sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n","            cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n","       yticklabels=classes, title=\"Confusion matrix\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9JXxvRsqRfw"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN8UH7Q4FuNcTzUOmhZEe2z","collapsed_sections":[],"name":"3_mlp_mnist_training.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
