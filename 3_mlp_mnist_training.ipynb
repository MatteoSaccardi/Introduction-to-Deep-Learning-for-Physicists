{"cells":[{"cell_type":"markdown","metadata":{"id":"zPTNy_tHUrrI"},"source":["<h1><center> MLP MNIST Training </center></h1>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N_fSAUpAZj9E"},"source":["### Cristiano De Nobili - My Contacts\n","For any questions or doubts you can find my contacts here:\n","\n","<p align=\"center\">\n","\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Instagram_logo_2016.svg/2048px-Instagram_logo_2016.svg.png\" width=\"20\">](https://www.instagram.com/denocris/?hl=it)\n","[<img src=\"https://1.bp.blogspot.com/-Rwqcet_SHbk/T8_acMUmlmI/AAAAAAAAGgw/KD_fx__8Q4w/s1600/Twitter+bird.png\" width=\"30\">](https://twitter.com/denocris) \n","[<img src=\"https://loghi-famosi.com/wp-content/uploads/2020/04/Linkedin-Simbolo.png\" width=\"40\">](https://www.linkedin.com/in/cristiano-de-nobili/)     \n","\n","</p>\n","\n","or here (https://denocris.com).\n","\n","### Useful Links\n","\n","All notebooks can be found [here!](https://drive.google.com/drive/folders/1i3cNfzWZTNXfvkFVVIIDXjRDdSa9L9Dv?usp=sharing)\n","\n","Introductory slides [here!](https://www.canva.com/design/DAEa5hLfuWg/-L2EFFfZLVuiDkmg4KiKkQ/view?utm_content=DAEa5hLfuWg&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink)\n","\n","Collection of references: [here!](https://denocris.notion.site/Deep-Learning-References-0c5af2dc5c8d40baba19f1328d596fff)\n"]},{"cell_type":"markdown","metadata":{"id":"rychVYL5J_-O"},"source":["### Tools used \n","\n","PyTorch, NumPy, sklearn. To perform grid search we will use the Optuna optimization library.\n","\n","### Notebook Outline\n","\n","* information theory background;\n","* write and train a MLP on MNIST using hold-out validation;\n","* implement grid search with Optuna;\n","* learn k-fold cross-validation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p1WdF9dANBDu"},"source":["## Packages and GPU Settings"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"M99bQapvPh-e"},"outputs":[],"source":["%%capture\n","!pip install -q tensorboard\n","!pip install -q optuna "]},{"cell_type":"markdown","metadata":{},"source":["**capture** avoids the printing of installation package garbage and warnings\\\n","**optuna** is a package to optimize hyperparameters (grid search)\\\n","**tensorboard** is a package to visualize data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchsummary in /Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages (1.5.1)\n"]}],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"53Fac-e3rNUw"},"outputs":[],"source":["import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import transforms\n","from torchsummary import summary\n","\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import random\n","import optuna"]},{"cell_type":"markdown","metadata":{"id":"SG9z4Rz_HCPS"},"source":["GPU info e device set up"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1641155537841,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"MjmlP7ZObnce","outputId":"3859c89e-e643-4c5a-929c-cc2b6a7bf65a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: nvidia-smi: command not found\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ozj-Ar_7b_55"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["use_cuda = torch.cuda.is_available()\n","print(use_cuda)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"]},{"cell_type":"markdown","metadata":{"id":"uOyMhZgTHinw"},"source":["## Information Theory Background \n"]},{"cell_type":"markdown","metadata":{"id":"a_h2KilRH39t"},"source":["Basic notions of Information Theory are essential for the understanding of many machine/deep learning mechanisms as we will see in a while."]},{"cell_type":"markdown","metadata":{"id":"CtrqdhkMID39"},"source":["### Self-Information\n","\n","Every event takes with it an amount of self-information. The idea behind self-information goes as follows\n","\n","* if an event always occurs, we associate it with a smaller amount of information. It will not suprise us!\n","* On the other side, a rare event is associated with a huge amount of information. It will suprise us!\n","\n","I am not surprise to see the sunrise every morning (likely event). Instead,  I would be really suprised if tomorrow the Sun will not rise (unlikely event). This amount of surprise or self-information of the event $x$ is quantified by\n","\n","$$I(x) = - \\log p(x),$$\n","\n","where $p(x)$ is the probability of the event $x$. If $p(x)=1$, then self-info is zero. A rare event instead has a huge surpise factor.\n","\n","### Shannon Entropy \n","\n","Here the original paper by Claude Shannon (1948): [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf).\n","\n","In terms of self-info, Shannon Entropy is the average self-information (expected value) over all possible values of X.\n","The entropy for a probability $p(x)$ distribution is\n","\n","$$ S = - \\sum_i p(x_i) \\log p(x_i),$$\n","\n","where we assume we know the probability $p$ for each outcome $i$ and $ \\sum_i p(x_i)=1$. If we use $log_2$ for our calculation we can interpret entropy as *the minimum number of bits it would take us to encode our information*.\n","\n","For continous variables, we can use the integral form\n","\n","$$ S = - \\int  p(x) \\log p(x) \\, dx,$$\n","\n","where now $p(x)$ is taking the role of a probability density function (PDF). Take in mind that a broad probability density has higher entropy than a narrowed one (think about Gaussian distribution vs delta Dirac, which has $S=0$).\n","\n","In both discrete and continous formulation, we are computing the expectation (i.e. average) of the negative log-probability (i.e. self-info) which is the theoretical minimum encoding size of the information from the event $x$. The same formula is usually written as\n","\n","$$S = \\mathbb E _{\\, x \\sim p} \\left[ -\\log p(x) \\right],$$\n","\n","where $x \\sim p$ means that we calculate the expectation with the probability distribution $p$."]},{"cell_type":"markdown","metadata":{"id":"8BEow_oxYSje"},"source":["Let's give an example! \n","\n","<!---\n","  REMIND to change open with uc\n"," https://drive.google.com/open?id=1Y52T3Z4dwRU4Rq5L5bEVYh0d3kU0aVB8\n","--->\n","\n","  <center>  <img src=https://drive.google.com/uc?id=1GaAeK8xIZCVDRb-oHQNUzRuoOprFh1eS \" width=\"700\">  </center> "]},{"cell_type":"markdown","metadata":{"id":"K5E4znWyYWDT"},"source":["Let us say we have to pass a message about what drink Cristiano will order during an event. In general, Cristiano loves [Midori Sour](https://drizly.com/midori-sour/r-b972d5282bec6fe8) , Daiquiri, Spritz and Wine.\n","\n","On Monday, Cristiano loves to listen Jazz and the probability distribution of his choice is: \n","\n","$$P(\\text Midori ) =  P(\\text Daiquiri ) = P(\\text Spritz ) = P(\\text Wine ) = 0.25,$$\n","\n","while the corresponding entropy\n","\n","$$S = - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} = 2$$\n","\n","On Wednesday, he usually meets with some friends after work: \n","\n","$$P(\\text Midori ) = 0.125,\\;  P(\\text Daiquiri ) =0.125,\\;  P(\\text Spritz ) = 0.5,\\; P(\\text  Wine ) = 0.25,$$\n","\n","while the corresponding entropy\n","\n","$$S = - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} = 1.75$$\n","\n","\n","On Thursday, he often goes to an event where cocktail attire dress code is required\n","\n","$$P(\\text Midori ) = 0.95,\\;  P(\\text Daiquiri ) =0.02,\\;  P(\\text Spritz ) = 0.018,\\; P(\\text  Wine ) = 0.012,$$\n","\n","and the corresponding entropy\n","\n","$$S = - 0.95 \\log 0.95 - 0.02 \\log 0.02 - 0.018 \\log 0.018 - 0.012 \\log 0.012 = 0.364$$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":734,"status":"ok","timestamp":1599731724144,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"KReLhZDvICt1","outputId":"3d38218f-ef94-4b5b-cbd3-1b05a3645d78"},"outputs":[{"name":"stdout","output_type":"stream","text":["On Monday, high entropy:  2.0\n","On Wednesday, medium entropy:  1.75\n","On Thursday, low entropy:  0.36407300467232967\n"]}],"source":["# On Monday, all drinks have equal probability to be chose\n","entropy_1 = -0.25*np.log2(0.25)-0.25*np.log2(0.25)-0.25*np.log2(0.25)-0.25*np.log2(0.25)\n","print('On Monday, high entropy: ', entropy_1)\n","\n","# On Wednesday, some are more probable than others\n","entropy_2 = -0.5*np.log2(0.5)-0.25*np.log2(0.25)-0.125*np.log2(0.125)-0.125*np.log2(0.125)\n","print('On Wednesday, medium entropy: ', entropy_2)\n","\n","# On Thursday, one drink is by far the most probable\n","entropy_3 = -0.95*np.log2(0.95)-0.02*np.log2(0.02)-0.018*np.log2(0.018)-0.012*np.log2(0.012)\n","print('On Thursday, low entropy: ', entropy_3)"]},{"cell_type":"markdown","metadata":{"id":"CZh-58l2Qwvu"},"source":["If entropy is high (encoding size $log_2 p(x)$ is big on average), it means we have many message types with small and almost equal probabilities. Hence, every time a new message arrives, you would expect a different type than previous messages. You may see it as a disorder or uncertainty or unpredictability.\n","\n","On the contrary, when a message has much smaller probability than other messages, it appears as a surprise because on average you would expect other more frequently sent message types. Moreover, a rare message type has more information than more frequent message types because it eliminates a lot of other probabilities and tells us more specific information.\n","\n","In the drink scenario, by sending “Wine” on thursday which happens 1.2% of the times, we are reducing the uncertainty by 98.8% of the probability distribution (“Midori, Daiquiri, Spritz”) provided we had no information before. If we were sending “Midori” (95%) instead, we would be reducing the uncertainty by 5% only.\n","\n","If the entropy is high (ex: fair coin), the average encoding size is significant which means each message tends to have more (specific) information. Again, this is why high entropy is associated with disorder, uncertainty, surprise, unpredictability, amount of information. The more random a message is, the more information will be gained from decoding the message.\n","\n","Low entropy (ex: sunrise) means that most of the times we are receiving the more predictable information which means less disorder, less uncertainty, less surprise, more predictability and less (specific) information. This is the Thursday case."]},{"cell_type":"markdown","metadata":{"id":"Fu6ZmWp5UGDa"},"source":["### Cross Entropy\n","\n","It is one of the most used loss functions in machine learning, as it is a measure of the \"distance\" (even though it is not symmetric) between twod distributions.\n","\n","Suppose to have two distributions, the true one $p(x)$ and the estimated $q(x)$. In the language of neural networks, $p(x)$ would be the grond truth (labels in one hot-encoding) and $q(x)$ the outcome of the net, i.e. the one that your machine learning algorithm is trying to match. Cross entropy is a mathematical tool for comparing two probability distributions $p(x)$ and $q(x)$ and it is expressed by the formula \n","\n","$$ H (p,q) = - \\int p(x) \\log q(x)\\,dx.$$\n","\n","If $\\log$ is in base $2$, then cross entropy measures the number of bits you will need encoding symbols from $p$ using the wrong distribution $q$. Subtracting to cross entropy the entropy of $p$, you are counting the cost in terms of bits of using the wrong distribution $q$ (this somehow will be KL-divergence). An important property of cross-entropy is that its value is minimum (and corresponds to $H(p)$) when $p(x)=q(x)$. That is the reason why, during training we want to minimize its value. This corresponds to force the estimated distribution $q(x)$ to be close to the true one $p(x)$."]},{"cell_type":"markdown","metadata":{"id":"60tieif3uc-l"},"source":["### Kullback-Leibler Divergence\n","\n","KL-divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution $h$ we add into the game our approximating distribution $g$. Then we look at the difference of the log values for each\n","\n","$$D_{KL}(h || g) =  \\sum_i h(x_i) (\\log h(x) - \\log g(x)) = \\sum_i h(x_i) \\log \\frac{h(x)}{g(x)}$$ \n","\n","from which\n","\n","$$H(h, g) =  H(h) + D_{KL}(h || g).$$ \n","\n","\n","KL-divergence is the expectation of the log-difference between the probability of data in the original distribution $h$ with the approximating distribution $g$. Again, if we think in terms of $\\log_2$ we can interpret this as how many bits of information we expect to lose when we choose an approximation $g$ of our original ditribution $h$. \n","\n","In the variational autoencoder loss function, the KL-divergence is used to force the distribution of latent variables $q(z | x)$ to be a normal distribution $n(z)$ so that we can sample latent variables from the normal distribution. As such, the KL-divergence is included in the loss function to improve the similarity between the distribution of latent variables and the normal distribution. More about **KL** can be found [here](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) and about **cross-entropy** [here](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)."]},{"cell_type":"markdown","metadata":{"id":"0hbIeSgD_YUw"},"source":["## Training, Validation and Test Sets\n","\n","* **parameters:** weights and biases of a DNN which transform the input before applying the activation function. Each layer has its own set of parameters. The parameters are adjusted through backpropagation to minimize the loss function or in other words are learned during the training process;\n","\n","* **hyperparameters:** unlike parameters, their values are not adapted by the learning algorithm itself. They can be viewed as settings that can be used to control the behaviour of the algorithm. Examples are number of layers, type of architecture, batch size, learning rate, etc...;\n","\n","* **training set:** a set of examples used for learning. It affects parameters and in particular they are optimized according to this set;\n","\n","* **validation set:** a set of examples not seen by the model during training. It is like a mini-test set that provides feedback to the model during training on how well the current weights generalize beyond the training set. It does not impact or adjust weights directly, but providing information about overfitting it can indirectly impact weights if some regularization techniques are applied. In addition, validation set is widely used to tune hyperparameters;\n","\n","* **test set:** a set of examples used at the end of training and validation to assess the predictive power of your model."]},{"cell_type":"markdown","metadata":{"id":"MHSBvKR3QNTV"},"source":["## A Complete Training of a MLP Model on MNIST Dataset\n","\n","In this section, we want to show a complete train of a MLP network . We will show the following pipelines\n","\n","* Import, preprocess and properly normalize training and test data;\n","* Build up the MLP and initialize its weights;\n","* Train and evaluate the model using standard hold-out validation;\n","* Tune hyperparameters with Optuna (one of the many libraries to do GridSearch);\n","* Train and evaluate the model using K-Fold Cross-Validation."]},{"cell_type":"markdown","metadata":{"id":"ZBKPCzq6T781"},"source":["## Train MLP Model with hold-out validation"]},{"cell_type":"markdown","metadata":{"id":"9AE9tPDSt6KK"},"source":["### Import and Preprocess Data"]},{"cell_type":"markdown","metadata":{"id":"hD8f7nPP_zs0"},"source":["MNIST is a dataset for handwritten digit recognition.\n","\n","* The dataset is composed of 60,000 grayscale images\n","  * by default, the dataset is already split into a training set of 50,000 images, while the remaining 10,000 images make up the test/validation set\n","* Each image is composed of 28x28 pixel\n","* Only one digit is present in each image\n","  * thus, we will be classifying digits from 0 to 9 (10 classes)\n","* The digit is centered within the image\n","\n","From **torchvision** we import the MNIST dataset "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"5_MazLV_qDmi"},"outputs":[],"source":["%%capture\n","#Download data\n","train_set = MNIST('./data', train=True, download=True)\n","test_set = MNIST('./data', train=False, download=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1641155589533,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"4JYVj4sWrWv4","outputId":"4a1ca433-0f22-481c-bdc5-582f22bf830c"},"outputs":[{"data":{"text/plain":["(60000, 28, 28)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_set.data.numpy().shape"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["array([5, 0, 4, ..., 5, 6, 8])"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["train_set.targets.numpy()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1641155590805,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"PYW3X_OjjrVt","outputId":"eb1c8b70-a145-4e03-a199-411dfc15df9c"},"outputs":[{"data":{"text/plain":["(28, 28)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# First image in dataset (from targets, we expect it to be 5)\n","img = train_set.data.numpy()[0]\n","img.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1641155592180,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"rCuQNDuWoKSA","outputId":"e5e465d3-9cf4-4a00-fe18-cbc545e9e95a"},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7ff735f512e0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.imshow(img, interpolation=\"nearest\", cmap=\"gray_r\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1641155596536,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"ykqVWzhgoE8t","outputId":"fe8d4dda-49ba-46a7-c96c-a7bdeb32b0dd"},"outputs":[{"data":{"text/plain":["(5, 28, 28)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["imgs = train_set.data.numpy()[0:5]\n","imgs.shape"]},{"cell_type":"markdown","metadata":{"id":"VErobQDOsuie"},"source":["Applying a reshape we can view more than one sample"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1641155600584,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"sVUuKi_hoAvG","outputId":"ad93e7b0-6a60-4842-dc4d-0e4abf812959"},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7ff735f77ee0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAFYAAAD8CAYAAADt0VN/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAimUlEQVR4nO2deXBb133vPwcAsZPgBu7iDpISKWozTUmWFMeKn5VKtSLHTdJp7Pi5Td6bvL64bjJO0nQmnWk7aeyXTDrTaWeS1Fkaj7MntieWoqXxIluSJUqUKG7iBhLcRJEiCJAgie28P0DckLYkiiQuN+EzgyFweXHvT18dnnPuOb9FSCmJE3s0K23AeiUurErEhVWJuLAqERdWJeLCqoRqwgohDgghWoUQ7UKIr6h1n9WKUGMeK4TQAteAh4Fe4Dzw51LKppjfbJWiVou9H2iXUnZKKf3AT4HDKt1rVaJT6bq5gGvW516g9nYnCyHW2uPfsJTSfqcT1BJW3OLYHPGEEJ8DPqfS/dWme74T1BK2F9gw63Me0D/7BCnld4HvwppssfOiVh97HnAIIYqEEHrgU8CrKt1rVaJKi5VSBoUQfw38HtACL0opG9W412pFlenWgo1Ye11BnZTyvjudEH/yUgm1Bq8VRaPRoNHMbTN2u52EhASEEBgMBoLBIL29vaSlpWGz2UhOTkaj0RAOhxkaGsLj8TA8PLxoG9adsBqNBpPJhMFgmHPssccew263YzQayc3Nxe12881vfpPDhw/z0EMPcfDgQbRaLVNTU/zwhz/k3Xff5eWXX160HWtSWKPRiNVqRavVYrVayc3NZcOGDSQmJmIymSgtLSU7O1s5XwjBhg0bMBgMaLVagsEgo6OjPPXUU+zatYvNmzcTCoVwu90MDQ3R0tKCy+W6gwXzs+aE1ev12O128vPz0ev1JCcnU1paSmlpKampqVgsFjZu3Eh+fv5tr9HV1cX4+DilpaWkpaUhhKC3t5fh4WH6+vpwOp0MDQ0tyc41JaxWq6WsrIyPf/zjfP7zn1darRBCeQHKz1shpeSVV17hypUr+Hw+Xn/9daSUDA0NMTExgc/no7Ozk+np6SXZuqaElVIyNjaGz+cjGAyi0+nQ6W79TwiHwwwMDDA9PU0wGCQvLw+9Xk8oFKK5uZlz584RDAaV86PXDAQC+P1+ljoNXXPCejweRkdHGRkZwWKxKGIZjUZlJiClxO/3097ezvj4ONPT0xgMBqxWK0IIurq6aGlpUdXWNSns6dOnmZ6e5oEHHkBKidPp5KmnnqKsrAyAgYEBurq6+PznP8/IyAh+vx+Hw4HD4eDAgQPcuHFDdVvXlLAQEffGjRtcvnyZ6elpwuEwg4ODbNu2Da1WS0lJCT09PdTV1XH9+nU8Hg+hUAin08nk5CThcDgu7O0YGRlhZGSEjo4OwuEwk5OTbN++Ha1WS3FxMW1tbbz77rv4fD5CoRAAw8PDDA8P09i4PEsWa1LYKD6fT3k/OTmJz+dDSsnWrVsRQnD8+PElj+6LZU0LO3vk7u7uJj09neHhYaxWK8XFxWRmZhIOh/F6vctu27pZhHnzzTd55ZVXuHr1KgaDgaqqKqqrq8nNzV0Re9bNsqEQgqSkJHbt2sVf/dVf8ZGPfIRLly7R1dVFa2sr3d3dOJ1Ozp49GwuT5102XNNdwWyklExMTHDlyhWampqUx9yUlBTsdjt5eXmkpaXhcrlwu93Kg4OqBq30i8hGY8xeW7ZskZ/5zGeky+WSPp9PhkIhGQqFZGNjo3zuuefkjh07ZHp6+lLucWG+f9O66Qpmk5ycTFpaGgcOHGDbtm3U1tZSUlLC9PQ0XV1dHDt2jIaGBo4fP648mS2Qe6crmI3b7cbr9fL73/8ej8eDyWTCZDKRmprKli1bcLvdmEwmWltb6enpUWdKttLdgBpdQfQlhJBms1lmZmbKT3ziE/I73/mODIVCMhAIyJs3b8qjR4/KQ4cOqdIVrMsWG0VKqTz2Xrx4kbS0NMLhMBqNBrPZTFlZGaWlpRQUFOByuQiHwzG797qZx96K2XtfAwMDDA4O/rFF6XTk5OSQk5NDVlbWB/bIlsqiW6wQYgPwYyALCAPflVL+qxAiFfgZUAg4gU9IKUeXburCiC7IOBwONm/eTHl5OcXFxWi1WgCmp6fp6OigoaGBS5cuxXzqtZSuIAh8UUp5UQiRCNQJIU4ATwGnpJT/MuMX+xXgy0s3dX40Gg1JSUnk5uaSlZXFjh072LBhg7IHlpKS8kfjg0G8Xi8+n49AIBBzWxYtrJRyABiYee8VQjQT8TI8DDw4c9qPgDdYBmGj29o5OTns27eP++67j/3795OcnExiYuJsuwmFQkxPTzMyMsLU1NSSdwtuRUwGLyFEIbANOAdkzoiOlHJACJERi3vcjoSEBGw2GzU1NZSXl/Poo4+Sm5tLeno6FotF+dMHGB8fp7u7m9dff53GxkbeeustRkZGVLFrycIKIazAr4C/kVJ67rSR977vLdqNU6vVotPpsFgspKenU1lZSW1tLcXFxVRUVJCYmIjZbAYifanP56Onp4fBwUGampp499136ezspLt7Xm/MxbPE+WcCEce3v511rBXInnmfDbTGeh5rNptlVlaWfOCBB+QXvvAF2djYKD0ej/LoOvvV19cn3377bfnZz35W7ty5U2o0mljMkdWbx4pI0/xPoFlK+e1Zv3oV+AzwLzM/X1nsPWZjMBiw2Ww89dRTyuCUn59PSkoKubm5GI1G5dz+/n4GBwc5duwYHR0digOG1+uN6Vz1TiylK3gAeAJoEELUzxz7OyKC/lwI8ZdAD/Bni72B0WjEYDCQnp6OzWbDbrezZ88ecnNzycjIICMjQ9n+llISCAQYGRmhpaWFa9eu8fbbbyvLhbHY0l4IS5kVnObWLvEA+xd73dkUFBRQWlrKk08+SVFREdnZ2bedzAcCAYaGhnj55Zc5evQoFy5cULZqVoJV/UjrcDjYsWMHNTU1JCYmKr4DExMTuN1uWltbmZycRErJ2bNncTqdNDc309/fr9o06m5Z1cKaTCbMZrOybxXdu4o6r507d47x8XHC4TDHjh3D6XQyMTGxwlZHWNXrsVqtVplazSY68obDYaVVhkKhZRuYWOvrsaFQiFAohN/vX2lTFsy6Xt1aSeLCqkRcWJWIC6sScWFVIi6sSsSFVYm4sCqxqh8QYoFOp0Ov15OSkoLJZCIpKQmPx8Pk5CSjo6P4/X5VfLjWvbApKSkUFBTwyU9+kurqah566CFOnTrFlStX+NnPfkZ3d/eSQjtvx7oVVqPRkJ2dzb59+zh8+DAbN24kNTUVjUZDZWUlWVlZbNiwgR/84AccP3485vdfl8JqtVqMRiOlpaXcf//9HDhwALPZrAQhZ2VlYbfbKSgo4M0330Sv1xMIBGK6zLguhc3JyaG0tJRvfetb5OXlKdvfwWCQyclJgsGg4qhcVFREVVUVzc3N+P1+JRhkqawbYbVarRJn++CDD1JTU0NOTg5Wq1U5x+/3MzIyQkNDA1JKDh06xJ49e7Barbz22mu0t7fT3t4eE3vWjbAmkwmbzUZ5eTkPPfQQ+/fvJzU1dY5fQSAQwO12c+bMGfx+P4888gi1tbVUV1czNjZGOByOC/t+nnjiCXbt2sXDDz+MxWKZEwIaJTExkU2bNpGWlsbAwAAXLlygpKSElJQUSkpKaGqKXSK7NS9s1BOmrKyMjRs3YrfbEULMyZThdruprKxUMmt4PB4GBwe5dOkSNpuN1NRUEhIS5rTupbLmhbVYLBQWFlJZWUlZWZkSUh8MBmlra6OtrY2WlhZyc3NJTk5mdHSUvr4+2tralJwF5eXlaDSaO4bjL5RYuBhpgQtAn5Ty0HK6cRYWFrJz506++MUvUlxcjMlkAqCjo4O2tjb+8R//EZ1OR1ZWFj/5yU8YGxvj9OnTtLW14fF4uHbtGocOHUKn01FbW4vL5eLMmTN0dXUteTsoFi32GaAZSJr5/BVUduPU6XQYjUYcDgcbN26koqJCSUni8/lwuVy0tLTQ1taGyWRSplFut5uGhgYmJiaUqZff70cIQXJyMnl5eWzcuJH+/v6VFVYIkQccBP4Z+NuZw6q7cVqtVrKysjh48CBbtmzBbDbj8Xjw+XwMDw9z/vx53nnnHbxeLyMjI/T29s57Tb1eT0VFBY899hjnz59fcpjoUlvsd4DngMRZx1R149TpdFRXV/Poo4+yf/9+7HY7Pp+PX/ziFzQ0NNDe3s7o6Chut3vBk32TyYTdbr9t1o4F2bnYLwohDgFDUso6IcSDi/j+gt04hRBYLBby8/OpqamhoKCAUChEe3s77733HhcvXqSzs1PJE7NQP4Poo3AsBrGlOsU9KoT4E8AIJAkhfgJcF0Jkz7TWbOCW6YDkIrJxarVaHA4H27Zt44EHHgDg3Llz/P3f/z1XrlxRzYl4MSx6oVtK+VUpZZ6UspBIts3/llJ+mj+6cUIM3Tgh4sp5+PBhtm3bBkRC6VtbW2lpaWF8fHzR1539IBGrKZca89iYuXHORqfTYbVa2bVrFyUlJQD09fXR09PDwMDAkq8vhFBcQWNBTISVUr5BZPRHSjlCjNw4Z1NaWkp1dTWVlZWkpKQQDoc5d+4cly9fXvK1o75gN2/epKWlJSYhoGvmyUuv12M0GklISCAcDuPz+WhqaqKrq2vB19JoNFitVnbs2EFeXh6hUAiXy0V9fT0nT55cUrcSZc0Iq9FoFO/DUCjE+Pg4ra2tOJ3OBV/LYDBgt9v56Ec/Sn5+PoFAgI6ODs6fP8/Jkyfn5JpZLGtG2NlE47Ru3ryJx+O56+8JIUhISODQoUPU1NTw9NNPK631pZdeor6+PmZe4GtSWK/Xi8vlYmpq6q7nqjabTVke3LNnDxUVFXg8Hi5fvszVq1dpaGjg+vXrMdueWZPCRhOaTU1N3dX50Y3FqqoqHnvsMfbu3YvZbOaNN97ge9/7HseOHYu5jWtK2OjSXl5eHvv27ePf/u3f5v1OeXk5tbW1fOpTn2LDhg1kZ2fT3NxMc3Mzzz///JLTmd6ONSVsFJPJREZGBg6Hg2AwyMjICKFQCCEEVquVpKQkrFarkku2pqaGsrIyEhMT8fv9dHZ2cvXqVTo7O1Vzr19TwkZjDoxGI+np6fzFX/wFV65c4cSJE3g8HhISEti8eTO1tbVUVlZSUVGh5N/u7++nv7+flpYWjh49SlNTk6pRNWtK2ChCCHQ6HR/60Ieoqqpi9+7djI+Po9PplD2sxMRELBYL4+PjNDU1cfXqVZqbm3n11VcZGhrC6/XGhYXIDuvU1BSTk5PKg0JOTg52u53c3FwmJyeVQSoaOu/1eunt7VWEbWpqor6+flnsXTPC9vX1IaWksbGR0tJSNmyIlLLR6/WkpaXNaX2Tk5OMjY1x9OhRjh8/zm9+85vZAdHLwpoR1ufzMTg4yIsvvsimTZvYunUre/fuVbxchBCMj49z/vx5mpqa6Ozs5NKlS3R3d8fMu2UhrBlh/X4/o6OjnDhxgv7+frxeL/n5+aSnpyvn3Lx5k3feeYezZ8/S2NhIb2/vcgbVzWFVRybe5lxlzUCv189ZP40u+wWDQcLhsJqiru3IxFshpSQYDBIMBlcs6e7dEHeVV4m4sCoRF1Yl4sKqRFxYlYgLqxJxYVViScIKIZKFEL8UQrQIIZqFELuEEKlCiBNCiLaZnynzX2n9sdQW+6/AMSllBbCFiDtn1I3TAZya+XzvsYT0e0lAFzOPxcuZgm8VvOZNwbeUFlsM3AB+IIS4JIT4vhDCwvvcOAFVs3GuVpYirA7YDvyHlHIbMMEC/uyFEJ8TQlwQQlxYgg1LRqvVYjAYSEtLIzExMXYBHkvoCrIA56zPe4Hfsca6gtLSUnnw4EFZV1cnX3jhBVlSUiL1ev3KdQVSykHAJYQonzm0H2gihm6c6enp5OfnU1pais1mW+xl7khycjI5OTlKeH2sWuxSlw3/L/DSTEX6TuB/EuleYuLGmZWVRWZmJsnJydTX1zM2NrZEcz9Iamoq2dnZDA0NxfT6SxJWSlkP3GrBNyZunBUVFVRUVJCXl6dUnIslQggl/2x2djZJSUmr2vE4ZkQzwt8qv+FSEUJgMpmUSPHJyUm8Xi8ejycmOw+r+pG2qqqK7du3q3JtrVZLSkoK1dXV7N69m8HBQVwuFwMDAzFJZbKqhZ1dHTnWZGRk8OUvf5kdO3YQDofp7+9ndDR2AZSrsiuIhgVFHTPUQK/X43A4SE5OJhwOc/369dUzeKmF2WymqKiIjIwMbDZbzOvAAEql+4SEBAKBAJcuXVpyhfrZrMquwGazsX37dtLS0lRpsRaLhZSUFDIyMjCZTEgpGR0djYmLfJRVKazRaCQ/Px+LxYIQgunp6Zh6s9jtdnJycrDZbGi1Wvx+Pz6fL6YJgFdlVxDNQRCN3O7o6FhQrMF8HDlyhA996EOkp6czMjJCT08P4+Pj619Y+GOEoN/vp6enZ9HR2FqtloSEBAoLC8nMzMThcPDQQw9RUVGBRqOhubmZM2fOMDY2dm8IGyUYDHLjxo1b9n+zn+t1Ot2cwmhCCPR6PQaDAZPJxJYtWyguLmbXrl1s3bqVjIzIamZHRwfnzp1jYmIipt3NqhdWq9Vis9kwGAwf+F1RUZFSCmXz5s1kZmYqYfEmk4lHHnlEieYOh8N4PB7F+9Dv96PVahUv71h7JK5KYaP+WVJKrFYrH/nIRygsLKS/v185RwiBw+HAZDIpDsdWqxWj0agkLRsaGmJqagqv10tfXx83b97E5XKRlpZGcnIyUkq8Xi/Dw8Mxd6BblcKGQiElrYjdbufIkSOMjo7O6WeFEJSWlmI0GpX/iOj3BgcHGRoa4sKFC/T399PT00NdXR1jY2N4PB4efvhhysvLlcIV90zSyLa2Nv7pn/6J5uZmNm3aRFlZ2ZxiElGGh4cZGRnh8uXL9Pf3Mzw8TGtrK4FAQIlejHomTk1NkZWVxf79+yktLcVsNjM4OKhapY9VKazf72doaIhLly4p06HbMTY2RltbGzdu3MDj8dzxXIPBgMPhwGq1EgwGcTqdS879cjtWpbBRzp49G6vq8gAkJSVRVVVFYmIik5OTvPfeezHJdXArVuWT13IwMTHB6dOn48LGmmhEo1pe4fessGpzTwobzVuQlZWlVAONNfeksBB5oktMTFRtIf2eFFZxqohxBs7ZLNWN81khRKMQ4qoQ4mUhhHGtuHHq9XoKCgrmpJqOJYsWVgiRC3wBuE9KWQVoiSQ2WxNunDqdTllfUIOldgU6wCSE0AFmoJ9INs4fzfz+R8DHlniPmKPm7m+Upfhu9QH/j4gb0QAwJqU8zip24/T5fDidzpjubd2OpXQFKURaZxGQA1iEEJ9ewPeX3Y3T6/Vy9epVRkdHlQUateJtFx2kLIT4M+CAlPIvZz4/Cewk4rf14KxsnG9IKcvvcKkFBSkvBZ1Oh9lsJjU1FZ1Ox/j4uJLQd4GoGqTcA+wUQpiBSSKCXiDigBzzouqxIJpRPpYbk7djKbW/zwkhfglcBILAJSL5YK2okI1zrbHm8hWsEubtCu7JJ6/lIC6sSsSFVYm4sCoRF1Yl4sKqRFxYlYgLqxKr2q9ALUpLS8nJyaGyshKNRoPf7+fEiROMjo7GLA7hnhNWCEFtbS27d+/mc5/7HDqdjrGxMZ588kkaGxvjwi6GzMxMdu3axac//Wm2bNkCoKTqi/Wj/T3VxxqNRrKzs8nMzCQlRd2tuHtK2ISEBDIyMjCbzTEPIX0/90RXIIRg48aN1NbWcuTIETIzMwkEAgwODvLqq6/y9ttvc/HixfUfQBdLEhISMJlMVFVVsXnzZgoLCzEajUxNTdHZ2UldXR2nT5/G7XbHJIY2yroXNj09nbKyMp599llKSkqwWCwAuN1uXnrpJc6fP8/169djft91L2xubi579uwhKyuLxMRENBoNLpeLhoYG3n777XiBicWg0Wiw2+1UV1eTnJyMXq8nHA7jcrlob2+nq6tLtfzd61ZYIQQ2m43q6mqOHDmCEAK/38/ExASvvfYap0+fVjV/97oUNlq5/vHHH2fnzp1otVolELmuro6GhgacTme8wMRCsVqt5OTk8MlPfpLCwkIgEuJ048YNTp8+TWtr65yYMTVYl8KWlJRQU1Oj1JqRUlJfX88777zDf/3Xfy1LmdV1JWxCQgK5ubls27aN3bt3K/PVsbEx3njjDerq6rh582bMKnnekbvI4vYikaK9V2cdSwVOAG0zP1Nm/e6rQDuRjHGP3GXWuZhkfbPZbPLIkSPy17/+tfR4PDIQCMienh556tQpuWPHDmmz2ZYtaeTd/KP3EclhOFvY54GvzLz/CvDNmfebgMuAgYizXAegXQ5hU1NT5Z49e2RbW5scHR2VgUBAhkIh2dzcLH/84x/LgoICqdFolk3YeRdhpJRvATffd/h2PrCHgZ9KKaellF1EWu79890jFuTn51NRUUFBQQFJSUlKeP3Nmzdpbm5mYmJiWcujLLaPvV1F+lxgdihh78yxD7CYoup3Ys+ePezduxdgjoDXrl3jV7/61bI4ws0m1oPXrdykbzlZlIsoqn4rzGYz2dnZVFdXs2nTJsVTOxgMcunSJa5cuUJvb+/yDFizWOx67PUZ31feV5G+F9gw67w8Iu7zqhHNeFRYWIjdbgciQc5jY2PU1dXR2dkZs1qzC+IuR+1C5g5eLzB38Hp+5n0lcwevTlQevB5++GE5ODgofT6fDAQCMhAIyPb2dvn666/LvLw8aTAYYjVgLWjwmrcrEEK8DDwIpAsheoGvc5uK9FLKRiHEz4nkkQ0C/0dKqWqVMp1OR1JS0pwdgdbWVt566y08Hs+ydwGKXfOdIKX889v86papTKWU/wz881KMultsNhs2mw2j0ThnwOrr66OxsXFBlZZjzZp98tJqtTzzzDPs2rULIQQajUbpR3t7e2loaIjpjsBCWZPCRgM0ysvLycvLQ0pJOBxmYmKCq1ev0tXVhcfjWf4BaxZrcpfWYrGQlZVFcXExOTk5SrKdsbEx3nrrLbq6ulZmJjCLNdli9Xo9iYmJpKamKlU+3W43LS0tvPDCCzFPp7cY1qSwgFKQMvroGg6HCQQCeDyeFe1bo6zJrgCYPQdelaxJYT0eD06nUxmoViNrsivw+/243W7OnDnD9evXaWtrY3R0lI6OjhWbt76feADd4ogH0K0UcWFVIi6sSsSFVYm4sCoRF1Yl4sKqRFxYlYgLqxJxYVUiLqxKxIVVibiwKjGvsEKIF4UQQ0KIq7OOvTBTof6KEOI3QojkWb/7qhCiXQjRKoR4RCW7Vz3zLhsKIfYB48CPZ9KZIoT4H8B/SymDQohvAkgpvyyE2AS8TMTDMAc4CZTN57QRq2VDvV5PYWEhZWVlFBQU4HA4SElJUeJmXS4Xx48fx+PxoNPpKC4u5tq1a/T09OB0OhcSQbP0FHxSyreEEIXvO3Z81sezwOMz7xU3TqBLCBF14zxztxYvFrPZrFTt3LJlC+Xl5UrNg2hBtO7ubsbHx3G73eh0OioqKjCbzWi1Wnp6elZddaSngZ/NvL9rN85Ys337dnbs2MGXvvQlbDabUtynr6+Puro6UlJSsNlsPPvss8p3NBoNDoeDiooKzp49G1N3pCUJK4T4GhEfrZeih25x2i3/zGPhHyuEwGw2Y7fb+dM//VPuv/9+kpOTcbvdOJ1O3nzzTZxOJx0dHVgsFhwOBx//+MfJz89XMh273W76+vpWT3UkIcRngEPAfvnHjvqu3Thj4R9rMplIT0+nsrKS3bt3s3XrVrRaLdevX6e5uZnf/va3OJ1Ouru7SUxMpKamhr1795KZmYnFYkFKycjICC6XK/Z7ZYt04zxAxKPQ/r7zls2NUwghDx48KL/xjW/IsbExOT09Ld1utzx58qR84oknZEZGhkxISJAajUZqNBp54MAB+Y1vfENOTU3JQCAgp6ampMvlks8884zMyspaaHyCam6cX50R78SMB/VZKeX/Xi43ztzcXMrKynj00UfZtGkTFouF9vZ2nE4nv/3tb7ly5Qput1vpM7VaLTU1NVRXVyvunl6vl+PHj9Pa2hqzet9zuJsWq/aLBbRUnU4n9+3bJ1944QU5NDQkg8GgnJ6elr/85S/lc889J202m0xISJjTsg0Gg3zttdeky+WSoVBITk9Py9bWVnn48GFZWFi4Mo7Hqwmj0ciTTz7Jgw8+yIEDBzCZTAwNDXH16lW+//3vU1dXh9frVVqfXq8nJyeHiooKCgsLSU1NBeDo0aOcPXuWU6dOMTU1pYqta0ZYrVaL2Wxmx44dOBwOkpKSGBgY4Nq1a/zud7+jvb2d0dFRwuEwBoMBi8VCbW0thYWFlJeXk5aWRiAQYGhoiLq6Oi5evMjExIR6bkor3Q3cbVdgMplkYWGhbGlpkWNjYzIQCMiTJ0/Kr33tax8Y1DIyMuT27dvlqVOnpNPplMFgUAaDQelyueRPf/pTuXPnTmkymVY2BmG1oNVqMRgMyuQ/FArxox/9iIaGBlJSUqitraWoqIht27aRnZ1NRkYGZWVlSnlVgBs3bnDs2DH6+vpU6wKirBlhoy0hmnxMCKGIV1RUxH333Ud+fj5bt24lOTkZi8WCwWBQ3DwDgYDiQ+v1elX3VFwzwobDYfx+Px6PB6vVitVq5fnnnwf4gEhjY2OMj48zMTGBxWIhKSmJ8fFxent7Y1qD8U6sGWGnp6cZGRnh29/+Nh/+8Ic5ePAgZrOZQCDAyMgI165do6+vj/feew+v14sQgqeffpr8/HwSExOpq6ujqalp2exdM8KGw2F8Ph/vvPMOZrNZKRnl9/u5ceMG9fX1dHV18Yc//IFgMIjVauWJJ54AIi26r6+PwcHBZbN3zQgLkfQjzc3NtLS08O///u/K8dmjcTgcpqSkhKKiIrZv347NZiMYDHL9+vVlyawRZU0JC3MHsduh0+nQ6XRKwLKUksbGRtra2pbLzPW55xUKhT7Qiru7u1VPsDObNddi74bOzk7Gx8dXLI4W1mmLTUlJIScnR5nDrgTrUli73Y7D4UCr1a6YDetS2NTUVHJycuLCxhqfz4fb7VYefdUuPHkr1qWwQ0NDtLW1EQ6HV0RUWKezghs3biCEwOVyIaVUdnLT09OXLZvRumyxgUCA8fFx2tvbGRoaQghBSUkJBQUFmM3mZZktrEthAaampvjFL37B+fPn0Wg0PP7443zsYx+jsLBQtQr1s1mXXQFE8m6dP3+ezMxMqqqqyM3NZefOnYyOjvLzn/+c9vZ2VXMarNsWGwqF6O/vp62tjfr6eoLBIGlpadTW1mK32zEYDOoasNL7XQvd/l7oy2g0SrvdLr/+9a/L1157Tfr9fvnZz35WFhcXq7rnte6jvzUaDQkJCWzZsoWsrCyKioo4deoULpdrKQUl5nXjXPfCqoSqJapjyTCR0tbDK23IbUhnrm0F831hVbRYACHEhflawUqxGNvW7axgpYkLqxKrSdjvrrQBd2DBtq2aPna9sZpa7LpiVQgrhDgwE3DXLoT4ygrasUEI8QchRLMQolEI8czM8X8QQvQJIepnXn8y78VWweOslki9hGJATySGYdMK2ZINbJ95nwhcI1Lb4R+ALy3kWquhxd4PtEspO6WUfuCnRALxlh0p5YCU8uLMey/QzCLj1FaDsLmAa9bnZQu6uxMz0ZjbgHMzh/56Jnb4RSHEvLVXV4Owdx10t1wIIazAr4C/kVJ6gP8ASoCtwADwrfmusRqEXfbaCXdCCJFARNSXpJS/BpBSXpdShqSUYeB73EWZl9Ug7HnAIYQoEkLogU8Br66EISKypfufQLOU8tuzjmfPOu0IcPX9330/K766JSOh+X8N/J7IDOFFKWXjCpnzAPAE0CCEqJ859nfAnwshthLpopzA/5rvQvEnL5VYDV3BuiQurErEhVWJuLAqERdWJeLCqkRcWJWIC6sS/x/ibr/6l1CPJgAAAABJRU5ErkJggg==","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# use this trick\n","multi_img_reshaped = imgs.reshape(5*28, 28)\n","plt.imshow(multi_img_reshaped, cmap=\"gray\") #cmap=\"gray_r\" for black digits"]},{"cell_type":"markdown","metadata":{"id":"pzPVNUawtNt8"},"source":["Normally, digital pixels have values from $0$ to $255$. On the other side, neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. Therefore it is a good practice to normalize input to small values. We will apply the centering procedure, that consist of dividing first all pixel values by the highest value $255$. Then, we subtract the mean value and dividing by the standard deviation.\n","\n","As a first step, let us compute mean and standard deviation of MNIST training dataset:"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680,"status":"ok","timestamp":1641155607419,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"U7oqJm4LuTB7","outputId":"f4a3ff44-888c-434f-d51a-7137886a4fdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train mean and std: 0.130660  0.308108\n","Test mean and std: 0.132515  0.310480\n"]}],"source":["train_set_array = train_set.data.numpy() / 255.0\n","test_set_array = test_set.data.numpy() / 255.0\n","\n","print('Train mean and std: %f  %f' %(train_set_array.mean(), train_set_array.std()))\n","print('Test mean and std: %f  %f' %(test_set_array.mean(), test_set_array.std()))"]},{"cell_type":"markdown","metadata":{"id":"SF-lJXJctxKO"},"source":["Now we go on with three steps:\n","\n","* Set random seeds;\n","* Apply **transforms** ([Official Docs](https://pytorch.org/vision/stable/transforms.html)) which allows a set of common image transformations that can be composed;\n","* Use `DataLoader`, that combines a dataset and a sampler, and provides an iterable over the given dataset. It handles the dataset in mini-batch for Stochastic Gradient Descent."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1641155635002,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"6_4rNQ74t9FF","outputId":"eb3312d5-00f9-409d-d4c1-b35dca548f02"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7ff74b1b50f0>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# set the seed: built-in python, numpy, and pytorch\n","seed = 172\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed) # works for all devices (CPU and GPU)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"fQj-ZUoUvjTC"},"outputs":[],"source":["batch_size = 16\n","\n","transform = transforms.Compose([transforms.ToTensor(), \n","                                        transforms.Normalize( (0.1307,), (0.3081,))])\n","\n","# DataLoader: combines a dataset and a sampler, and provides an iterable over the given dataset.\n","train_loader = DataLoader(MNIST(root = './data', train = True, transform = transform, download=True), batch_size=batch_size , shuffle=False)\n","# Splitting the test images (tot 10k) in valid and test set.\n","valid_loader_tmp, test_loader_tmp = random_split(MNIST(root = './data', train = False, transform = transform, download=True), [7000, 3000]) \n","\n","valid_loader = DataLoader(valid_loader_tmp, batch_size=batch_size , shuffle=False)\n","test_loader = DataLoader(test_loader_tmp, batch_size=batch_size , shuffle=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1641155639747,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"N2FRkLfBA6yW","outputId":"2804aabb-89cc-4e56-da7f-612d3519e3f1"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# check for seed = 172\n","imgs_check = next(iter(valid_loader))\n","np.array_equal(imgs_check[1].numpy() , np.array([1, 4, 6, 5, 9, 7, 3, 4, 7, 0, 2, 1, 9, 7, 6, 4]))"]},{"cell_type":"markdown","metadata":{"id":"nxdgzVqr0Zgg"},"source":["**A good practice** is to normalize the test set using the training normalization parameters (mean and std).\n","\n","We want testing data points to represent real-world data that the network has never seen. If we take the mean and variance of the whole dataset we will be introducing future information into the training explanatory variables (i.e. the mean and variance).\n","\n","Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.\n","\n","To better understand imagine now that we have trained our model and we are on a production where new data keep coming for prediction. We might not get them in mass, but one by one such as in an API call. We do not have the mean and standard deviation of those new data. We only have the mean and std during the training process.  To sum up, the goal is to be as close as possible to real problems. Therefore, during training we should not use any knowledge we get from the test data."]},{"cell_type":"markdown","metadata":{"id":"5ryJmjq3amVy"},"source":["### MLP Model\n","\n","The idea is to introduce a few concepts, build up a pipeline and be as clear as possible. "]},{"cell_type":"code","execution_count":22,"metadata":{"id":"H6UteiKb27Fc"},"outputs":[],"source":["# write here the class for your MLP\n","class MLP(nn.Module):\n","    def __init__(self, dropout_rate = .2):\n","        super().__init__()\n","\n","\n","        self.flat = torch.nn.Flatten() # X comes in as a n x 1 x 28 x 28 -> we need n 784-size vectors (or, a n x 784 matrix). flatten does this\n","        self.layer1 = torch.nn.Linear(784, 64) # 784 = 28 * 28\n","        self.layer2 = torch.nn.Linear(64, 32)\n","        self.layer3 = torch.nn.Linear(32, 24)\n","        self.layer4 = torch.nn.Linear(24, 10)\n","        \n","        self.droput = nn.Dropout2d(p = dropout_rate)\n","\n","\n","    def forward(self, X): \n","        out = self.flat(X)\n","        out = self.layer1(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.layer2(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.layer3(out)\n","        out = torch.nn.functional.relu(out)\n","        out = self.droput(out)\n","        logits = self.layer4(out)\n","        # out = torch.nn.functional.softmax(out) -> this is wrong since we will use NegativeLogLikelihood Loss\n","        # softmax transforms a vector into a simplex (vector of positive nums summing to 1): \n","        # [1,2,3,4] -> [0.1, 0.2, 0.3, 0.4]\n","        # instead we need the logsoftmax\n","        #out = torch.nn.functional.log_softmax(out)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"UUEiwOtSqFx1"},"source":["### Weights Initialization"]},{"cell_type":"markdown","metadata":{"id":"dimObwP6bUgb"},"source":["We will now initialize weights (and biases) using Xavier initialization. If you are more interested in understanding its details, you can find here the original paper by [Xavier and Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n","\n","There are several ways to initialize parmeters. However, the most naively ones might cause vanishing gradient effects during training. Researchers found safer initializations. One of them is Xavier initialization.\n","It sets a layer’s weights to values chosen from a random uniform distribution that is bounded between\n","\n","\n","$$\\pm \\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}, $$\n","\n","where $n_i$ is the number of incoming network connections, or *fan-in*, to the layer, and $n_{i+1}$ is the number of outgoing network connections from that layer, also known as the *fan-out*. We are going to use it for weights initialization (`nn.init.xavier_uniform_(m.weight)`). For biases initialization instead we will adopt a slightly different approach, just to show you how you can play with initializations."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"eUUdTKP7fhFo"},"outputs":[],"source":["def weight_init(m):\n","    torch.manual_seed(seed) \n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n","            bound = 1 / np.sqrt(fan_in)\n","            nn.init.uniform_(m.bias, -bound, bound)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8581,"status":"ok","timestamp":1641155753358,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"22InO0WGtZu_","outputId":"ace35660-be6f-4c11-9e2a-c1794c23f65c"},"outputs":[{"data":{"text/plain":["MLP(\n","  (flat): Flatten(start_dim=1, end_dim=-1)\n","  (layer1): Linear(in_features=784, out_features=64, bias=True)\n","  (layer2): Linear(in_features=64, out_features=32, bias=True)\n","  (layer3): Linear(in_features=32, out_features=24, bias=True)\n","  (layer4): Linear(in_features=24, out_features=10, bias=True)\n","  (droput): Dropout2d(p=0.2, inplace=False)\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model = MLP(dropout_rate = 0.2)\n","#lenet = MLP(dropout_rate = 0.000001)\n","\n","# apply weight init\n","model.apply(weight_init)\n","\n","# put the model on the device\n","model.to(device) \n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1641155757937,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"74o3mkz0vfYR","outputId":"dd596361-8975-4525-ac4a-62af7a442597"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","           Flatten-1                  [-1, 784]               0\n","            Linear-2                   [-1, 64]          50,240\n","            Linear-3                   [-1, 32]           2,080\n","            Linear-4                   [-1, 24]             792\n","         Dropout2d-5                   [-1, 24]               0\n","            Linear-6                   [-1, 10]             250\n","================================================================\n","Total params: 53,362\n","Trainable params: 53,362\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.01\n","Params size (MB): 0.20\n","Estimated Total Size (MB): 0.21\n","----------------------------------------------------------------\n"]}],"source":["# check that the model is working\n","summary(model, (1, 28, 28))"]},{"cell_type":"markdown","metadata":{"id":"cl5aSZSDdC0W"},"source":["To keep under control all steps"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1641155761218,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"vDDrKnst15wX","outputId":"2186c187-f9cd-43e3-820c-d935d4802530"},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[-0.0311,  0.0236, -0.0136,  ...,  0.0019, -0.0206,  0.0245],\n","        [ 0.0326,  0.0269, -0.0080,  ...,  0.0148, -0.0015,  0.0042],\n","        [-0.0238,  0.0115,  0.0241,  ..., -0.0081,  0.0088, -0.0018],\n","        ...,\n","        [ 0.0335, -0.0336,  0.0169,  ...,  0.0233, -0.0081,  0.0204],\n","        [ 0.0124,  0.0176,  0.0163,  ...,  0.0053, -0.0113,  0.0307],\n","        [-0.0352, -0.0239, -0.0170,  ..., -0.0089, -0.0082, -0.0091]],\n","       requires_grad=True)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["torch.save(model.state_dict(), './weights_init')\n","\n","list(model.parameters())[0]"]},{"cell_type":"markdown","metadata":{"id":"ROpfooUCqNrT"},"source":["### Training and Evaluation\n","\n","Let us first define \n","\n","* a function that computes accuracy from logits (applying a softmax function);\n","\n","* a function that computes evaluation.\n","\n","Logits are the unnormalized final scores (predictions) of our model. We must apply softmax to it to get a probability distribution over our classes. If you remind, the LeNet() class returns logits\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1)"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["torch.argmax(torch.tensor([0.2,0.7,0.1])).long() #index of max"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.8000)"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["(torch.tensor([0,1,2,5,0]) == torch.tensor([0,1,2,5,9])).float().mean()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1.6038e-28, 1.0000e+00, 3.6251e-34]])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["softmax = nn.Softmax(dim=1)\n","softmax(torch.tensor([[25, 89, 12]]).float())"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"--ZryiNvxyth"},"outputs":[],"source":["def get_accuracy_from_logits(logits, labels):\n","    softmax = nn.Softmax(dim=1)\n","    argmax = torch.argmax(softmax(logits.float()), dim=1)\n","    pred_class = argmax.long()\n","    acc = (pred_class == labels.long()).float().mean()\n","    return acc\n","\n","def evaluate(net, crit, dataloader, device):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for data, labels in dataloader:\n","            data, labels = data.to(device), labels.to(device)\n","            logits = net(data)\n","            mean_loss += crit(logits, labels).item()\n","            mean_acc += get_accuracy_from_logits(logits, labels)\n","            count += 1\n","\n","    return mean_acc / count, mean_loss / count"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1641156325691,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"TnmsFxJppv1n","outputId":"1d0559af-909b-4d28-af8f-626e9394db1b"},"outputs":[{"data":{"text/plain":["3750"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["len(train_loader) # 3750*batch_size"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["[(0, 'a'), (1, 'b'), (2, 'c')]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["list(enumerate(['a','b','c']))"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"1hotozlhni1N"},"outputs":[],"source":["# The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard.\n","writer = SummaryWriter() # it will output to ./runs/ directory by default\n","\n","def training(model, train_loader, valid_loader, optim, crit, epochs, device, model_save = True):\n","    # set up the model on training phase\n","    model.train()\n","    best_acc = 0\n","    tot_len = len(train_loader)\n","    for epoch in range(1, epochs + 1):\n","        for batch_idx, (data, labels) in enumerate(train_loader):\n","            # transfer data on the device\n","            data, labels = data.to(device), labels.to(device)\n","            #Clear gradients  \n","            # Since the backward() function accumulates gradients, and you do not want to mix up \n","            # gradients between different batches, you have to zero them out at the start of a new batch.\n","            optim.zero_grad() # same as model.zero_grad()\n","            logits = model(data)\n","            loss = crit(logits, labels)\n","            loss.backward()\n","            optim.step()\n","            if batch_idx % 200 == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx * len(data), len(train_loader.dataset),\n","                    100. * batch_idx / len(train_loader), loss.item()))\n","                writer.add_scalars('Loss', {'train': loss.item()}, batch_idx + tot_len*(epoch-1))\n","\n","            if batch_idx % 500 == 0:\n","                _, valid_loss = evaluate(model, crit, valid_loader, device)\n","                writer.add_scalars('Loss', {'valid': valid_loss}, batch_idx + tot_len*(epoch-1))\n","                # set again the model to train mode\n","                model.train()\n","    \n","        valid_acc, _ = evaluate(model, crit, valid_loader, device)\n","        #writer.add_scalar('Accuracy', valid_acc, tot_len + tot_len*(epoch-1))\n","        writer.add_scalars('Acc', {'valid': valid_acc}, tot_len + tot_len*(epoch-1))\n","\n","        if valid_acc > best_acc:\n","          print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, valid_acc))\n","          best_acc = valid_acc\n","          if model_save:\n","            torch.save(model.state_dict(), 'Models/lenet_{}.pt'.format(epoch))\n","\n","    return best_acc\n","    \n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, labels).item() # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"dcNjSHBbEiI0"},"outputs":[],"source":["#!rm -rf runs\n","#!rm -rf Models"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"4kENFWEdZKsh"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: Models: File exists\n"]}],"source":["epochs = 3\n","!mkdir Models\n"]},{"cell_type":"markdown","metadata":{"id":"61YjipGhv2za"},"source":["Let us train the MLP model"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80926,"status":"ok","timestamp":1641156410319,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"xjjff52rZJGE","outputId":"e179e194-376d-455f-ce11-7894eaf6e8f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.519160\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.955020\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.088764\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.834272\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.788240\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.710138\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.504867\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.618955\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.665975\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.376760\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.180071\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.470167\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.468446\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.659052\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.237709\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.705568\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.339401\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.470147\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.406442\n","Best validation accuracy improved from 0 to 0.9128139019012451, saving model...\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.070014\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.483822\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.209130\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.400762\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.348382\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.305544\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.299506\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.249032\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.498614\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.069890\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.123359\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.168366\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.438277\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.214137\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.043107\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.540513\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.265629\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.228538\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.054975\n","Best validation accuracy improved from 0.9128139019012451 to 0.934503436088562, saving model...\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.026855\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.081940\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.065838\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.145191\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.170150\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.249901\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.524863\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.252051\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.114042\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.128974\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.178151\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.209941\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.369434\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.161532\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.066048\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.457015\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.180665\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.367107\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.062075\n","Best validation accuracy improved from 0.934503436088562 to 0.9429223537445068, saving model...\n"]}],"source":["# define loss function\n","criterion = nn.CrossEntropyLoss() # The input is expected to contain raw, unnormalized scores for each class (logits)\n","\n","# define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) \n","\n","kwargs = {'optim': optimizer, 'crit': criterion, \n","          'epochs': epochs, 'device': device}\n","\n","training(model, train_loader, valid_loader, **kwargs)\n","\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"W6QyPq_6H6YO"},"source":["Let us now evaluate the model on the test set"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":569,"status":"ok","timestamp":1641148473803,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"_pBd_Wz83GlS","outputId":"94687a59-0b0e-4057-efbf-ebcd6c767558"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set: Loss: 0.0103, Accuracy: 2842/3000 (94.7%)\n","\n"]}],"source":["# Model with Batch Normalization active\n","test(model, device = device, test_loader=test_loader)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"vKQKAqaI3KGu"},"outputs":[{"data":{"text/plain":["tensor([-0.0569, -0.0634,  0.0530,  0.0657,  0.0440,  0.0353,  0.0581, -0.0976,\n","        -0.0304, -0.0765, -0.0630,  0.0670, -0.0928, -0.0251, -0.0437, -0.0466,\n","        -0.0598, -0.0732,  0.0125, -0.0373,  0.0483, -0.0902, -0.0454, -0.0171,\n","        -0.0824, -0.0630,  0.0451, -0.0501,  0.0148, -0.0854, -0.0656,  0.0338,\n","        -0.0037, -0.0388,  0.0671, -0.0948,  0.0232,  0.0179,  0.0175,  0.0140,\n","        -0.0113,  0.0332, -0.0633, -0.0948,  0.0575,  0.0266, -0.0137,  0.0138,\n","        -0.0260,  0.0137, -0.0920,  0.0590, -0.0730, -0.0383,  0.0643,  0.0564,\n","         0.0228,  0.0274,  0.0089, -0.0783, -0.0435, -0.0028, -0.0399, -0.0731,\n","         0.0610, -0.0361,  0.0450,  0.0100, -0.0283,  0.0336,  0.0283, -0.0131,\n","        -0.0983, -0.0291,  0.0326, -0.0959,  0.0506, -0.0590, -0.0851, -0.0709,\n","         0.0103, -0.0568, -0.0851, -0.0529, -0.0846, -0.0349, -0.0407,  0.0319,\n","         0.0085,  0.0480, -0.0039, -0.0177, -0.0599,  0.0396,  0.0605, -0.0782,\n","         0.0152,  0.0194, -0.0451,  0.0061, -0.0471, -0.0725, -0.1096, -0.0235,\n","        -0.0284, -0.0679, -0.0398, -0.0107,  0.0211, -0.0291, -0.0599, -0.0794,\n","        -0.0497, -0.0811, -0.0779,  0.0485, -0.0630,  0.0591, -0.0190,  0.0633,\n","        -0.0772,  0.0626, -0.0216,  0.0495, -0.0718, -0.0305, -0.0199,  0.0219,\n","        -0.0666, -0.0565, -0.0332, -0.0828, -0.0800,  0.0551, -0.0205,  0.0500,\n","        -0.0283,  0.0189,  0.0494, -0.0485,  0.0142,  0.0350,  0.0017,  0.0132,\n","         0.0252, -0.0415,  0.0687, -0.0729,  0.0279,  0.0256,  0.0003,  0.0330,\n","         0.0819,  0.0886, -0.0301,  0.0836, -0.0224,  0.0585,  0.0479, -0.0067,\n","         0.0311,  0.0503, -0.0475, -0.0332, -0.0425, -0.0673, -0.0328,  0.0418,\n","        -0.0824, -0.0279, -0.0146, -0.0748,  0.0397,  0.0456, -0.0760,  0.0005,\n","         0.0790, -0.0184, -0.0292, -0.0579,  0.0874,  0.0845,  0.0884,  0.0485,\n","         0.0683, -0.0137, -0.0216, -0.0267, -0.0554, -0.0262, -0.0356,  0.0088,\n","        -0.0151,  0.0034, -0.0027,  0.0117, -0.0937, -0.0228, -0.0704,  0.0663,\n","         0.0141, -0.0694, -0.0293,  0.0257, -0.0283, -0.0254,  0.0149,  0.0107,\n","        -0.0376,  0.0605,  0.0396,  0.0245,  0.0597, -0.0051,  0.0804,  0.1081,\n","        -0.0613,  0.0649, -0.0065,  0.0474, -0.0143, -0.0135, -0.0014,  0.0172,\n","        -0.0528, -0.0294, -0.0840, -0.0995,  0.0583, -0.0580,  0.0770,  0.0749,\n","         0.0360, -0.0072,  0.0835,  0.0850,  0.0692,  0.0317,  0.0573, -0.0414,\n","         0.0793, -0.0254,  0.0419,  0.0388,  0.0010,  0.0694, -0.0026, -0.0438,\n","         0.0521, -0.0804, -0.0242,  0.0496,  0.0480, -0.0765, -0.0098,  0.0581,\n","         0.0320, -0.0029,  0.1104,  0.0439,  0.0387,  0.0336, -0.0323,  0.0626,\n","         0.0608,  0.0719,  0.0037,  0.0172,  0.0200, -0.0470,  0.0243,  0.0328,\n","        -0.0075,  0.0142,  0.1112,  0.1002, -0.0338, -0.0187, -0.0533, -0.0780,\n","        -0.0387, -0.0313,  0.0375,  0.0534,  0.0086, -0.0304,  0.0015,  0.0779,\n","         0.0964, -0.0504,  0.0305, -0.0029, -0.0395,  0.0375, -0.0206,  0.0321,\n","        -0.0146,  0.0554,  0.0051,  0.0631,  0.0192,  0.0416,  0.0267,  0.0614,\n","         0.0198,  0.0535,  0.0174, -0.0910, -0.0149, -0.0578, -0.0162,  0.0024,\n","         0.0040,  0.0887,  0.0662,  0.0755,  0.0282,  0.0944,  0.0821,  0.0638,\n","        -0.0018, -0.0192, -0.0035,  0.0262, -0.0945, -0.0222, -0.0569,  0.0092,\n","         0.1113, -0.0189,  0.1139,  0.0147, -0.0028, -0.0682, -0.0017, -0.0262,\n","        -0.0582, -0.0389,  0.0128,  0.0800,  0.0027,  0.0091,  0.0442, -0.0295,\n","         0.0038, -0.0216,  0.0525, -0.0624, -0.0391,  0.0694,  0.0380, -0.0053,\n","         0.0683,  0.0693,  0.0347, -0.0463,  0.0755,  0.0509,  0.1088,  0.1023,\n","         0.0574,  0.0386,  0.0470,  0.0297, -0.0514, -0.0930,  0.0727, -0.0343,\n","        -0.0088,  0.0383, -0.0518, -0.0162, -0.0760, -0.0596,  0.0023,  0.0142,\n","        -0.0170,  0.0130, -0.0156,  0.0833, -0.0713,  0.0234, -0.0090,  0.0171,\n","         0.0786,  0.0615,  0.0477,  0.0145, -0.0021, -0.0854, -0.0837, -0.0693,\n","         0.0445, -0.0121, -0.0209, -0.0910,  0.0088, -0.0736, -0.0867, -0.0600,\n","        -0.0584, -0.0981, -0.0515, -0.0034, -0.0103, -0.0404,  0.0519,  0.0672,\n","         0.0113,  0.0136, -0.0255,  0.0244, -0.0591, -0.0305,  0.0610, -0.0641,\n","        -0.0773,  0.0064,  0.0517, -0.0640,  0.0534, -0.0098,  0.0285,  0.0449,\n","        -0.0543,  0.0348, -0.0458,  0.0376,  0.0575,  0.0785, -0.0498, -0.0333,\n","        -0.0044,  0.0770,  0.0808,  0.0390,  0.0598, -0.0549,  0.0095, -0.0955,\n","        -0.0903, -0.0066,  0.0728, -0.0402,  0.0298, -0.0880,  0.0674,  0.0008,\n","        -0.0704, -0.0771,  0.0152, -0.0835, -0.0326, -0.0135,  0.0321, -0.0020,\n","         0.0870,  0.0243,  0.0158,  0.0672,  0.0906,  0.0334, -0.0264, -0.0724,\n","         0.0198,  0.0111, -0.0220, -0.0125, -0.0176, -0.1035,  0.0326,  0.0003,\n","        -0.0283, -0.0821, -0.0039, -0.0682, -0.0555, -0.0297,  0.0283, -0.0587,\n","        -0.0684, -0.0914, -0.0983,  0.0177,  0.0319,  0.0928,  0.0669,  0.0954,\n","         0.0248, -0.0265, -0.0331,  0.0273, -0.0996,  0.0188, -0.0855, -0.0611,\n","        -0.0034, -0.0802,  0.0556, -0.0798, -0.0083,  0.0487, -0.0305, -0.0805,\n","         0.0450,  0.0423,  0.0266, -0.0360,  0.0195,  0.0115, -0.0745, -0.0131,\n","         0.0443,  0.0583,  0.0531,  0.0095,  0.0064,  0.0048,  0.0094,  0.0111,\n","        -0.0017,  0.0222,  0.0181, -0.0182,  0.0143,  0.0557, -0.0349, -0.0227,\n","         0.0123,  0.0278, -0.0349, -0.0169, -0.0019, -0.0115, -0.0247, -0.0461,\n","         0.0006, -0.0329, -0.0765,  0.0609,  0.0160, -0.0203,  0.0020,  0.0383,\n","         0.0088, -0.0285, -0.0581, -0.1023, -0.0041,  0.0387, -0.0371, -0.0241,\n","        -0.0656, -0.0702,  0.0053, -0.0128, -0.0242, -0.0536, -0.0757, -0.0271,\n","        -0.0227, -0.0384,  0.0492, -0.0542, -0.0367,  0.0553, -0.0136, -0.0013,\n","        -0.0529, -0.0448,  0.0870, -0.0723,  0.0823, -0.0350, -0.0761, -0.0582,\n","        -0.0382,  0.0124,  0.0693,  0.0548,  0.0960, -0.0186, -0.0238,  0.0260,\n","        -0.0617, -0.0278, -0.0577, -0.0704, -0.0875, -0.0152, -0.0870, -0.0928,\n","        -0.0445, -0.0146,  0.0351,  0.0163, -0.0112, -0.0415,  0.0624,  0.0502,\n","        -0.0077, -0.0423,  0.0769,  0.0592,  0.0155,  0.0720, -0.0627,  0.0137,\n","        -0.0180, -0.0408,  0.0122, -0.0151, -0.0582, -0.0216, -0.0359,  0.0398,\n","        -0.0446,  0.0629,  0.0381, -0.0340,  0.0353,  0.0233, -0.0053,  0.0437,\n","        -0.0015,  0.0574, -0.0502,  0.0421, -0.0047,  0.1032, -0.0400, -0.0450,\n","         0.0132, -0.0043,  0.0891,  0.0736, -0.0038,  0.0357,  0.0889,  0.0368,\n","         0.0187, -0.0005, -0.0843,  0.0557, -0.0896, -0.0311, -0.0908, -0.0097,\n","         0.0006, -0.0552, -0.0491, -0.0005, -0.0178, -0.0162,  0.1033,  0.0435,\n","         0.1397,  0.1031,  0.0397, -0.0178,  0.0306,  0.0323, -0.0096,  0.0824,\n","         0.0661, -0.0300, -0.0770, -0.0568,  0.0369, -0.0161,  0.0307,  0.0070,\n","        -0.0076, -0.0809,  0.0454, -0.0682, -0.1063,  0.0342,  0.0104,  0.0014,\n","        -0.0433,  0.0164,  0.0133,  0.0317,  0.0125, -0.0245,  0.1123,  0.0812,\n","         0.0604, -0.0638,  0.0033,  0.0619, -0.0105,  0.0125, -0.0534,  0.0708,\n","         0.0054,  0.0324, -0.0151, -0.0337,  0.0614, -0.0491,  0.0465, -0.0282,\n","        -0.0558,  0.0097,  0.0126,  0.0432, -0.0636, -0.0414, -0.0611, -0.0563,\n","         0.0871, -0.0703, -0.0444,  0.0342, -0.0080, -0.0366,  0.0535, -0.0235,\n","        -0.0329,  0.0032,  0.0568,  0.0340, -0.0848,  0.0574, -0.0059, -0.0904,\n","        -0.0686, -0.0437,  0.0635, -0.0887, -0.0263, -0.0065,  0.0159,  0.0599,\n","         0.0656, -0.0223, -0.0899,  0.0474,  0.0505, -0.0643,  0.0013, -0.0394,\n","        -0.0528,  0.0513, -0.0253,  0.0490,  0.0607, -0.0022,  0.0136,  0.0099,\n","        -0.0384, -0.0905,  0.0652,  0.0113,  0.0485,  0.0448, -0.0457,  0.0605,\n","        -0.0190, -0.0178, -0.0145,  0.0249, -0.0164,  0.0002,  0.0011, -0.0687,\n","         0.0218, -0.0853, -0.0430,  0.0502, -0.0756, -0.0830, -0.0632, -0.0770,\n","         0.0356, -0.0398, -0.0730, -0.0847,  0.0368, -0.0267,  0.0397,  0.0595],\n","       grad_fn=<SelectBackward0>)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["list(model.parameters())[0][0]"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1641148551648,"user":{"displayName":"Cristiano De Nobili","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZpMUz0UjFumTuheFxMf484NA45i0hBZh7Sahq3Q=s64","userId":"14225405973604893991"},"user_tz":-60},"id":"9m8rzjnKmU9P","outputId":"33200abc-bbc8-4be5-9c59-1a4c1d60465c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["model_ = MLP(dropout_rate = 0.2)\n","#model_.cuda()\n","model_.load_state_dict(torch.load('./weights_init'))"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Qydnfz95mfrS"},"outputs":[{"data":{"text/plain":["tensor([ 3.8584e-03, -1.5147e-02,  6.0446e-03,  3.2745e-02,  8.0632e-03,\n","        -1.7842e-02,  9.6149e-03, -1.3357e-02,  2.2955e-02, -9.3721e-03,\n","        -2.1160e-02,  3.3052e-03,  3.5138e-02,  1.4077e-02,  2.6295e-02,\n","        -2.7852e-02, -2.8208e-02,  1.4668e-02, -1.1731e-05,  2.0231e-03,\n","         2.5752e-02,  6.5711e-03, -3.4762e-02, -8.0566e-03, -1.6671e-02,\n","         2.7585e-02, -4.0031e-03, -1.8622e-02,  2.6973e-02, -9.3839e-04,\n","         1.2551e-02,  1.1226e-02, -1.8753e-03, -1.5022e-02, -2.0468e-02,\n","         2.3832e-02,  1.1744e-02, -3.3191e-02, -3.1611e-02, -2.4949e-02,\n","        -6.8206e-03,  2.9384e-02, -2.9626e-02,  1.9541e-02, -2.0918e-02,\n","         3.5110e-02, -1.6200e-02,  4.1499e-03, -1.6383e-02,  7.5244e-03,\n","         1.6329e-02,  1.4245e-03, -3.1783e-03,  7.7838e-03, -5.3412e-03,\n","         1.1802e-02, -2.5361e-02,  1.9863e-02, -2.9489e-02, -2.2563e-02,\n","        -8.3935e-03, -2.2526e-02,  1.2433e-02, -2.7012e-02, -1.8162e-02,\n","         2.5265e-02, -2.0538e-02,  2.0758e-02, -3.3734e-02,  1.3095e-03,\n","         1.1126e-02,  7.8477e-05,  1.9818e-02, -1.1748e-02, -3.6970e-03,\n","        -1.2330e-02, -1.1421e-02, -2.3355e-02,  3.2355e-02,  2.9099e-03,\n","        -3.4840e-02,  2.8562e-02,  2.9858e-02,  3.1980e-02,  2.4100e-02,\n","        -1.8219e-02,  6.6848e-03, -1.3738e-02, -1.3464e-02,  1.4248e-02,\n","         1.9906e-02,  3.5234e-02,  8.6688e-03, -2.6286e-02, -6.0752e-03,\n","         1.9210e-02, -1.5164e-02,  2.3207e-02,  1.6599e-02,  3.3968e-02,\n","         4.6509e-03, -5.5766e-03,  3.5117e-02, -3.4988e-02,  2.5742e-02,\n","        -2.4892e-02,  3.1579e-02, -1.4925e-03,  3.1502e-02,  1.6064e-02,\n","        -7.7121e-03,  1.5774e-02,  3.1201e-02,  8.7758e-03, -1.2837e-02,\n","        -3.6809e-03, -2.8614e-02,  1.8884e-03, -1.8255e-02,  5.9511e-04,\n","         1.4316e-02,  1.3977e-02, -1.4308e-02, -1.4601e-02, -1.6200e-02,\n","        -5.5690e-04, -9.4259e-03,  2.7085e-02,  1.5149e-02, -3.2412e-02,\n","         3.0233e-02,  2.3974e-02, -2.1850e-02,  3.8322e-03,  2.2217e-02,\n","         3.0997e-02, -1.9471e-02,  1.5068e-03,  2.0195e-03,  1.7882e-02,\n","        -1.3862e-03,  2.4555e-02,  1.6807e-02,  1.0889e-02, -3.4818e-02,\n","        -2.9501e-02, -1.3347e-02, -3.4979e-02, -1.0844e-02,  3.9837e-03,\n","        -3.3724e-02, -3.4686e-02,  1.7349e-02, -8.6799e-03, -3.5147e-02,\n","        -1.5315e-02, -2.4054e-02,  1.6115e-04, -8.5922e-03,  9.3547e-03,\n","         2.6706e-02, -8.2413e-03,  3.0672e-02,  3.2702e-02,  1.3714e-02,\n","         2.6066e-02, -1.0304e-02,  1.7722e-02,  2.2920e-02, -2.1747e-02,\n","         1.0270e-03,  1.8900e-02,  2.5629e-02, -6.6705e-03,  2.5580e-03,\n","         1.1091e-02,  2.8606e-02, -2.8511e-02, -1.7172e-02, -4.8842e-03,\n","         1.9764e-02, -2.3524e-02,  3.4648e-02, -2.7528e-04,  8.1311e-03,\n","        -1.9039e-02, -3.0209e-02,  1.7343e-02,  3.5256e-03,  1.4680e-02,\n","         1.4510e-02,  2.4063e-03, -6.0882e-03, -4.8955e-03, -2.9184e-02,\n","        -2.4852e-02, -6.9952e-03, -5.5304e-03,  1.6729e-02,  2.3266e-02,\n","         3.0698e-02, -1.3216e-02,  7.9454e-04,  2.0991e-02,  2.9177e-02,\n","         1.7952e-02,  9.1745e-04,  2.9839e-02,  1.5806e-02,  2.9712e-02,\n","        -1.9958e-02,  2.8042e-02, -3.3021e-02,  6.1178e-03, -1.4191e-03,\n","         2.6315e-02,  2.9707e-02, -3.1906e-03,  1.8339e-02,  1.8050e-03,\n","         1.2968e-02,  2.7986e-02, -3.5069e-02, -1.4588e-02, -2.9113e-02,\n","        -2.8669e-02,  1.5388e-02,  1.2752e-02,  3.3673e-04, -2.8050e-02,\n","        -2.6942e-02,  2.9983e-02,  8.0138e-03,  7.2005e-03, -4.1606e-03,\n","         1.1963e-02, -2.4810e-02, -1.4419e-02,  1.8603e-02, -1.6108e-02,\n","         1.6544e-02, -3.4756e-02,  3.3150e-02, -2.5047e-02,  1.8389e-02,\n","        -2.0929e-02, -9.5333e-03,  2.6680e-02, -2.8254e-02, -1.8729e-02,\n","        -2.1888e-02,  1.9006e-02,  2.9372e-02,  2.1869e-02,  2.5168e-02,\n","        -1.3905e-02, -1.7536e-02,  2.6888e-02, -3.8728e-03, -2.6829e-02,\n","         1.9429e-02,  1.0241e-02, -2.8337e-02, -1.6245e-02,  3.0778e-02,\n","         1.1307e-02, -2.7390e-02,  2.1107e-02,  4.1476e-03, -6.2690e-03,\n","        -7.1150e-03, -3.1403e-02, -8.3027e-03,  6.2327e-03,  1.7352e-03,\n","         1.3042e-02, -9.4626e-03,  1.6213e-02,  1.8789e-02,  3.0405e-02,\n","        -1.0532e-02,  2.9275e-03,  6.3632e-03, -1.5529e-02,  6.2185e-03,\n","         1.1515e-02, -3.4152e-02,  1.3974e-02,  1.0889e-02,  2.5293e-02,\n","         2.1852e-02,  5.3180e-03, -1.6344e-02, -1.4212e-02, -3.5415e-03,\n","         1.7022e-02,  2.2589e-02,  2.9868e-02, -2.6119e-02,  2.4958e-03,\n","         2.0960e-02, -2.3880e-02,  2.7720e-02, -3.8130e-03,  1.7925e-02,\n","         2.9334e-02,  1.2863e-02, -5.8956e-03,  9.2603e-03, -1.9241e-02,\n","         7.6427e-03,  2.5986e-03, -1.8516e-02,  1.7890e-02, -3.1939e-02,\n","        -3.3275e-02,  1.1002e-02,  1.6016e-02,  1.5419e-02, -1.6579e-02,\n","        -2.7068e-02, -3.4294e-03, -7.8657e-03,  1.7892e-02, -1.6967e-02,\n","         1.8142e-02,  1.2809e-02,  1.0943e-02,  1.0358e-02, -2.6363e-02,\n","         2.9759e-02,  4.0178e-03, -1.1628e-02, -2.8701e-02, -1.0754e-02,\n","        -3.0473e-02, -1.5339e-02, -4.0459e-03,  3.3670e-02, -1.5281e-02,\n","        -1.6886e-02,  1.8841e-02, -2.4926e-02,  2.8408e-02, -3.3369e-02,\n","        -2.0155e-02,  4.4431e-03,  2.5677e-02,  9.2399e-03, -7.9605e-03,\n","         1.0137e-02,  8.5798e-03, -2.6149e-02,  7.1864e-03,  8.9898e-03,\n","        -1.3113e-02, -5.9342e-03,  1.6030e-02,  1.7257e-02,  2.1589e-02,\n","        -1.8366e-02,  1.9015e-02,  5.0671e-03, -2.6493e-02, -2.9033e-02,\n","        -9.5683e-03, -2.8204e-02,  2.3850e-02,  1.5214e-02,  1.3845e-02,\n","        -1.7054e-02, -2.6634e-02, -2.0273e-02, -3.5112e-02, -1.1098e-02,\n","        -1.9491e-02,  2.7062e-02, -3.4777e-02, -3.3277e-02, -1.1610e-02,\n","        -4.5769e-03,  5.1150e-03, -2.6911e-02,  1.2429e-02, -2.2596e-02,\n","        -1.0546e-02, -1.7784e-02,  1.1119e-02, -2.1689e-05, -2.6995e-02,\n","        -5.3577e-03,  9.7484e-04, -2.5771e-02, -1.3226e-02, -7.3324e-03,\n","        -2.8965e-02,  1.6601e-02, -4.8958e-03,  1.2841e-03,  1.8215e-02,\n","         1.8276e-02,  3.7467e-03,  2.2931e-02, -3.5624e-02, -2.5418e-02,\n","        -1.4596e-02, -7.2351e-03,  3.2727e-02, -2.2290e-02,  2.4928e-02,\n","        -3.4829e-02, -3.3408e-02, -1.8675e-02, -1.0687e-02,  3.5709e-02,\n","         3.4642e-02, -3.0056e-02, -1.1385e-02,  1.5237e-02, -1.8210e-03,\n","         3.0185e-02, -6.9405e-03,  1.7940e-02, -1.3206e-02, -3.4496e-02,\n","         1.8931e-02, -2.6358e-02, -3.2856e-02, -3.5113e-02, -2.1080e-02,\n","        -2.8269e-02, -2.7607e-03, -1.5931e-02,  2.9666e-02, -3.3079e-02,\n","        -3.4267e-02, -2.0124e-02, -2.5572e-02,  1.0703e-02,  3.1340e-02,\n","         1.2611e-03,  1.4902e-02, -1.8493e-02, -3.0800e-02,  2.0623e-02,\n","         1.4318e-02, -2.2427e-02,  2.9254e-02, -1.5023e-02,  1.4498e-02,\n","        -2.3289e-02, -3.2842e-03,  2.2900e-02,  1.3898e-02, -1.0701e-02,\n","        -4.9064e-03,  2.1388e-02, -3.3262e-02,  2.0167e-02,  3.0711e-04,\n","        -1.6947e-03,  5.0930e-03,  2.2755e-02,  1.8579e-02, -3.3382e-02,\n","         3.2004e-02, -2.9686e-02,  2.5697e-02,  3.5631e-02,  2.4014e-02,\n","         3.2966e-02, -2.9170e-02, -2.5199e-02, -1.0657e-02, -3.4496e-02,\n","         2.1285e-02, -1.8560e-02, -6.1822e-03,  2.7577e-02,  1.0345e-02,\n","        -5.3202e-03,  1.1276e-02,  5.4580e-03, -3.0424e-02,  2.3996e-02,\n","        -2.7171e-02, -2.0628e-02,  1.9313e-02,  7.6540e-03,  2.7576e-02,\n","        -1.2268e-02, -1.9656e-02,  1.7110e-03, -2.9270e-02,  1.6684e-02,\n","         1.7765e-02, -2.1663e-02, -1.3154e-02,  2.4070e-02, -3.3156e-02,\n","        -8.0330e-03, -2.9190e-02,  1.3569e-02, -1.8396e-03,  2.4244e-02,\n","        -2.5821e-02, -1.7383e-02, -1.5493e-02,  5.2570e-03,  3.3988e-02,\n","        -1.0873e-02,  3.0308e-02, -3.5584e-02,  1.2653e-02, -1.6159e-02,\n","        -2.5371e-02,  5.4726e-03,  1.9133e-02, -5.0670e-03, -1.8050e-02,\n","        -2.4044e-02,  2.6434e-02,  1.6562e-02,  2.3094e-02,  1.4223e-02,\n","         2.7445e-02,  3.5664e-02, -3.4172e-02, -5.5967e-03,  1.6890e-02,\n","        -1.8378e-02,  2.8343e-02,  3.5670e-02, -4.8020e-03,  1.7953e-02,\n","        -1.9432e-02,  1.2921e-02, -3.1003e-02, -6.0335e-03,  1.1875e-02,\n","        -3.4498e-02,  3.1079e-02,  1.4211e-02, -1.5234e-02,  3.1039e-03,\n","        -2.6948e-02, -1.4013e-02, -8.3324e-04, -1.7722e-02,  3.0036e-02,\n","        -1.0824e-02,  2.9195e-02, -1.2276e-02,  1.0500e-02,  1.8182e-02,\n","         2.9127e-02, -9.0392e-03,  2.5399e-02, -2.2795e-02, -3.2461e-02,\n","         5.5762e-03, -1.8631e-02,  3.3142e-02,  9.4400e-03,  2.6210e-02,\n","         2.7747e-02,  1.5525e-02, -3.4328e-02, -1.0902e-02, -6.0400e-03,\n","        -2.7220e-02,  1.9097e-02, -1.7082e-02, -2.8184e-02,  7.5177e-03,\n","        -1.6000e-02,  7.0846e-03, -5.1811e-03, -2.0556e-03,  2.8802e-02,\n","         3.5027e-02,  2.3215e-02, -2.0447e-02, -2.5739e-02,  2.6081e-02,\n","         2.5381e-03,  1.2933e-02, -1.6743e-02,  5.5615e-03,  2.6443e-02,\n","        -2.2154e-02,  2.2116e-02, -3.2579e-04, -1.8192e-02,  1.9357e-02,\n","        -3.1090e-02, -2.0684e-02, -2.1069e-02, -1.8279e-02,  2.1097e-02,\n","        -9.5968e-03, -3.2477e-02, -1.4625e-02, -2.9330e-03, -2.6835e-02,\n","         4.7655e-03,  1.2070e-02,  3.4063e-02,  1.1828e-02,  2.2324e-02,\n","        -1.6741e-02,  3.1817e-02,  1.2498e-02, -1.4929e-02, -3.4274e-02,\n","        -2.3317e-02, -2.8403e-02,  1.5718e-02, -6.3409e-03, -3.4364e-02,\n","        -5.0114e-03, -1.2228e-02, -2.5454e-03,  1.2049e-02,  2.4749e-03,\n","         2.1863e-02,  3.5685e-02,  1.0521e-02,  1.3235e-02, -3.3462e-02,\n","         9.4800e-03,  2.8086e-02, -7.5498e-03, -3.0236e-02,  1.4481e-02,\n","         1.1450e-02, -3.3957e-02,  3.3621e-02,  3.4883e-02, -1.5557e-02,\n","         2.4778e-02, -2.4663e-02, -3.9030e-03, -1.5756e-02, -1.6498e-03,\n","         1.0696e-02,  2.7580e-02,  9.1484e-03, -3.3414e-02, -3.2986e-02,\n","         1.0794e-02, -2.0603e-02,  1.1709e-02,  6.8686e-03,  2.2491e-02,\n","         1.2847e-02, -1.3505e-02, -2.9073e-02,  2.8611e-02,  3.5408e-02,\n","        -2.5718e-02, -2.1509e-02, -2.6490e-02, -9.9611e-03,  1.1551e-02,\n","         1.1026e-02, -3.5244e-02,  1.8201e-02, -2.7933e-03,  2.1130e-02,\n","        -2.5651e-02,  2.2395e-02, -3.3641e-03, -3.2095e-02,  3.2719e-02,\n","        -2.5848e-04,  2.8373e-02,  2.0364e-02,  1.6092e-03,  1.4433e-02,\n","        -2.0178e-02, -1.0290e-02,  2.5395e-02, -3.1510e-02, -2.5192e-02,\n","         4.9518e-03,  1.7227e-02,  3.3171e-02, -1.3843e-02,  4.2568e-03,\n","        -9.7735e-03, -5.8809e-04, -1.4592e-02,  6.9649e-03,  1.0165e-02,\n","         9.9293e-03, -2.2645e-02, -3.9524e-03, -1.1383e-03,  3.3327e-02,\n","        -3.3700e-02, -2.2459e-02, -5.8538e-03,  1.9976e-02, -1.1269e-02,\n","         3.2340e-02,  2.2891e-02, -2.9509e-02, -3.0802e-02,  1.7386e-02,\n","         2.6359e-02, -2.6893e-02,  1.1684e-02, -3.0663e-02, -1.1857e-02,\n","         2.2040e-03,  1.6755e-02, -8.4967e-03,  2.5963e-02,  2.1731e-02,\n","         1.6498e-02,  1.0123e-02,  2.1199e-02,  1.7929e-02,  2.4197e-02,\n","        -1.8968e-03,  2.1219e-02,  2.4091e-02, -6.9748e-03, -4.4054e-03,\n","         9.8444e-03, -2.6452e-02,  1.4378e-02, -1.8664e-02, -3.5441e-02,\n","         3.4231e-03,  2.6782e-03, -2.4970e-02, -5.2940e-03,  2.7580e-02,\n","         2.1335e-02,  2.2688e-02, -1.7031e-02,  1.3298e-02, -2.5266e-02,\n","        -1.9044e-02, -1.4464e-02, -1.1980e-02,  1.9699e-02, -2.3381e-02,\n","         4.8940e-03,  2.6635e-02,  2.1627e-02,  4.6700e-04, -1.5034e-02,\n","         2.4017e-02,  8.5050e-03, -1.9029e-02,  2.2378e-02,  2.5679e-03,\n","        -2.0490e-02, -3.1962e-03,  2.3738e-02,  2.1515e-02,  1.5912e-03,\n","         3.4963e-02,  9.8850e-03, -1.9366e-02,  2.4769e-03, -1.9395e-02,\n","         3.4415e-02,  3.1107e-02, -8.1006e-03,  5.2534e-03, -3.5309e-02,\n","        -2.5771e-02, -1.3912e-02, -1.7009e-02, -1.6153e-02,  3.3704e-02,\n","         5.5156e-03, -7.9589e-04,  2.3012e-02,  1.0137e-02],\n","       grad_fn=<SelectBackward0>)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["list(model_.parameters())[0][0]"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"DoSdmZ-BblFi"},"outputs":[{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-318933b35a3ebdda\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-318933b35a3ebdda\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Start tensorboard.\n","%load_ext tensorboard\n","%tensorboard --logdir runs/"]},{"cell_type":"markdown","metadata":{"id":"2VEH5WiYLYjB"},"source":["## Hyperparameters Tuning with Optuna\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zJJo9vkBl2Qv"},"source":["Hyperparameter optimization is an important issue in machine learning. Parameters are learned during training, as we have seen so far. Hyperparameters are instead setup before training and they do not take part to backpropagation. Different hyperparameter setups can evolve in different performances. Therefore it is important to explore the landscape of hyperparameter combinations and find out the best one according to a choosen metric.\n","\n","[Optuna](https://optuna.org/) is an open source hyperparameter optimization framework to automate hyperparameter search. Optuna is framework agnostic. You can use it with any machine learning or deep learning framework.\n","\n","The main objects of an Optuna pipeline are\n","\n","* a **trial** corresponds to a single execution of the objective function and it is internally instantiated upon each invocation of the function;\n","\n","* **suggest methods** are called inside the objective function to obtain parameters for a trial;\n","\n","* a **study object** is created to start the optimization.\n","\n","The main idea is to do a lot of small trainings and see their performance to understand the best choice for hyperparameters that will be later applied for the larger training."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"SXgEz_vuPAQp"},"outputs":[],"source":["# let us define our objective function\n","def train_mnist(trial):\n","\n","  # configuration\n","  cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","          'n_epochs' : 1,\n","          'seed' : 0, #sometimes also the seed is treated as a hyperparameter\n","          'model_save' : False,\n","          'lr' : trial.suggest_loguniform('lr', 1e-4, 1e-2),          \n","          'dropout': trial.suggest_discrete_uniform('dropout', 0.1, 0.3, 0.5),\n","          'optimizer': trial.suggest_categorical('optimizer',[torch.optim.SGD, torch.optim.Adam]),\n","          'batch_norm': trial.suggest_categorical('batch_norm',[True, False]), #after each layer, re-normalize images\n","          }\n","\n","  torch.manual_seed(cfg['seed'])\n","  # model\n","  model = MLP(dropout_rate = cfg['dropout']).to(device)\n","  # apply weight init\n","  model.apply(weight_init)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = cfg['optimizer'](model.parameters(), lr=cfg['lr'])\n","\n","  best_valid_acc = training(model, train_loader, valid_loader, optimizer, criterion, \n","                  epochs = cfg['n_epochs'], device = cfg['device'], model_save = cfg['model_save'])\n","\n","  return best_valid_acc"]},{"cell_type":"markdown","metadata":{"id":"RKpYTX-vEW6U"},"source":["To determine the hyperparameter values to be used in a trial, we must define a sampler. We will use `optuna.samplers.TPESampler()` that is based on Tree-structured Parzen Estimator algorithm (it makes use of Gaussian Mixture Models, more details [here](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler)). There are [several samplers](https://optuna.readthedocs.io/en/stable/reference/samplers.html), for instance also the most basic ones like `optuna.samplers.GridSampler` which suggests all combinations of parameters in the given search space during the study.\n","\n","It means that there are some methods to optimize the choice of hyperparameters in the landscape without having to try all different combinations, which could be an issue with a large number of hyperparameters."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"BoZYv0C3jCj4"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 17:57:55,963]\u001b[0m A new study created in memory with name: no-name-5b8c0adc-af83-44d8-83f8-4cbf9411787a\u001b[0m\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.494748\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.092462\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.104019\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.121471\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.208904\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.978616\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.054073\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.883446\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.727507\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.619683\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.779263\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.457513\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.634196\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.690945\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.434799\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.551895\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.637646\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.262252\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 17:59:25,698]\u001b[0m Trial 0 finished with value: 0.6720890402793884 and parameters: {'lr': 0.00023278085917784587, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 0 with value: 0.6720890402793884.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.6720890402793884, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.883943\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.309014\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.411424\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.492133\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.357390\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.473962\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.241656\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.159409\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.088040\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.110540\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.209604\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.453261\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.281269\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.150640\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.280782\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.159264\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.265765\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.078372\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:00:30,189]\u001b[0m Trial 1 finished with value: 0.9307934045791626 and parameters: {'lr': 0.008784481027289823, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9307934045791626, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.228235\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.514100\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.462236\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.509313\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.440080\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.343912\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.285059\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.248133\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.168040\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175920\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.122569\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.549037\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.348577\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.270956\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.357593\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.126347\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.439777\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.229186\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:01:11,127]\u001b[0m Trial 2 finished with value: 0.926369845867157 and parameters: {'lr': 0.00018004592881943406, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.926369845867157, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.965606\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.819054\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.662052\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.849141\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.681040\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.540123\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.387952\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.337729\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.153108\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.216139\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.350124\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.492593\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.493967\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.304758\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.541966\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.276962\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.515260\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.328307\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:02:01,288]\u001b[0m Trial 3 finished with value: 0.9083904027938843 and parameters: {'lr': 0.0030397467520163106, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9083904027938843, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.391250\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.512558\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.557247\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.556767\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.472775\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.494646\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.297415\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.243198\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.073210\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.150676\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.232505\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.463293\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.333018\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.209033\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.279825\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.212515\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.368974\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.134490\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:02:46,193]\u001b[0m Trial 4 finished with value: 0.9218036532402039 and parameters: {'lr': 0.0052866493031210205, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9218036532402039, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.699884\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.114749\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.223537\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.803008\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.271105\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.396977\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.529173\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.264421\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.650856\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.009412\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.417080\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.177717\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.139154\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.128013\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.278634\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.143189\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.356260\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.209902\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:03:44,129]\u001b[0m Trial 5 finished with value: 0.9289383292198181 and parameters: {'lr': 0.004108710888277533, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': True}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9289383292198181, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.401831\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.595640\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.375442\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.466427\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.363805\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.032730\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.694756\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.672732\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.479528\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.547238\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.649782\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.569390\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.741621\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.509869\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.589606\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.566208\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.853193\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.647205\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:04:24,816]\u001b[0m Trial 6 finished with value: 0.8724315166473389 and parameters: {'lr': 0.001100843682008041, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': True}. Best is trial 1 with value: 0.9307934045791626.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.8724315166473389, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.570442\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.266451\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.223411\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.330189\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.291158\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.235367\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.257140\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.176181\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.087879\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.089252\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.165538\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.219323\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.049366\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.319210\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.417736\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.075034\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.382192\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.022498\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:05:13,324]\u001b[0m Trial 7 finished with value: 0.9417808055877686 and parameters: {'lr': 0.0006261305805776257, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9417808055877686, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.460467\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.987927\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.939613\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.923046\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.979488\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.712029\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.622507\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.380103\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.190598\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.152883\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.187064\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.951885\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.225465\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.150710\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.840808\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.924130\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.222534\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.887957\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:06:01,276]\u001b[0m Trial 8 finished with value: 0.7926655411720276 and parameters: {'lr': 0.00045432589882273184, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.7926655411720276, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.503995\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.114073\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.141714\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.156370\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.238408\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.032573\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.128327\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.980339\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.837846\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.738936\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.888973\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.580300\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.746938\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.809150\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.580158\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.710398\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.737336\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.362350\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:06:59,665]\u001b[0m Trial 9 finished with value: 0.6188641786575317 and parameters: {'lr': 0.00019302578802524956, 'dropout': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.6188641786575317, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.615054\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.155074\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.231808\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.319466\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.231409\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.232942\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.199395\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.199479\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.355122\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.060366\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.208013\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.279862\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.093000\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.098325\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.217035\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.110258\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.372370\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.022674\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:08:02,871]\u001b[0m Trial 10 finished with value: 0.9366438388824463 and parameters: {'lr': 0.000964883184377486, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9366438388824463, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.723148\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.105746\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.241613\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.382077\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.196875\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.216832\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.255942\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.104057\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.323600\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.053440\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.232144\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.203127\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.194696\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.155008\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.162117\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.097391\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.351761\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.009878\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:08:49,155]\u001b[0m Trial 11 finished with value: 0.9346461296081543 and parameters: {'lr': 0.0011797535379781383, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9346461296081543, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.566587\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.188763\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.260596\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.340757\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.290412\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.284328\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.214818\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.201886\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.290353\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.200275\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.233787\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.269019\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.210574\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.339876\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.420219\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.153755\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.413021\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.012878\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:09:43,197]\u001b[0m Trial 12 finished with value: 0.9360730648040771 and parameters: {'lr': 0.0006031019188404036, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9360730648040771, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.613116\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.106823\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.164160\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.476436\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.326474\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.119897\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.359290\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.146936\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.310062\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.074982\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.400625\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.212997\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.308026\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.205361\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.226634\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.085853\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.451772\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.083684\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:10:40,811]\u001b[0m Trial 13 finished with value: 0.9362157583236694 and parameters: {'lr': 0.0018593236160173235, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.9362157583236694, saving model...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.494128\n"]},{"name":"stderr","output_type":"stream","text":["/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [0.1, 0.3] and q=0.5, but the range is not divisible by `q`. It will be replaced by [0.1, 0.1].\n","  warnings.warn(\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n","  warnings.warn(message)\n","/Users/MatteoSaccardi/opt/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n","  warnings.warn(message)\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.643715\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.198633\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.255838\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.303712\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.266561\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.286474\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.225111\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.164019\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.281774\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.179816\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.206428\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.314528\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.160263\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.311938\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.319112\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.058163\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.404050\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.031031\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-01-18 18:11:35,219]\u001b[0m Trial 14 finished with value: 0.931506872177124 and parameters: {'lr': 0.00046355663563725405, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}. Best is trial 7 with value: 0.9417808055877686.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best validation accuracy improved from 0 to 0.931506872177124, saving model...\n"]},{"data":{"text/plain":["['./optuna_report.pkl']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","\n","# a sampler has the responsibility to determine the parameter values to be evaluated in a trial.\n","sampler = optuna.samplers.TPESampler()\n","# You can also try \n","# sampler = optuna.samplers.GridSampler()\n","    \n","study = optuna.create_study(sampler=sampler, direction='maximize')\n","study.optimize(func=train_mnist, n_trials=15)\n","\n","# persist an arbitrary Python object into one file.\n","joblib.dump(study, './optuna_report.pkl')"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":1780,"status":"ok","timestamp":1599665015980,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"XR7f5Y9gfxMz","outputId":"7267f7e4-32a2-4632-f1a9-0dbca7ff5efb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>value</th>\n","      <th>duration</th>\n","      <th>params_batch_norm</th>\n","      <th>params_dropout</th>\n","      <th>params_lr</th>\n","      <th>params_optimizer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.672089</td>\n","      <td>0 days 00:01:29.727911</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000233</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.930793</td>\n","      <td>0 days 00:01:04.488929</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.008784</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.926370</td>\n","      <td>0 days 00:00:40.936897</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.000180</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.908390</td>\n","      <td>0 days 00:00:50.160119</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.003040</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.921804</td>\n","      <td>0 days 00:00:44.903595</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.005287</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.928938</td>\n","      <td>0 days 00:00:57.934054</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.004109</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.872432</td>\n","      <td>0 days 00:00:40.684789</td>\n","      <td>True</td>\n","      <td>0.1</td>\n","      <td>0.001101</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.941781</td>\n","      <td>0 days 00:00:48.506490</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000626</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.792666</td>\n","      <td>0 days 00:00:47.950122</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000454</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.618864</td>\n","      <td>0 days 00:00:58.388479</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000193</td>\n","      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.936644</td>\n","      <td>0 days 00:01:03.202522</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000965</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.934646</td>\n","      <td>0 days 00:00:46.282617</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.001180</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>0.936073</td>\n","      <td>0 days 00:00:54.039700</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000603</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>0.936216</td>\n","      <td>0 days 00:00:57.612724</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.001859</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>0.931507</td>\n","      <td>0 days 00:00:54.406329</td>\n","      <td>False</td>\n","      <td>0.1</td>\n","      <td>0.000464</td>\n","      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    number     value               duration  params_batch_norm  \\\n","0        0  0.672089 0 days 00:01:29.727911               True   \n","1        1  0.930793 0 days 00:01:04.488929               True   \n","2        2  0.926370 0 days 00:00:40.936897               True   \n","3        3  0.908390 0 days 00:00:50.160119              False   \n","4        4  0.921804 0 days 00:00:44.903595              False   \n","5        5  0.928938 0 days 00:00:57.934054               True   \n","6        6  0.872432 0 days 00:00:40.684789               True   \n","7        7  0.941781 0 days 00:00:48.506490              False   \n","8        8  0.792666 0 days 00:00:47.950122              False   \n","9        9  0.618864 0 days 00:00:58.388479              False   \n","10      10  0.936644 0 days 00:01:03.202522              False   \n","11      11  0.934646 0 days 00:00:46.282617              False   \n","12      12  0.936073 0 days 00:00:54.039700              False   \n","13      13  0.936216 0 days 00:00:57.612724              False   \n","14      14  0.931507 0 days 00:00:54.406329              False   \n","\n","    params_dropout  params_lr                 params_optimizer  \n","0              0.1   0.000233    <class 'torch.optim.sgd.SGD'>  \n","1              0.1   0.008784    <class 'torch.optim.sgd.SGD'>  \n","2              0.1   0.000180  <class 'torch.optim.adam.Adam'>  \n","3              0.1   0.003040    <class 'torch.optim.sgd.SGD'>  \n","4              0.1   0.005287    <class 'torch.optim.sgd.SGD'>  \n","5              0.1   0.004109  <class 'torch.optim.adam.Adam'>  \n","6              0.1   0.001101    <class 'torch.optim.sgd.SGD'>  \n","7              0.1   0.000626  <class 'torch.optim.adam.Adam'>  \n","8              0.1   0.000454    <class 'torch.optim.sgd.SGD'>  \n","9              0.1   0.000193    <class 'torch.optim.sgd.SGD'>  \n","10             0.1   0.000965  <class 'torch.optim.adam.Adam'>  \n","11             0.1   0.001180  <class 'torch.optim.adam.Adam'>  \n","12             0.1   0.000603  <class 'torch.optim.adam.Adam'>  \n","13             0.1   0.001859  <class 'torch.optim.adam.Adam'>  \n","14             0.1   0.000464  <class 'torch.optim.adam.Adam'>  "]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# load the saved study\n","study = joblib.load('./optuna_report.pkl')\n","# convert in dataframe\n","df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n","df"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":1154,"status":"ok","timestamp":1599665083110,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"dSJzu-9PgZCV","outputId":"4d0cef9f-0239-41b2-f761-c0f4cceddf3e"},"outputs":[{"data":{"text/plain":["FrozenTrial(number=7, values=[0.9417808055877686], datetime_start=datetime.datetime(2022, 1, 18, 18, 4, 24, 817430), datetime_complete=datetime.datetime(2022, 1, 18, 18, 5, 13, 323920), params={'lr': 0.0006261305805776257, 'dropout': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>, 'batch_norm': False}, distributions={'lr': LogUniformDistribution(high=0.01, low=0.0001), 'dropout': DiscreteUniformDistribution(high=0.1, low=0.1, q=0.5), 'optimizer': CategoricalDistribution(choices=(<class 'torch.optim.sgd.SGD'>, <class 'torch.optim.adam.Adam'>)), 'batch_norm': CategoricalDistribution(choices=(True, False))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=7, state=TrialState.COMPLETE, value=None)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["study.best_trial"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":3068,"status":"ok","timestamp":1599665101897,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"jVpuIr6mgZMv","outputId":"7e2217ee-0292-4da4-bd96-f2e185337cb2"},"outputs":[{"ename":"ImportError","evalue":"Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/optuna/visualization/_plotly_imports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtry_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_imports\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplotly_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/var/folders/3m/vfsrsg7x1c517wng2_5yjw2w0000gn/T/ipykernel_75229/615545545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#optuna.visualization.plot_contour(study, params=['lr','optimizer'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_optimization_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/optuna/visualization/_optimization_history.py\u001b[0m in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0m_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0m_check_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_optimization_history_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/optuna/_imports.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deferred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deferred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."]}],"source":["#optuna.visualization.plot_contour(study, params=['lr','optimizer'])\n","optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"markdown","metadata":{},"source":["We can also add a \"green parameter\" to have a better understanding of performances with respect to the choice of hyperparameters and their carbon footprint, to better decide which model to use."]},{"cell_type":"markdown","metadata":{"id":"jB704nfU-eiB"},"source":["## K-Fold Cross Validation"]},{"cell_type":"markdown","metadata":{"id":"ZPyYQxI1x8hB"},"source":["So far, we have used holdout set for model validation that consists of splitting once and for all the whole dataset in training and validation sets. This method is widely used but can suffer from the lucky split phenomenon.\n","\n","It is a good practice in machine learning to overwhelm hold-out validation and exploit cross-validation. This means that multiple rounds of train/validation are performed using different data partitions. Validation results are then averaged over the rounds to give a fair estimate of the model predictive performances. This process allows to reduce variance and avoids the lucky split phonomenon.\n","\n","Let see how it works. Cross-validation is always performed on training set (Think about MNIST dataset, that has 60k samples in training set and 10K in test set). In case of k-fold cross validation, say number of samples in training set is 100 and you have taken k = 5, then train set is equally divided in 5 equal parts: p1, p2, p3, p4, p5.\n","Now, in first iteration/fold p1 will be left out and remaining 4 parts (p2, p3, p4, p5) will be used for training the algorithm and p1 to validate it. Once algorithm is trained this trained model will be validated on p1 from this you will get error/accuracy metric. Now in 2nd iteration/fold p2 will be left out and again algorithm will be trained on remaining 4 parts( p1, p3, p4, p5) and once algorithm is trained it gets validated on p2. It continues till all 5 iterations are over. At last you will get average train and validated error/accuracy metrics from cross validation exercise. And this is how cross-validation works and is used.\n","\n","<center>  <img src=\"https://drive.google.com/uc?export=view&id=1qRpYnHSzc1lhutp-F3ehpEGMSSa18Eza\"width=\"800\">  </center> \n","\n","\n","The best scenario is that our accuracy is similar in all our folds, say 92.0, 91.5, 92.0, 92.5 and 91.8. This means that our algorithm (and our data) is consistent and we can be confident that by training it on all the data set and deploy it in production will lead to similar performance.\n","However, we could end up in a slightly different scenario, say 92.0, 44.0, 91.5, 92.5 and 91.8. These results look very strange. It looks like one of our folds is from a different distribution, we have to go back and make sure that our data is what we think it is.\n","\n","The worst scenario we can end up in is when we have considerable variation in our results, say 80, 44, 99, 60 and 87. Here it looks like that our algorithm or our data (or both) is no consistent, it could be that our algorithm is unable to learn, or our data is very complicated.\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"pR3ds_7kCPko"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor(), \n","                                        transforms.Normalize( (0.1307,), (0.3081,))])\n","\n","train_dataset = MNIST(root = './data', train = True, transform = transform, download=True)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"p3MsWG5l6CHH"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","kfold = KFold(n_splits=5)\n","\n","def cross_validation(network, optim, crit, epochs, device, batch_size = 16, model_save = False):\n","    final_acc = []\n","    for fold, (train_index, valid_index) in enumerate(kfold.split(train_dataset.data, train_dataset.targets)):\n","        # reset weights to init values at each fold\n","        network.load_state_dict(torch.load('./weights_init'))\n","        print(list(network.parameters())[0][0])\n","        \n","        ### Dividing data into folds\n","        train_fold = torch.utils.data.Subset(train_dataset, train_index)\n","        valid_fold = torch.utils.data.Subset(train_dataset, valid_index)\n","\n","        train_loader_fold = DataLoader(train_fold, batch_size = batch_size, shuffle = False)\n","        valid_loader_fold = DataLoader(valid_fold, batch_size = batch_size, shuffle = False)\n","\n","        #print(len(train_loader_fold))\n","        #print(len(valid_loader_fold))\n","\n","        kwargs = {'optim': optim, 'crit': crit, \n","          'epochs': epochs, 'device': device, 'model_save': model_save}\n","        \n","        valid_acc = training(network, train_loader_fold, valid_loader_fold, **kwargs)\n","        final_acc.append(valid_acc.to('cpu'))\n","        #print(final_acc)\n","\n","        print('\\nFold number {} , Valid Accuracy: {}\\n'.format(fold + 1 , valid_acc))\n","    #print(final_acc)\n","    print('\\nFinal Accuracy Mean: {} , Final Accuracy Std: {}\\n'.format(np.array(final_acc).mean() , np.array(final_acc).std()))  \n","    return np.array(final_acc).mean(), np.array(final_acc).std()"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"HvRbV5CRGdQ8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-3.1108e-02,  2.3562e-02, -1.3610e-02,  2.1022e-02,  1.3821e-02,\n","        -3.0939e-02,  3.3304e-02,  3.0860e-02,  2.1511e-02,  8.8448e-03,\n","         2.0658e-02, -9.6553e-04,  1.1264e-02, -8.9087e-03,  2.9645e-02,\n","         2.0959e-02, -9.1753e-03, -2.2291e-02, -1.9601e-02, -1.7701e-03,\n","        -6.6580e-03,  1.3585e-02, -4.0529e-03,  3.5579e-02, -1.2637e-03,\n","        -1.4234e-02, -3.7397e-03,  1.6321e-02, -1.4445e-02, -2.8103e-02,\n","         2.8013e-02,  2.0685e-02, -1.4932e-03, -3.2996e-03,  3.4189e-02,\n","        -6.5714e-03, -1.8120e-02, -9.9337e-03,  4.2255e-03,  4.7978e-03,\n","        -1.3997e-02, -1.1449e-03, -1.1023e-02,  2.4119e-02, -2.8557e-02,\n","         7.2616e-03, -6.5016e-03, -2.7686e-02,  8.0898e-03,  2.4436e-02,\n","        -3.0726e-02,  1.2432e-02, -1.0231e-02,  2.7347e-02, -3.2042e-02,\n","         1.3188e-02,  1.8907e-02, -2.3698e-02, -2.0206e-03, -2.5372e-02,\n","        -3.4477e-02,  2.0135e-02,  1.2449e-02,  2.0430e-03,  1.8638e-02,\n","        -8.5880e-03,  1.7140e-02,  7.1184e-03, -9.1825e-03, -4.3100e-03,\n","         3.4261e-03, -2.3917e-02, -1.2106e-02, -1.0867e-03,  1.4593e-02,\n","         1.6603e-03,  1.7173e-02,  2.8741e-02, -3.2021e-02, -1.9180e-04,\n","         7.7470e-03,  2.1300e-02,  7.8060e-03, -2.3635e-02, -1.4178e-02,\n","        -1.1676e-02, -3.2540e-02,  2.7707e-02, -8.5022e-03,  3.1863e-02,\n","        -3.4404e-02, -1.1572e-02, -2.1900e-02,  1.9367e-02,  1.9737e-02,\n","         2.5524e-02, -8.7473e-03,  2.8780e-02,  2.4359e-02, -6.9092e-03,\n","        -4.3599e-03,  1.9465e-02,  3.2965e-02, -2.6606e-02,  2.3262e-02,\n","         1.1251e-03, -2.3204e-02, -2.2275e-02, -1.8088e-02,  8.3276e-03,\n","         3.3410e-03,  2.0782e-03,  2.9487e-02, -4.2639e-03,  2.1908e-02,\n","         1.8294e-02,  3.5594e-02,  6.0315e-03,  5.9471e-03, -3.1772e-02,\n","         1.0458e-02, -2.3988e-02,  1.7502e-02, -7.3774e-03,  3.4989e-02,\n","         3.1809e-02,  3.1702e-02, -6.7919e-03, -2.3305e-02, -7.8322e-03,\n","        -2.9915e-02, -3.2329e-02,  1.1694e-02, -8.4946e-03,  3.5100e-02,\n","         1.3522e-02, -5.0619e-03, -6.1967e-03,  1.5649e-02, -3.5049e-02,\n","        -3.3163e-02, -1.4512e-02,  1.4703e-03, -1.4072e-02,  2.4506e-03,\n","         2.0077e-02, -2.4523e-02, -2.7942e-02, -2.4531e-03,  2.9514e-03,\n","        -3.1328e-02,  2.9858e-02, -2.7193e-02,  1.1742e-03, -1.9484e-03,\n","         2.6516e-02,  2.4714e-02, -2.0983e-02,  3.0581e-02, -1.8198e-02,\n","        -5.1935e-03, -5.2360e-03,  1.2961e-02, -2.3514e-02, -1.2884e-02,\n","        -2.3628e-02, -3.1636e-02, -3.5485e-02, -3.7532e-03,  5.1559e-03,\n","         3.5420e-02, -1.1816e-02,  3.0634e-02,  1.2500e-02,  3.0829e-02,\n","         1.2731e-02, -2.1480e-02, -2.0407e-02,  6.6126e-03, -1.4124e-02,\n","         2.9933e-02, -1.6640e-02,  8.2632e-03, -8.3206e-03,  7.8198e-03,\n","        -2.0932e-02,  1.2071e-02,  1.6597e-03, -1.8342e-02, -3.5629e-02,\n","         3.5667e-02,  1.8433e-02, -1.9436e-02, -2.5018e-03, -5.9386e-03,\n","         9.5387e-03,  3.4445e-02, -1.0506e-02, -1.7918e-02,  1.9254e-02,\n","         2.6210e-02,  2.9034e-02, -3.4959e-02,  3.4201e-02, -1.1839e-02,\n","        -3.0739e-04,  1.0280e-02,  5.0073e-04,  1.9889e-02,  2.0633e-02,\n","        -2.5743e-02, -2.3863e-02, -2.8247e-02, -5.2535e-03,  1.1367e-02,\n","        -1.3596e-02,  2.1466e-02,  1.9084e-02,  6.3256e-03,  9.8533e-03,\n","         7.9822e-03, -1.6440e-02,  2.8640e-02,  1.1714e-02,  5.5387e-03,\n","         3.0800e-02, -1.3671e-02,  1.3649e-02, -1.3083e-02,  1.1843e-02,\n","         1.6142e-02, -1.7152e-02, -2.3917e-02, -1.0103e-02,  1.5654e-02,\n","        -2.3061e-02, -2.6950e-02,  2.1743e-02, -3.2777e-02,  2.1591e-02,\n","         1.8160e-02,  4.0270e-03, -2.6607e-02,  1.6059e-02,  2.1128e-02,\n","        -2.4237e-02, -1.1960e-02,  2.7454e-02, -2.2509e-03, -1.1405e-02,\n","        -3.0731e-02, -2.9233e-02,  5.7814e-03,  4.6998e-03,  2.3831e-04,\n","         1.4176e-02,  1.8754e-02,  2.7220e-03, -8.9005e-03,  1.6792e-02,\n","         3.5149e-02,  1.0942e-02,  1.3230e-02, -1.4788e-02,  2.9122e-02,\n","         1.1616e-03, -6.9433e-03,  3.2809e-02, -2.0309e-02, -1.2029e-02,\n","         1.2572e-02,  2.2130e-02, -2.8379e-02, -6.8678e-03,  3.5000e-02,\n","        -2.6722e-02, -1.0942e-03, -6.6117e-03, -4.6719e-03, -3.7166e-03,\n","         3.8235e-03, -3.4946e-02, -1.0788e-02, -2.6446e-02, -9.3842e-03,\n","         1.4883e-02,  2.6005e-03, -3.5277e-02, -1.0719e-02,  3.1520e-02,\n","         5.9205e-04, -1.4752e-02,  1.2906e-02,  7.7259e-03,  1.9088e-02,\n","        -8.6026e-03, -1.1294e-03, -1.2804e-02, -3.5588e-02,  2.2912e-02,\n","         5.4993e-03, -2.6627e-02, -1.8863e-02, -3.2767e-02,  1.5669e-02,\n","         1.4470e-02, -2.1004e-02,  1.0201e-02, -2.4873e-02,  1.2721e-02,\n","        -2.0890e-02,  2.9335e-02,  8.4590e-03,  3.1612e-02, -1.6410e-02,\n","        -4.3074e-03,  3.5622e-02,  3.4801e-03, -1.2590e-02,  2.1668e-02,\n","         2.0644e-02, -3.4191e-02, -1.5032e-02,  3.8574e-03,  3.5194e-02,\n","        -1.0082e-02, -6.8383e-03,  2.4914e-02,  3.9131e-03,  2.8826e-02,\n","        -2.3657e-02,  1.7922e-02,  2.3088e-02, -3.1920e-02,  1.7088e-04,\n","        -2.6320e-02,  2.9709e-03,  6.6105e-03,  9.6403e-03, -2.2107e-02,\n","         1.8260e-03, -5.4769e-03, -3.6621e-03, -2.8487e-02, -5.6615e-03,\n","        -2.8985e-02,  1.5701e-02,  7.6726e-03, -2.7654e-02, -3.5044e-02,\n","        -2.1235e-03,  5.2059e-03,  6.7579e-03,  1.6026e-02,  3.0274e-02,\n","         2.3207e-02,  7.5014e-03,  3.3570e-02, -1.8227e-02, -4.9395e-03,\n","        -3.4447e-02, -1.6357e-02,  2.2363e-02, -1.2721e-02,  1.6730e-02,\n","        -3.4551e-02, -6.7659e-03,  1.1813e-02, -2.8829e-02, -2.1908e-03,\n","        -1.0931e-02,  3.4059e-02,  2.8418e-02,  6.1077e-03,  9.0828e-03,\n","        -1.5658e-02,  1.1254e-04,  9.6228e-03,  2.5801e-02,  2.2231e-02,\n","         2.1628e-02,  2.9192e-02,  2.7201e-02, -5.3290e-04, -3.2435e-02,\n","         1.1606e-02,  2.0386e-03,  3.0118e-02,  1.8588e-02, -3.0772e-02,\n","         1.8126e-02, -2.2488e-02, -9.5115e-03, -3.4270e-02,  2.9253e-02,\n","        -1.7760e-02, -3.5102e-02,  4.2821e-03, -1.8724e-02,  2.3523e-02,\n","        -1.1340e-02,  1.7731e-02, -3.8702e-03, -1.3534e-02,  8.8067e-04,\n","        -2.9126e-02,  4.9423e-03, -1.6016e-02, -6.0132e-03,  6.4760e-03,\n","         1.3887e-02, -3.5304e-02,  1.2898e-02,  9.4187e-03,  2.3850e-02,\n","        -3.3215e-02,  8.0435e-03, -5.0447e-03, -3.0127e-02,  2.7067e-02,\n","        -2.0843e-02, -9.8425e-03, -4.1286e-03,  2.3763e-02,  1.2256e-02,\n","         8.4853e-03,  1.9214e-02, -3.5586e-02,  1.6501e-02, -2.3770e-03,\n","         2.6716e-02,  2.9882e-02,  2.4439e-02,  1.7453e-02, -2.4469e-02,\n","         1.3200e-02, -1.3409e-02, -2.0289e-02, -2.0192e-02, -1.8638e-02,\n","        -3.2642e-02, -1.0186e-02,  2.3828e-02,  3.0878e-02,  1.1174e-02,\n","        -1.1620e-02, -1.3368e-02, -4.8857e-03, -6.9446e-03, -2.1758e-02,\n","         2.5447e-02, -3.2655e-04, -3.2778e-02, -1.2407e-02, -2.5746e-03,\n","         1.1249e-02,  8.4687e-03,  2.7508e-02,  7.3380e-03, -2.4991e-02,\n","         4.0053e-04,  2.5850e-02, -1.6093e-02,  1.3535e-02,  2.9198e-02,\n","        -3.0009e-02, -6.0037e-03, -3.4853e-02,  9.3856e-03, -4.8135e-03,\n","         3.5677e-02,  5.6428e-03,  9.1762e-03,  2.9572e-02, -3.3303e-02,\n","        -2.9699e-02,  9.8307e-03, -1.9110e-02,  3.1789e-02,  2.1047e-02,\n","        -2.7718e-02,  1.9382e-02, -4.4371e-03,  3.1035e-03,  1.2912e-03,\n","         2.0250e-02, -1.5161e-03, -1.5698e-02, -3.3978e-02, -1.6710e-02,\n","        -1.1362e-03, -3.1087e-02,  7.2027e-03,  3.5034e-02,  6.0924e-03,\n","         2.1756e-02,  3.4730e-02,  8.4615e-03, -3.2396e-02, -2.5138e-02,\n","        -1.5917e-02, -2.4192e-02, -2.4682e-02, -1.0266e-02, -2.8302e-02,\n","        -4.9269e-03, -2.7959e-02, -3.3961e-02, -6.1202e-03,  3.1271e-02,\n","        -1.8146e-02,  2.5531e-02,  2.4184e-02,  2.6373e-02, -3.4721e-02,\n","        -2.3258e-02,  1.5469e-02,  3.3899e-02,  3.4953e-02, -9.5330e-03,\n","        -7.6447e-03, -1.5982e-02,  1.7682e-02,  1.4647e-03,  2.5155e-02,\n","         2.1435e-02, -2.8598e-02,  3.5195e-02, -3.9483e-03,  9.3768e-03,\n","        -1.9650e-02, -2.9577e-02, -2.7503e-02, -8.2164e-03, -3.2761e-03,\n","        -2.9794e-02,  3.3573e-02, -2.5794e-02, -1.7871e-02, -4.3952e-03,\n","         3.0800e-02, -1.0957e-02,  2.4444e-02,  3.1848e-02,  1.5286e-02,\n","         3.1893e-02, -2.9608e-03,  3.1325e-02, -1.1912e-02,  1.5376e-02,\n","        -8.2521e-03,  2.4302e-03, -4.7494e-03,  1.8111e-02, -2.9871e-02,\n","        -2.7772e-02, -2.1450e-02,  8.2388e-03,  2.4070e-02,  1.5319e-02,\n","         1.1052e-02, -1.6135e-02, -1.3371e-02,  7.5438e-04,  2.0630e-02,\n","         1.6169e-03,  6.3129e-03,  2.3711e-02, -2.3336e-02,  1.3298e-02,\n","         1.6929e-02, -7.6259e-03, -2.5192e-02, -1.3810e-02, -1.3114e-02,\n","         5.7672e-03, -2.0410e-02,  1.3049e-03, -2.0924e-02,  1.9192e-02,\n","         6.4911e-03,  1.1237e-02, -2.6982e-02, -1.4590e-02,  1.5074e-02,\n","         1.7755e-02,  6.8836e-05,  2.8960e-02,  6.5492e-03, -6.4526e-03,\n","        -1.7838e-02, -9.7690e-03,  1.6658e-02, -3.4443e-02, -7.4412e-03,\n","        -3.4939e-02,  5.1932e-03,  3.8833e-03,  1.7178e-02,  1.9874e-02,\n","         2.0285e-02,  1.3708e-02,  1.2074e-02, -1.4510e-02,  3.2547e-02,\n","         9.8103e-04,  3.1207e-02,  3.0911e-03,  2.8954e-02,  3.4061e-02,\n","        -1.5131e-02, -3.3309e-02,  1.0564e-03, -2.7906e-02,  3.1918e-02,\n","        -2.7322e-02, -2.1863e-02, -2.1211e-02, -1.8289e-02, -1.4092e-04,\n","        -3.1349e-02, -6.5347e-03,  1.7494e-02,  3.5585e-02, -2.3558e-02,\n","         1.8044e-02,  1.8246e-03,  1.6488e-02,  2.1206e-03,  2.3520e-02,\n","         3.5143e-02,  7.5667e-03, -3.5086e-02, -1.6132e-02,  2.7012e-02,\n","         1.2543e-02, -3.1106e-02, -3.5359e-02,  3.4191e-03,  2.3654e-02,\n","         2.0159e-02,  7.6107e-03, -1.7745e-02, -1.6661e-02, -2.8765e-02,\n","         9.5250e-03,  1.6091e-02, -1.7923e-02,  2.9256e-02,  2.8095e-02,\n","         5.2800e-04,  2.3636e-02, -2.7322e-02, -6.2480e-03, -4.3347e-03,\n","        -1.0311e-02,  1.7093e-03, -2.2769e-02, -3.5393e-02,  9.9167e-03,\n","         2.2256e-02, -1.9395e-02, -2.5735e-02,  2.6626e-02, -3.3112e-02,\n","         3.0576e-02,  2.6292e-02, -2.8800e-02, -1.6690e-02, -3.2146e-02,\n","        -3.6030e-03,  6.4922e-03,  2.6992e-02,  1.8004e-02, -1.9767e-02,\n","        -3.5530e-03,  2.9089e-04,  2.9437e-04, -7.1626e-03, -3.0182e-02,\n","         1.0185e-02, -1.9814e-02, -1.6088e-02,  2.1539e-02,  3.9423e-03,\n","        -1.7734e-02,  1.9036e-02, -2.1679e-02, -2.1672e-03, -2.7933e-02,\n","        -3.4809e-02,  1.4321e-02,  1.0941e-02, -5.8556e-03,  1.8746e-02,\n","        -2.0126e-02, -1.4028e-02,  2.7493e-02,  2.2200e-02, -2.9914e-02,\n","        -6.4312e-03,  6.7892e-03, -2.6286e-02, -3.4793e-02,  3.5271e-02,\n","        -1.7848e-03, -2.5655e-02,  6.2843e-03,  2.1279e-02,  4.5552e-03,\n","        -4.4332e-03,  2.7816e-02,  6.3435e-03, -3.0658e-02,  2.7486e-02,\n","         6.3648e-04, -2.9292e-03, -2.6239e-02,  1.5762e-02, -2.6464e-02,\n","         2.6633e-02, -3.4193e-02,  1.2745e-02, -2.8574e-02,  3.3291e-02,\n","         2.4922e-03,  1.8956e-02,  7.4373e-03,  3.1659e-02, -4.3523e-03,\n","        -3.0179e-02, -1.3443e-03, -9.5118e-04, -2.4038e-02,  1.9001e-02,\n","         1.7315e-02, -3.2907e-02,  2.1788e-02, -7.6555e-03,  1.8424e-02,\n","         3.5684e-03, -4.4144e-03,  2.2026e-03,  1.3002e-02, -1.7966e-03,\n","        -2.2696e-03,  2.7091e-02,  4.0474e-03,  2.5138e-02,  1.3399e-02,\n","         1.7410e-02,  1.2133e-02, -1.2321e-02, -1.9545e-02, -7.9674e-03,\n","         2.0563e-02, -1.5978e-02,  2.3388e-02, -1.9403e-02, -2.5561e-02,\n","        -2.4557e-02, -2.1437e-02,  2.8891e-02,  2.3200e-02,  3.7684e-03,\n","        -1.5712e-02, -2.7450e-02,  1.0863e-02,  1.0099e-02, -1.3729e-02,\n","         7.1165e-03,  6.6021e-03,  2.1306e-02, -2.4786e-02,  1.7868e-02,\n","         8.7222e-03, -1.1271e-02, -1.8776e-02, -1.3188e-02,  8.9272e-03,\n","         3.6319e-03,  1.8686e-03, -2.0596e-02,  2.4475e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.342316\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 1.054491\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.595155\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.421145\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.267257\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.208329\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.785187\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.250739\n","Best validation accuracy improved from 0 to 0.9291666746139526, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.243478\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.318818\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.233041\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.135197\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.115444\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.074561\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.828129\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.224875\n","Best validation accuracy improved from 0.9291666746139526 to 0.9475833177566528, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.160440\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.192573\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.109723\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.125248\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.150839\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.068140\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.885735\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.153779\n","Best validation accuracy improved from 0.9475833177566528 to 0.956250011920929, saving model...\n","\n","Fold number 1 , Valid Accuracy: 0.956250011920929\n","\n","tensor([-3.1108e-02,  2.3562e-02, -1.3610e-02,  2.1022e-02,  1.3821e-02,\n","        -3.0939e-02,  3.3304e-02,  3.0860e-02,  2.1511e-02,  8.8448e-03,\n","         2.0658e-02, -9.6553e-04,  1.1264e-02, -8.9087e-03,  2.9645e-02,\n","         2.0959e-02, -9.1753e-03, -2.2291e-02, -1.9601e-02, -1.7701e-03,\n","        -6.6580e-03,  1.3585e-02, -4.0529e-03,  3.5579e-02, -1.2637e-03,\n","        -1.4234e-02, -3.7397e-03,  1.6321e-02, -1.4445e-02, -2.8103e-02,\n","         2.8013e-02,  2.0685e-02, -1.4932e-03, -3.2996e-03,  3.4189e-02,\n","        -6.5714e-03, -1.8120e-02, -9.9337e-03,  4.2255e-03,  4.7978e-03,\n","        -1.3997e-02, -1.1449e-03, -1.1023e-02,  2.4119e-02, -2.8557e-02,\n","         7.2616e-03, -6.5016e-03, -2.7686e-02,  8.0898e-03,  2.4436e-02,\n","        -3.0726e-02,  1.2432e-02, -1.0231e-02,  2.7347e-02, -3.2042e-02,\n","         1.3188e-02,  1.8907e-02, -2.3698e-02, -2.0206e-03, -2.5372e-02,\n","        -3.4477e-02,  2.0135e-02,  1.2449e-02,  2.0430e-03,  1.8638e-02,\n","        -8.5880e-03,  1.7140e-02,  7.1184e-03, -9.1825e-03, -4.3100e-03,\n","         3.4261e-03, -2.3917e-02, -1.2106e-02, -1.0867e-03,  1.4593e-02,\n","         1.6603e-03,  1.7173e-02,  2.8741e-02, -3.2021e-02, -1.9180e-04,\n","         7.7470e-03,  2.1300e-02,  7.8060e-03, -2.3635e-02, -1.4178e-02,\n","        -1.1676e-02, -3.2540e-02,  2.7707e-02, -8.5022e-03,  3.1863e-02,\n","        -3.4404e-02, -1.1572e-02, -2.1900e-02,  1.9367e-02,  1.9737e-02,\n","         2.5524e-02, -8.7473e-03,  2.8780e-02,  2.4359e-02, -6.9092e-03,\n","        -4.3599e-03,  1.9465e-02,  3.2965e-02, -2.6606e-02,  2.3262e-02,\n","         1.1251e-03, -2.3204e-02, -2.2275e-02, -1.8088e-02,  8.3276e-03,\n","         3.3410e-03,  2.0782e-03,  2.9487e-02, -4.2639e-03,  2.1908e-02,\n","         1.8294e-02,  3.5594e-02,  6.0315e-03,  5.9471e-03, -3.1772e-02,\n","         1.0458e-02, -2.3988e-02,  1.7502e-02, -7.3774e-03,  3.4989e-02,\n","         3.1809e-02,  3.1702e-02, -6.7919e-03, -2.3305e-02, -7.8322e-03,\n","        -2.9915e-02, -3.2329e-02,  1.1694e-02, -8.4946e-03,  3.5100e-02,\n","         1.3522e-02, -5.0619e-03, -6.1967e-03,  1.5649e-02, -3.5049e-02,\n","        -3.3163e-02, -1.4512e-02,  1.4703e-03, -1.4072e-02,  2.4506e-03,\n","         2.0077e-02, -2.4523e-02, -2.7942e-02, -2.4531e-03,  2.9514e-03,\n","        -3.1328e-02,  2.9858e-02, -2.7193e-02,  1.1742e-03, -1.9484e-03,\n","         2.6516e-02,  2.4714e-02, -2.0983e-02,  3.0581e-02, -1.8198e-02,\n","        -5.1935e-03, -5.2360e-03,  1.2961e-02, -2.3514e-02, -1.2884e-02,\n","        -2.3628e-02, -3.1636e-02, -3.5485e-02, -3.7532e-03,  5.1559e-03,\n","         3.5420e-02, -1.1816e-02,  3.0634e-02,  1.2500e-02,  3.0829e-02,\n","         1.2731e-02, -2.1480e-02, -2.0407e-02,  6.6126e-03, -1.4124e-02,\n","         2.9933e-02, -1.6640e-02,  8.2632e-03, -8.3206e-03,  7.8198e-03,\n","        -2.0932e-02,  1.2071e-02,  1.6597e-03, -1.8342e-02, -3.5629e-02,\n","         3.5667e-02,  1.8433e-02, -1.9436e-02, -2.5018e-03, -5.9386e-03,\n","         9.5387e-03,  3.4445e-02, -1.0506e-02, -1.7918e-02,  1.9254e-02,\n","         2.6210e-02,  2.9034e-02, -3.4959e-02,  3.4201e-02, -1.1839e-02,\n","        -3.0739e-04,  1.0280e-02,  5.0073e-04,  1.9889e-02,  2.0633e-02,\n","        -2.5743e-02, -2.3863e-02, -2.8247e-02, -5.2535e-03,  1.1367e-02,\n","        -1.3596e-02,  2.1466e-02,  1.9084e-02,  6.3256e-03,  9.8533e-03,\n","         7.9822e-03, -1.6440e-02,  2.8640e-02,  1.1714e-02,  5.5387e-03,\n","         3.0800e-02, -1.3671e-02,  1.3649e-02, -1.3083e-02,  1.1843e-02,\n","         1.6142e-02, -1.7152e-02, -2.3917e-02, -1.0103e-02,  1.5654e-02,\n","        -2.3061e-02, -2.6950e-02,  2.1743e-02, -3.2777e-02,  2.1591e-02,\n","         1.8160e-02,  4.0270e-03, -2.6607e-02,  1.6059e-02,  2.1128e-02,\n","        -2.4237e-02, -1.1960e-02,  2.7454e-02, -2.2509e-03, -1.1405e-02,\n","        -3.0731e-02, -2.9233e-02,  5.7814e-03,  4.6998e-03,  2.3831e-04,\n","         1.4176e-02,  1.8754e-02,  2.7220e-03, -8.9005e-03,  1.6792e-02,\n","         3.5149e-02,  1.0942e-02,  1.3230e-02, -1.4788e-02,  2.9122e-02,\n","         1.1616e-03, -6.9433e-03,  3.2809e-02, -2.0309e-02, -1.2029e-02,\n","         1.2572e-02,  2.2130e-02, -2.8379e-02, -6.8678e-03,  3.5000e-02,\n","        -2.6722e-02, -1.0942e-03, -6.6117e-03, -4.6719e-03, -3.7166e-03,\n","         3.8235e-03, -3.4946e-02, -1.0788e-02, -2.6446e-02, -9.3842e-03,\n","         1.4883e-02,  2.6005e-03, -3.5277e-02, -1.0719e-02,  3.1520e-02,\n","         5.9205e-04, -1.4752e-02,  1.2906e-02,  7.7259e-03,  1.9088e-02,\n","        -8.6026e-03, -1.1294e-03, -1.2804e-02, -3.5588e-02,  2.2912e-02,\n","         5.4993e-03, -2.6627e-02, -1.8863e-02, -3.2767e-02,  1.5669e-02,\n","         1.4470e-02, -2.1004e-02,  1.0201e-02, -2.4873e-02,  1.2721e-02,\n","        -2.0890e-02,  2.9335e-02,  8.4590e-03,  3.1612e-02, -1.6410e-02,\n","        -4.3074e-03,  3.5622e-02,  3.4801e-03, -1.2590e-02,  2.1668e-02,\n","         2.0644e-02, -3.4191e-02, -1.5032e-02,  3.8574e-03,  3.5194e-02,\n","        -1.0082e-02, -6.8383e-03,  2.4914e-02,  3.9131e-03,  2.8826e-02,\n","        -2.3657e-02,  1.7922e-02,  2.3088e-02, -3.1920e-02,  1.7088e-04,\n","        -2.6320e-02,  2.9709e-03,  6.6105e-03,  9.6403e-03, -2.2107e-02,\n","         1.8260e-03, -5.4769e-03, -3.6621e-03, -2.8487e-02, -5.6615e-03,\n","        -2.8985e-02,  1.5701e-02,  7.6726e-03, -2.7654e-02, -3.5044e-02,\n","        -2.1235e-03,  5.2059e-03,  6.7579e-03,  1.6026e-02,  3.0274e-02,\n","         2.3207e-02,  7.5014e-03,  3.3570e-02, -1.8227e-02, -4.9395e-03,\n","        -3.4447e-02, -1.6357e-02,  2.2363e-02, -1.2721e-02,  1.6730e-02,\n","        -3.4551e-02, -6.7659e-03,  1.1813e-02, -2.8829e-02, -2.1908e-03,\n","        -1.0931e-02,  3.4059e-02,  2.8418e-02,  6.1077e-03,  9.0828e-03,\n","        -1.5658e-02,  1.1254e-04,  9.6228e-03,  2.5801e-02,  2.2231e-02,\n","         2.1628e-02,  2.9192e-02,  2.7201e-02, -5.3290e-04, -3.2435e-02,\n","         1.1606e-02,  2.0386e-03,  3.0118e-02,  1.8588e-02, -3.0772e-02,\n","         1.8126e-02, -2.2488e-02, -9.5115e-03, -3.4270e-02,  2.9253e-02,\n","        -1.7760e-02, -3.5102e-02,  4.2821e-03, -1.8724e-02,  2.3523e-02,\n","        -1.1340e-02,  1.7731e-02, -3.8702e-03, -1.3534e-02,  8.8067e-04,\n","        -2.9126e-02,  4.9423e-03, -1.6016e-02, -6.0132e-03,  6.4760e-03,\n","         1.3887e-02, -3.5304e-02,  1.2898e-02,  9.4187e-03,  2.3850e-02,\n","        -3.3215e-02,  8.0435e-03, -5.0447e-03, -3.0127e-02,  2.7067e-02,\n","        -2.0843e-02, -9.8425e-03, -4.1286e-03,  2.3763e-02,  1.2256e-02,\n","         8.4853e-03,  1.9214e-02, -3.5586e-02,  1.6501e-02, -2.3770e-03,\n","         2.6716e-02,  2.9882e-02,  2.4439e-02,  1.7453e-02, -2.4469e-02,\n","         1.3200e-02, -1.3409e-02, -2.0289e-02, -2.0192e-02, -1.8638e-02,\n","        -3.2642e-02, -1.0186e-02,  2.3828e-02,  3.0878e-02,  1.1174e-02,\n","        -1.1620e-02, -1.3368e-02, -4.8857e-03, -6.9446e-03, -2.1758e-02,\n","         2.5447e-02, -3.2655e-04, -3.2778e-02, -1.2407e-02, -2.5746e-03,\n","         1.1249e-02,  8.4687e-03,  2.7508e-02,  7.3380e-03, -2.4991e-02,\n","         4.0053e-04,  2.5850e-02, -1.6093e-02,  1.3535e-02,  2.9198e-02,\n","        -3.0009e-02, -6.0037e-03, -3.4853e-02,  9.3856e-03, -4.8135e-03,\n","         3.5677e-02,  5.6428e-03,  9.1762e-03,  2.9572e-02, -3.3303e-02,\n","        -2.9699e-02,  9.8307e-03, -1.9110e-02,  3.1789e-02,  2.1047e-02,\n","        -2.7718e-02,  1.9382e-02, -4.4371e-03,  3.1035e-03,  1.2912e-03,\n","         2.0250e-02, -1.5161e-03, -1.5698e-02, -3.3978e-02, -1.6710e-02,\n","        -1.1362e-03, -3.1087e-02,  7.2027e-03,  3.5034e-02,  6.0924e-03,\n","         2.1756e-02,  3.4730e-02,  8.4615e-03, -3.2396e-02, -2.5138e-02,\n","        -1.5917e-02, -2.4192e-02, -2.4682e-02, -1.0266e-02, -2.8302e-02,\n","        -4.9269e-03, -2.7959e-02, -3.3961e-02, -6.1202e-03,  3.1271e-02,\n","        -1.8146e-02,  2.5531e-02,  2.4184e-02,  2.6373e-02, -3.4721e-02,\n","        -2.3258e-02,  1.5469e-02,  3.3899e-02,  3.4953e-02, -9.5330e-03,\n","        -7.6447e-03, -1.5982e-02,  1.7682e-02,  1.4647e-03,  2.5155e-02,\n","         2.1435e-02, -2.8598e-02,  3.5195e-02, -3.9483e-03,  9.3768e-03,\n","        -1.9650e-02, -2.9577e-02, -2.7503e-02, -8.2164e-03, -3.2761e-03,\n","        -2.9794e-02,  3.3573e-02, -2.5794e-02, -1.7871e-02, -4.3952e-03,\n","         3.0800e-02, -1.0957e-02,  2.4444e-02,  3.1848e-02,  1.5286e-02,\n","         3.1893e-02, -2.9608e-03,  3.1325e-02, -1.1912e-02,  1.5376e-02,\n","        -8.2521e-03,  2.4302e-03, -4.7494e-03,  1.8111e-02, -2.9871e-02,\n","        -2.7772e-02, -2.1450e-02,  8.2388e-03,  2.4070e-02,  1.5319e-02,\n","         1.1052e-02, -1.6135e-02, -1.3371e-02,  7.5438e-04,  2.0630e-02,\n","         1.6169e-03,  6.3129e-03,  2.3711e-02, -2.3336e-02,  1.3298e-02,\n","         1.6929e-02, -7.6259e-03, -2.5192e-02, -1.3810e-02, -1.3114e-02,\n","         5.7672e-03, -2.0410e-02,  1.3049e-03, -2.0924e-02,  1.9192e-02,\n","         6.4911e-03,  1.1237e-02, -2.6982e-02, -1.4590e-02,  1.5074e-02,\n","         1.7755e-02,  6.8836e-05,  2.8960e-02,  6.5492e-03, -6.4526e-03,\n","        -1.7838e-02, -9.7690e-03,  1.6658e-02, -3.4443e-02, -7.4412e-03,\n","        -3.4939e-02,  5.1932e-03,  3.8833e-03,  1.7178e-02,  1.9874e-02,\n","         2.0285e-02,  1.3708e-02,  1.2074e-02, -1.4510e-02,  3.2547e-02,\n","         9.8103e-04,  3.1207e-02,  3.0911e-03,  2.8954e-02,  3.4061e-02,\n","        -1.5131e-02, -3.3309e-02,  1.0564e-03, -2.7906e-02,  3.1918e-02,\n","        -2.7322e-02, -2.1863e-02, -2.1211e-02, -1.8289e-02, -1.4092e-04,\n","        -3.1349e-02, -6.5347e-03,  1.7494e-02,  3.5585e-02, -2.3558e-02,\n","         1.8044e-02,  1.8246e-03,  1.6488e-02,  2.1206e-03,  2.3520e-02,\n","         3.5143e-02,  7.5667e-03, -3.5086e-02, -1.6132e-02,  2.7012e-02,\n","         1.2543e-02, -3.1106e-02, -3.5359e-02,  3.4191e-03,  2.3654e-02,\n","         2.0159e-02,  7.6107e-03, -1.7745e-02, -1.6661e-02, -2.8765e-02,\n","         9.5250e-03,  1.6091e-02, -1.7923e-02,  2.9256e-02,  2.8095e-02,\n","         5.2800e-04,  2.3636e-02, -2.7322e-02, -6.2480e-03, -4.3347e-03,\n","        -1.0311e-02,  1.7093e-03, -2.2769e-02, -3.5393e-02,  9.9167e-03,\n","         2.2256e-02, -1.9395e-02, -2.5735e-02,  2.6626e-02, -3.3112e-02,\n","         3.0576e-02,  2.6292e-02, -2.8800e-02, -1.6690e-02, -3.2146e-02,\n","        -3.6030e-03,  6.4922e-03,  2.6992e-02,  1.8004e-02, -1.9767e-02,\n","        -3.5530e-03,  2.9089e-04,  2.9437e-04, -7.1626e-03, -3.0182e-02,\n","         1.0185e-02, -1.9814e-02, -1.6088e-02,  2.1539e-02,  3.9423e-03,\n","        -1.7734e-02,  1.9036e-02, -2.1679e-02, -2.1672e-03, -2.7933e-02,\n","        -3.4809e-02,  1.4321e-02,  1.0941e-02, -5.8556e-03,  1.8746e-02,\n","        -2.0126e-02, -1.4028e-02,  2.7493e-02,  2.2200e-02, -2.9914e-02,\n","        -6.4312e-03,  6.7892e-03, -2.6286e-02, -3.4793e-02,  3.5271e-02,\n","        -1.7848e-03, -2.5655e-02,  6.2843e-03,  2.1279e-02,  4.5552e-03,\n","        -4.4332e-03,  2.7816e-02,  6.3435e-03, -3.0658e-02,  2.7486e-02,\n","         6.3648e-04, -2.9292e-03, -2.6239e-02,  1.5762e-02, -2.6464e-02,\n","         2.6633e-02, -3.4193e-02,  1.2745e-02, -2.8574e-02,  3.3291e-02,\n","         2.4922e-03,  1.8956e-02,  7.4373e-03,  3.1659e-02, -4.3523e-03,\n","        -3.0179e-02, -1.3443e-03, -9.5118e-04, -2.4038e-02,  1.9001e-02,\n","         1.7315e-02, -3.2907e-02,  2.1788e-02, -7.6555e-03,  1.8424e-02,\n","         3.5684e-03, -4.4144e-03,  2.2026e-03,  1.3002e-02, -1.7966e-03,\n","        -2.2696e-03,  2.7091e-02,  4.0474e-03,  2.5138e-02,  1.3399e-02,\n","         1.7410e-02,  1.2133e-02, -1.2321e-02, -1.9545e-02, -7.9674e-03,\n","         2.0563e-02, -1.5978e-02,  2.3388e-02, -1.9403e-02, -2.5561e-02,\n","        -2.4557e-02, -2.1437e-02,  2.8891e-02,  2.3200e-02,  3.7684e-03,\n","        -1.5712e-02, -2.7450e-02,  1.0863e-02,  1.0099e-02, -1.3729e-02,\n","         7.1165e-03,  6.6021e-03,  2.1306e-02, -2.4786e-02,  1.7868e-02,\n","         8.7222e-03, -1.1271e-02, -1.8776e-02, -1.3188e-02,  8.9272e-03,\n","         3.6319e-03,  1.8686e-03, -2.0596e-02,  2.4475e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.305928\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.969539\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.461229\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.382342\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.445962\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.242531\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.063132\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.223464\n","Best validation accuracy improved from 0 to 0.9298333525657654, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.274370\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.366873\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.208573\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.126252\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.119940\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.104044\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.793814\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.166176\n","Best validation accuracy improved from 0.9298333525657654 to 0.9450833201408386, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.083577\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.144009\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.179618\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.063345\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.097742\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.089122\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.593010\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.170127\n","Best validation accuracy improved from 0.9450833201408386 to 0.9519166946411133, saving model...\n","\n","Fold number 2 , Valid Accuracy: 0.9519166946411133\n","\n","tensor([-3.1108e-02,  2.3562e-02, -1.3610e-02,  2.1022e-02,  1.3821e-02,\n","        -3.0939e-02,  3.3304e-02,  3.0860e-02,  2.1511e-02,  8.8448e-03,\n","         2.0658e-02, -9.6553e-04,  1.1264e-02, -8.9087e-03,  2.9645e-02,\n","         2.0959e-02, -9.1753e-03, -2.2291e-02, -1.9601e-02, -1.7701e-03,\n","        -6.6580e-03,  1.3585e-02, -4.0529e-03,  3.5579e-02, -1.2637e-03,\n","        -1.4234e-02, -3.7397e-03,  1.6321e-02, -1.4445e-02, -2.8103e-02,\n","         2.8013e-02,  2.0685e-02, -1.4932e-03, -3.2996e-03,  3.4189e-02,\n","        -6.5714e-03, -1.8120e-02, -9.9337e-03,  4.2255e-03,  4.7978e-03,\n","        -1.3997e-02, -1.1449e-03, -1.1023e-02,  2.4119e-02, -2.8557e-02,\n","         7.2616e-03, -6.5016e-03, -2.7686e-02,  8.0898e-03,  2.4436e-02,\n","        -3.0726e-02,  1.2432e-02, -1.0231e-02,  2.7347e-02, -3.2042e-02,\n","         1.3188e-02,  1.8907e-02, -2.3698e-02, -2.0206e-03, -2.5372e-02,\n","        -3.4477e-02,  2.0135e-02,  1.2449e-02,  2.0430e-03,  1.8638e-02,\n","        -8.5880e-03,  1.7140e-02,  7.1184e-03, -9.1825e-03, -4.3100e-03,\n","         3.4261e-03, -2.3917e-02, -1.2106e-02, -1.0867e-03,  1.4593e-02,\n","         1.6603e-03,  1.7173e-02,  2.8741e-02, -3.2021e-02, -1.9180e-04,\n","         7.7470e-03,  2.1300e-02,  7.8060e-03, -2.3635e-02, -1.4178e-02,\n","        -1.1676e-02, -3.2540e-02,  2.7707e-02, -8.5022e-03,  3.1863e-02,\n","        -3.4404e-02, -1.1572e-02, -2.1900e-02,  1.9367e-02,  1.9737e-02,\n","         2.5524e-02, -8.7473e-03,  2.8780e-02,  2.4359e-02, -6.9092e-03,\n","        -4.3599e-03,  1.9465e-02,  3.2965e-02, -2.6606e-02,  2.3262e-02,\n","         1.1251e-03, -2.3204e-02, -2.2275e-02, -1.8088e-02,  8.3276e-03,\n","         3.3410e-03,  2.0782e-03,  2.9487e-02, -4.2639e-03,  2.1908e-02,\n","         1.8294e-02,  3.5594e-02,  6.0315e-03,  5.9471e-03, -3.1772e-02,\n","         1.0458e-02, -2.3988e-02,  1.7502e-02, -7.3774e-03,  3.4989e-02,\n","         3.1809e-02,  3.1702e-02, -6.7919e-03, -2.3305e-02, -7.8322e-03,\n","        -2.9915e-02, -3.2329e-02,  1.1694e-02, -8.4946e-03,  3.5100e-02,\n","         1.3522e-02, -5.0619e-03, -6.1967e-03,  1.5649e-02, -3.5049e-02,\n","        -3.3163e-02, -1.4512e-02,  1.4703e-03, -1.4072e-02,  2.4506e-03,\n","         2.0077e-02, -2.4523e-02, -2.7942e-02, -2.4531e-03,  2.9514e-03,\n","        -3.1328e-02,  2.9858e-02, -2.7193e-02,  1.1742e-03, -1.9484e-03,\n","         2.6516e-02,  2.4714e-02, -2.0983e-02,  3.0581e-02, -1.8198e-02,\n","        -5.1935e-03, -5.2360e-03,  1.2961e-02, -2.3514e-02, -1.2884e-02,\n","        -2.3628e-02, -3.1636e-02, -3.5485e-02, -3.7532e-03,  5.1559e-03,\n","         3.5420e-02, -1.1816e-02,  3.0634e-02,  1.2500e-02,  3.0829e-02,\n","         1.2731e-02, -2.1480e-02, -2.0407e-02,  6.6126e-03, -1.4124e-02,\n","         2.9933e-02, -1.6640e-02,  8.2632e-03, -8.3206e-03,  7.8198e-03,\n","        -2.0932e-02,  1.2071e-02,  1.6597e-03, -1.8342e-02, -3.5629e-02,\n","         3.5667e-02,  1.8433e-02, -1.9436e-02, -2.5018e-03, -5.9386e-03,\n","         9.5387e-03,  3.4445e-02, -1.0506e-02, -1.7918e-02,  1.9254e-02,\n","         2.6210e-02,  2.9034e-02, -3.4959e-02,  3.4201e-02, -1.1839e-02,\n","        -3.0739e-04,  1.0280e-02,  5.0073e-04,  1.9889e-02,  2.0633e-02,\n","        -2.5743e-02, -2.3863e-02, -2.8247e-02, -5.2535e-03,  1.1367e-02,\n","        -1.3596e-02,  2.1466e-02,  1.9084e-02,  6.3256e-03,  9.8533e-03,\n","         7.9822e-03, -1.6440e-02,  2.8640e-02,  1.1714e-02,  5.5387e-03,\n","         3.0800e-02, -1.3671e-02,  1.3649e-02, -1.3083e-02,  1.1843e-02,\n","         1.6142e-02, -1.7152e-02, -2.3917e-02, -1.0103e-02,  1.5654e-02,\n","        -2.3061e-02, -2.6950e-02,  2.1743e-02, -3.2777e-02,  2.1591e-02,\n","         1.8160e-02,  4.0270e-03, -2.6607e-02,  1.6059e-02,  2.1128e-02,\n","        -2.4237e-02, -1.1960e-02,  2.7454e-02, -2.2509e-03, -1.1405e-02,\n","        -3.0731e-02, -2.9233e-02,  5.7814e-03,  4.6998e-03,  2.3831e-04,\n","         1.4176e-02,  1.8754e-02,  2.7220e-03, -8.9005e-03,  1.6792e-02,\n","         3.5149e-02,  1.0942e-02,  1.3230e-02, -1.4788e-02,  2.9122e-02,\n","         1.1616e-03, -6.9433e-03,  3.2809e-02, -2.0309e-02, -1.2029e-02,\n","         1.2572e-02,  2.2130e-02, -2.8379e-02, -6.8678e-03,  3.5000e-02,\n","        -2.6722e-02, -1.0942e-03, -6.6117e-03, -4.6719e-03, -3.7166e-03,\n","         3.8235e-03, -3.4946e-02, -1.0788e-02, -2.6446e-02, -9.3842e-03,\n","         1.4883e-02,  2.6005e-03, -3.5277e-02, -1.0719e-02,  3.1520e-02,\n","         5.9205e-04, -1.4752e-02,  1.2906e-02,  7.7259e-03,  1.9088e-02,\n","        -8.6026e-03, -1.1294e-03, -1.2804e-02, -3.5588e-02,  2.2912e-02,\n","         5.4993e-03, -2.6627e-02, -1.8863e-02, -3.2767e-02,  1.5669e-02,\n","         1.4470e-02, -2.1004e-02,  1.0201e-02, -2.4873e-02,  1.2721e-02,\n","        -2.0890e-02,  2.9335e-02,  8.4590e-03,  3.1612e-02, -1.6410e-02,\n","        -4.3074e-03,  3.5622e-02,  3.4801e-03, -1.2590e-02,  2.1668e-02,\n","         2.0644e-02, -3.4191e-02, -1.5032e-02,  3.8574e-03,  3.5194e-02,\n","        -1.0082e-02, -6.8383e-03,  2.4914e-02,  3.9131e-03,  2.8826e-02,\n","        -2.3657e-02,  1.7922e-02,  2.3088e-02, -3.1920e-02,  1.7088e-04,\n","        -2.6320e-02,  2.9709e-03,  6.6105e-03,  9.6403e-03, -2.2107e-02,\n","         1.8260e-03, -5.4769e-03, -3.6621e-03, -2.8487e-02, -5.6615e-03,\n","        -2.8985e-02,  1.5701e-02,  7.6726e-03, -2.7654e-02, -3.5044e-02,\n","        -2.1235e-03,  5.2059e-03,  6.7579e-03,  1.6026e-02,  3.0274e-02,\n","         2.3207e-02,  7.5014e-03,  3.3570e-02, -1.8227e-02, -4.9395e-03,\n","        -3.4447e-02, -1.6357e-02,  2.2363e-02, -1.2721e-02,  1.6730e-02,\n","        -3.4551e-02, -6.7659e-03,  1.1813e-02, -2.8829e-02, -2.1908e-03,\n","        -1.0931e-02,  3.4059e-02,  2.8418e-02,  6.1077e-03,  9.0828e-03,\n","        -1.5658e-02,  1.1254e-04,  9.6228e-03,  2.5801e-02,  2.2231e-02,\n","         2.1628e-02,  2.9192e-02,  2.7201e-02, -5.3290e-04, -3.2435e-02,\n","         1.1606e-02,  2.0386e-03,  3.0118e-02,  1.8588e-02, -3.0772e-02,\n","         1.8126e-02, -2.2488e-02, -9.5115e-03, -3.4270e-02,  2.9253e-02,\n","        -1.7760e-02, -3.5102e-02,  4.2821e-03, -1.8724e-02,  2.3523e-02,\n","        -1.1340e-02,  1.7731e-02, -3.8702e-03, -1.3534e-02,  8.8067e-04,\n","        -2.9126e-02,  4.9423e-03, -1.6016e-02, -6.0132e-03,  6.4760e-03,\n","         1.3887e-02, -3.5304e-02,  1.2898e-02,  9.4187e-03,  2.3850e-02,\n","        -3.3215e-02,  8.0435e-03, -5.0447e-03, -3.0127e-02,  2.7067e-02,\n","        -2.0843e-02, -9.8425e-03, -4.1286e-03,  2.3763e-02,  1.2256e-02,\n","         8.4853e-03,  1.9214e-02, -3.5586e-02,  1.6501e-02, -2.3770e-03,\n","         2.6716e-02,  2.9882e-02,  2.4439e-02,  1.7453e-02, -2.4469e-02,\n","         1.3200e-02, -1.3409e-02, -2.0289e-02, -2.0192e-02, -1.8638e-02,\n","        -3.2642e-02, -1.0186e-02,  2.3828e-02,  3.0878e-02,  1.1174e-02,\n","        -1.1620e-02, -1.3368e-02, -4.8857e-03, -6.9446e-03, -2.1758e-02,\n","         2.5447e-02, -3.2655e-04, -3.2778e-02, -1.2407e-02, -2.5746e-03,\n","         1.1249e-02,  8.4687e-03,  2.7508e-02,  7.3380e-03, -2.4991e-02,\n","         4.0053e-04,  2.5850e-02, -1.6093e-02,  1.3535e-02,  2.9198e-02,\n","        -3.0009e-02, -6.0037e-03, -3.4853e-02,  9.3856e-03, -4.8135e-03,\n","         3.5677e-02,  5.6428e-03,  9.1762e-03,  2.9572e-02, -3.3303e-02,\n","        -2.9699e-02,  9.8307e-03, -1.9110e-02,  3.1789e-02,  2.1047e-02,\n","        -2.7718e-02,  1.9382e-02, -4.4371e-03,  3.1035e-03,  1.2912e-03,\n","         2.0250e-02, -1.5161e-03, -1.5698e-02, -3.3978e-02, -1.6710e-02,\n","        -1.1362e-03, -3.1087e-02,  7.2027e-03,  3.5034e-02,  6.0924e-03,\n","         2.1756e-02,  3.4730e-02,  8.4615e-03, -3.2396e-02, -2.5138e-02,\n","        -1.5917e-02, -2.4192e-02, -2.4682e-02, -1.0266e-02, -2.8302e-02,\n","        -4.9269e-03, -2.7959e-02, -3.3961e-02, -6.1202e-03,  3.1271e-02,\n","        -1.8146e-02,  2.5531e-02,  2.4184e-02,  2.6373e-02, -3.4721e-02,\n","        -2.3258e-02,  1.5469e-02,  3.3899e-02,  3.4953e-02, -9.5330e-03,\n","        -7.6447e-03, -1.5982e-02,  1.7682e-02,  1.4647e-03,  2.5155e-02,\n","         2.1435e-02, -2.8598e-02,  3.5195e-02, -3.9483e-03,  9.3768e-03,\n","        -1.9650e-02, -2.9577e-02, -2.7503e-02, -8.2164e-03, -3.2761e-03,\n","        -2.9794e-02,  3.3573e-02, -2.5794e-02, -1.7871e-02, -4.3952e-03,\n","         3.0800e-02, -1.0957e-02,  2.4444e-02,  3.1848e-02,  1.5286e-02,\n","         3.1893e-02, -2.9608e-03,  3.1325e-02, -1.1912e-02,  1.5376e-02,\n","        -8.2521e-03,  2.4302e-03, -4.7494e-03,  1.8111e-02, -2.9871e-02,\n","        -2.7772e-02, -2.1450e-02,  8.2388e-03,  2.4070e-02,  1.5319e-02,\n","         1.1052e-02, -1.6135e-02, -1.3371e-02,  7.5438e-04,  2.0630e-02,\n","         1.6169e-03,  6.3129e-03,  2.3711e-02, -2.3336e-02,  1.3298e-02,\n","         1.6929e-02, -7.6259e-03, -2.5192e-02, -1.3810e-02, -1.3114e-02,\n","         5.7672e-03, -2.0410e-02,  1.3049e-03, -2.0924e-02,  1.9192e-02,\n","         6.4911e-03,  1.1237e-02, -2.6982e-02, -1.4590e-02,  1.5074e-02,\n","         1.7755e-02,  6.8836e-05,  2.8960e-02,  6.5492e-03, -6.4526e-03,\n","        -1.7838e-02, -9.7690e-03,  1.6658e-02, -3.4443e-02, -7.4412e-03,\n","        -3.4939e-02,  5.1932e-03,  3.8833e-03,  1.7178e-02,  1.9874e-02,\n","         2.0285e-02,  1.3708e-02,  1.2074e-02, -1.4510e-02,  3.2547e-02,\n","         9.8103e-04,  3.1207e-02,  3.0911e-03,  2.8954e-02,  3.4061e-02,\n","        -1.5131e-02, -3.3309e-02,  1.0564e-03, -2.7906e-02,  3.1918e-02,\n","        -2.7322e-02, -2.1863e-02, -2.1211e-02, -1.8289e-02, -1.4092e-04,\n","        -3.1349e-02, -6.5347e-03,  1.7494e-02,  3.5585e-02, -2.3558e-02,\n","         1.8044e-02,  1.8246e-03,  1.6488e-02,  2.1206e-03,  2.3520e-02,\n","         3.5143e-02,  7.5667e-03, -3.5086e-02, -1.6132e-02,  2.7012e-02,\n","         1.2543e-02, -3.1106e-02, -3.5359e-02,  3.4191e-03,  2.3654e-02,\n","         2.0159e-02,  7.6107e-03, -1.7745e-02, -1.6661e-02, -2.8765e-02,\n","         9.5250e-03,  1.6091e-02, -1.7923e-02,  2.9256e-02,  2.8095e-02,\n","         5.2800e-04,  2.3636e-02, -2.7322e-02, -6.2480e-03, -4.3347e-03,\n","        -1.0311e-02,  1.7093e-03, -2.2769e-02, -3.5393e-02,  9.9167e-03,\n","         2.2256e-02, -1.9395e-02, -2.5735e-02,  2.6626e-02, -3.3112e-02,\n","         3.0576e-02,  2.6292e-02, -2.8800e-02, -1.6690e-02, -3.2146e-02,\n","        -3.6030e-03,  6.4922e-03,  2.6992e-02,  1.8004e-02, -1.9767e-02,\n","        -3.5530e-03,  2.9089e-04,  2.9437e-04, -7.1626e-03, -3.0182e-02,\n","         1.0185e-02, -1.9814e-02, -1.6088e-02,  2.1539e-02,  3.9423e-03,\n","        -1.7734e-02,  1.9036e-02, -2.1679e-02, -2.1672e-03, -2.7933e-02,\n","        -3.4809e-02,  1.4321e-02,  1.0941e-02, -5.8556e-03,  1.8746e-02,\n","        -2.0126e-02, -1.4028e-02,  2.7493e-02,  2.2200e-02, -2.9914e-02,\n","        -6.4312e-03,  6.7892e-03, -2.6286e-02, -3.4793e-02,  3.5271e-02,\n","        -1.7848e-03, -2.5655e-02,  6.2843e-03,  2.1279e-02,  4.5552e-03,\n","        -4.4332e-03,  2.7816e-02,  6.3435e-03, -3.0658e-02,  2.7486e-02,\n","         6.3648e-04, -2.9292e-03, -2.6239e-02,  1.5762e-02, -2.6464e-02,\n","         2.6633e-02, -3.4193e-02,  1.2745e-02, -2.8574e-02,  3.3291e-02,\n","         2.4922e-03,  1.8956e-02,  7.4373e-03,  3.1659e-02, -4.3523e-03,\n","        -3.0179e-02, -1.3443e-03, -9.5118e-04, -2.4038e-02,  1.9001e-02,\n","         1.7315e-02, -3.2907e-02,  2.1788e-02, -7.6555e-03,  1.8424e-02,\n","         3.5684e-03, -4.4144e-03,  2.2026e-03,  1.3002e-02, -1.7966e-03,\n","        -2.2696e-03,  2.7091e-02,  4.0474e-03,  2.5138e-02,  1.3399e-02,\n","         1.7410e-02,  1.2133e-02, -1.2321e-02, -1.9545e-02, -7.9674e-03,\n","         2.0563e-02, -1.5978e-02,  2.3388e-02, -1.9403e-02, -2.5561e-02,\n","        -2.4557e-02, -2.1437e-02,  2.8891e-02,  2.3200e-02,  3.7684e-03,\n","        -1.5712e-02, -2.7450e-02,  1.0863e-02,  1.0099e-02, -1.3729e-02,\n","         7.1165e-03,  6.6021e-03,  2.1306e-02, -2.4786e-02,  1.7868e-02,\n","         8.7222e-03, -1.1271e-02, -1.8776e-02, -1.3188e-02,  8.9272e-03,\n","         3.6319e-03,  1.8686e-03, -2.0596e-02,  2.4475e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.311916\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 1.176116\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.600500\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.475224\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.346757\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.263315\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.319942\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.169453\n","Best validation accuracy improved from 0 to 0.9227499961853027, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.196062\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.284701\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.173975\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.093727\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.111221\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.133493\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.874540\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.104338\n","Best validation accuracy improved from 0.9227499961853027 to 0.9471666812896729, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.057756\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.351367\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.244622\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.058632\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.046540\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.049029\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.578600\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.219105\n","Best validation accuracy improved from 0.9471666812896729 to 0.9578333497047424, saving model...\n","\n","Fold number 3 , Valid Accuracy: 0.9578333497047424\n","\n","tensor([-3.1108e-02,  2.3562e-02, -1.3610e-02,  2.1022e-02,  1.3821e-02,\n","        -3.0939e-02,  3.3304e-02,  3.0860e-02,  2.1511e-02,  8.8448e-03,\n","         2.0658e-02, -9.6553e-04,  1.1264e-02, -8.9087e-03,  2.9645e-02,\n","         2.0959e-02, -9.1753e-03, -2.2291e-02, -1.9601e-02, -1.7701e-03,\n","        -6.6580e-03,  1.3585e-02, -4.0529e-03,  3.5579e-02, -1.2637e-03,\n","        -1.4234e-02, -3.7397e-03,  1.6321e-02, -1.4445e-02, -2.8103e-02,\n","         2.8013e-02,  2.0685e-02, -1.4932e-03, -3.2996e-03,  3.4189e-02,\n","        -6.5714e-03, -1.8120e-02, -9.9337e-03,  4.2255e-03,  4.7978e-03,\n","        -1.3997e-02, -1.1449e-03, -1.1023e-02,  2.4119e-02, -2.8557e-02,\n","         7.2616e-03, -6.5016e-03, -2.7686e-02,  8.0898e-03,  2.4436e-02,\n","        -3.0726e-02,  1.2432e-02, -1.0231e-02,  2.7347e-02, -3.2042e-02,\n","         1.3188e-02,  1.8907e-02, -2.3698e-02, -2.0206e-03, -2.5372e-02,\n","        -3.4477e-02,  2.0135e-02,  1.2449e-02,  2.0430e-03,  1.8638e-02,\n","        -8.5880e-03,  1.7140e-02,  7.1184e-03, -9.1825e-03, -4.3100e-03,\n","         3.4261e-03, -2.3917e-02, -1.2106e-02, -1.0867e-03,  1.4593e-02,\n","         1.6603e-03,  1.7173e-02,  2.8741e-02, -3.2021e-02, -1.9180e-04,\n","         7.7470e-03,  2.1300e-02,  7.8060e-03, -2.3635e-02, -1.4178e-02,\n","        -1.1676e-02, -3.2540e-02,  2.7707e-02, -8.5022e-03,  3.1863e-02,\n","        -3.4404e-02, -1.1572e-02, -2.1900e-02,  1.9367e-02,  1.9737e-02,\n","         2.5524e-02, -8.7473e-03,  2.8780e-02,  2.4359e-02, -6.9092e-03,\n","        -4.3599e-03,  1.9465e-02,  3.2965e-02, -2.6606e-02,  2.3262e-02,\n","         1.1251e-03, -2.3204e-02, -2.2275e-02, -1.8088e-02,  8.3276e-03,\n","         3.3410e-03,  2.0782e-03,  2.9487e-02, -4.2639e-03,  2.1908e-02,\n","         1.8294e-02,  3.5594e-02,  6.0315e-03,  5.9471e-03, -3.1772e-02,\n","         1.0458e-02, -2.3988e-02,  1.7502e-02, -7.3774e-03,  3.4989e-02,\n","         3.1809e-02,  3.1702e-02, -6.7919e-03, -2.3305e-02, -7.8322e-03,\n","        -2.9915e-02, -3.2329e-02,  1.1694e-02, -8.4946e-03,  3.5100e-02,\n","         1.3522e-02, -5.0619e-03, -6.1967e-03,  1.5649e-02, -3.5049e-02,\n","        -3.3163e-02, -1.4512e-02,  1.4703e-03, -1.4072e-02,  2.4506e-03,\n","         2.0077e-02, -2.4523e-02, -2.7942e-02, -2.4531e-03,  2.9514e-03,\n","        -3.1328e-02,  2.9858e-02, -2.7193e-02,  1.1742e-03, -1.9484e-03,\n","         2.6516e-02,  2.4714e-02, -2.0983e-02,  3.0581e-02, -1.8198e-02,\n","        -5.1935e-03, -5.2360e-03,  1.2961e-02, -2.3514e-02, -1.2884e-02,\n","        -2.3628e-02, -3.1636e-02, -3.5485e-02, -3.7532e-03,  5.1559e-03,\n","         3.5420e-02, -1.1816e-02,  3.0634e-02,  1.2500e-02,  3.0829e-02,\n","         1.2731e-02, -2.1480e-02, -2.0407e-02,  6.6126e-03, -1.4124e-02,\n","         2.9933e-02, -1.6640e-02,  8.2632e-03, -8.3206e-03,  7.8198e-03,\n","        -2.0932e-02,  1.2071e-02,  1.6597e-03, -1.8342e-02, -3.5629e-02,\n","         3.5667e-02,  1.8433e-02, -1.9436e-02, -2.5018e-03, -5.9386e-03,\n","         9.5387e-03,  3.4445e-02, -1.0506e-02, -1.7918e-02,  1.9254e-02,\n","         2.6210e-02,  2.9034e-02, -3.4959e-02,  3.4201e-02, -1.1839e-02,\n","        -3.0739e-04,  1.0280e-02,  5.0073e-04,  1.9889e-02,  2.0633e-02,\n","        -2.5743e-02, -2.3863e-02, -2.8247e-02, -5.2535e-03,  1.1367e-02,\n","        -1.3596e-02,  2.1466e-02,  1.9084e-02,  6.3256e-03,  9.8533e-03,\n","         7.9822e-03, -1.6440e-02,  2.8640e-02,  1.1714e-02,  5.5387e-03,\n","         3.0800e-02, -1.3671e-02,  1.3649e-02, -1.3083e-02,  1.1843e-02,\n","         1.6142e-02, -1.7152e-02, -2.3917e-02, -1.0103e-02,  1.5654e-02,\n","        -2.3061e-02, -2.6950e-02,  2.1743e-02, -3.2777e-02,  2.1591e-02,\n","         1.8160e-02,  4.0270e-03, -2.6607e-02,  1.6059e-02,  2.1128e-02,\n","        -2.4237e-02, -1.1960e-02,  2.7454e-02, -2.2509e-03, -1.1405e-02,\n","        -3.0731e-02, -2.9233e-02,  5.7814e-03,  4.6998e-03,  2.3831e-04,\n","         1.4176e-02,  1.8754e-02,  2.7220e-03, -8.9005e-03,  1.6792e-02,\n","         3.5149e-02,  1.0942e-02,  1.3230e-02, -1.4788e-02,  2.9122e-02,\n","         1.1616e-03, -6.9433e-03,  3.2809e-02, -2.0309e-02, -1.2029e-02,\n","         1.2572e-02,  2.2130e-02, -2.8379e-02, -6.8678e-03,  3.5000e-02,\n","        -2.6722e-02, -1.0942e-03, -6.6117e-03, -4.6719e-03, -3.7166e-03,\n","         3.8235e-03, -3.4946e-02, -1.0788e-02, -2.6446e-02, -9.3842e-03,\n","         1.4883e-02,  2.6005e-03, -3.5277e-02, -1.0719e-02,  3.1520e-02,\n","         5.9205e-04, -1.4752e-02,  1.2906e-02,  7.7259e-03,  1.9088e-02,\n","        -8.6026e-03, -1.1294e-03, -1.2804e-02, -3.5588e-02,  2.2912e-02,\n","         5.4993e-03, -2.6627e-02, -1.8863e-02, -3.2767e-02,  1.5669e-02,\n","         1.4470e-02, -2.1004e-02,  1.0201e-02, -2.4873e-02,  1.2721e-02,\n","        -2.0890e-02,  2.9335e-02,  8.4590e-03,  3.1612e-02, -1.6410e-02,\n","        -4.3074e-03,  3.5622e-02,  3.4801e-03, -1.2590e-02,  2.1668e-02,\n","         2.0644e-02, -3.4191e-02, -1.5032e-02,  3.8574e-03,  3.5194e-02,\n","        -1.0082e-02, -6.8383e-03,  2.4914e-02,  3.9131e-03,  2.8826e-02,\n","        -2.3657e-02,  1.7922e-02,  2.3088e-02, -3.1920e-02,  1.7088e-04,\n","        -2.6320e-02,  2.9709e-03,  6.6105e-03,  9.6403e-03, -2.2107e-02,\n","         1.8260e-03, -5.4769e-03, -3.6621e-03, -2.8487e-02, -5.6615e-03,\n","        -2.8985e-02,  1.5701e-02,  7.6726e-03, -2.7654e-02, -3.5044e-02,\n","        -2.1235e-03,  5.2059e-03,  6.7579e-03,  1.6026e-02,  3.0274e-02,\n","         2.3207e-02,  7.5014e-03,  3.3570e-02, -1.8227e-02, -4.9395e-03,\n","        -3.4447e-02, -1.6357e-02,  2.2363e-02, -1.2721e-02,  1.6730e-02,\n","        -3.4551e-02, -6.7659e-03,  1.1813e-02, -2.8829e-02, -2.1908e-03,\n","        -1.0931e-02,  3.4059e-02,  2.8418e-02,  6.1077e-03,  9.0828e-03,\n","        -1.5658e-02,  1.1254e-04,  9.6228e-03,  2.5801e-02,  2.2231e-02,\n","         2.1628e-02,  2.9192e-02,  2.7201e-02, -5.3290e-04, -3.2435e-02,\n","         1.1606e-02,  2.0386e-03,  3.0118e-02,  1.8588e-02, -3.0772e-02,\n","         1.8126e-02, -2.2488e-02, -9.5115e-03, -3.4270e-02,  2.9253e-02,\n","        -1.7760e-02, -3.5102e-02,  4.2821e-03, -1.8724e-02,  2.3523e-02,\n","        -1.1340e-02,  1.7731e-02, -3.8702e-03, -1.3534e-02,  8.8067e-04,\n","        -2.9126e-02,  4.9423e-03, -1.6016e-02, -6.0132e-03,  6.4760e-03,\n","         1.3887e-02, -3.5304e-02,  1.2898e-02,  9.4187e-03,  2.3850e-02,\n","        -3.3215e-02,  8.0435e-03, -5.0447e-03, -3.0127e-02,  2.7067e-02,\n","        -2.0843e-02, -9.8425e-03, -4.1286e-03,  2.3763e-02,  1.2256e-02,\n","         8.4853e-03,  1.9214e-02, -3.5586e-02,  1.6501e-02, -2.3770e-03,\n","         2.6716e-02,  2.9882e-02,  2.4439e-02,  1.7453e-02, -2.4469e-02,\n","         1.3200e-02, -1.3409e-02, -2.0289e-02, -2.0192e-02, -1.8638e-02,\n","        -3.2642e-02, -1.0186e-02,  2.3828e-02,  3.0878e-02,  1.1174e-02,\n","        -1.1620e-02, -1.3368e-02, -4.8857e-03, -6.9446e-03, -2.1758e-02,\n","         2.5447e-02, -3.2655e-04, -3.2778e-02, -1.2407e-02, -2.5746e-03,\n","         1.1249e-02,  8.4687e-03,  2.7508e-02,  7.3380e-03, -2.4991e-02,\n","         4.0053e-04,  2.5850e-02, -1.6093e-02,  1.3535e-02,  2.9198e-02,\n","        -3.0009e-02, -6.0037e-03, -3.4853e-02,  9.3856e-03, -4.8135e-03,\n","         3.5677e-02,  5.6428e-03,  9.1762e-03,  2.9572e-02, -3.3303e-02,\n","        -2.9699e-02,  9.8307e-03, -1.9110e-02,  3.1789e-02,  2.1047e-02,\n","        -2.7718e-02,  1.9382e-02, -4.4371e-03,  3.1035e-03,  1.2912e-03,\n","         2.0250e-02, -1.5161e-03, -1.5698e-02, -3.3978e-02, -1.6710e-02,\n","        -1.1362e-03, -3.1087e-02,  7.2027e-03,  3.5034e-02,  6.0924e-03,\n","         2.1756e-02,  3.4730e-02,  8.4615e-03, -3.2396e-02, -2.5138e-02,\n","        -1.5917e-02, -2.4192e-02, -2.4682e-02, -1.0266e-02, -2.8302e-02,\n","        -4.9269e-03, -2.7959e-02, -3.3961e-02, -6.1202e-03,  3.1271e-02,\n","        -1.8146e-02,  2.5531e-02,  2.4184e-02,  2.6373e-02, -3.4721e-02,\n","        -2.3258e-02,  1.5469e-02,  3.3899e-02,  3.4953e-02, -9.5330e-03,\n","        -7.6447e-03, -1.5982e-02,  1.7682e-02,  1.4647e-03,  2.5155e-02,\n","         2.1435e-02, -2.8598e-02,  3.5195e-02, -3.9483e-03,  9.3768e-03,\n","        -1.9650e-02, -2.9577e-02, -2.7503e-02, -8.2164e-03, -3.2761e-03,\n","        -2.9794e-02,  3.3573e-02, -2.5794e-02, -1.7871e-02, -4.3952e-03,\n","         3.0800e-02, -1.0957e-02,  2.4444e-02,  3.1848e-02,  1.5286e-02,\n","         3.1893e-02, -2.9608e-03,  3.1325e-02, -1.1912e-02,  1.5376e-02,\n","        -8.2521e-03,  2.4302e-03, -4.7494e-03,  1.8111e-02, -2.9871e-02,\n","        -2.7772e-02, -2.1450e-02,  8.2388e-03,  2.4070e-02,  1.5319e-02,\n","         1.1052e-02, -1.6135e-02, -1.3371e-02,  7.5438e-04,  2.0630e-02,\n","         1.6169e-03,  6.3129e-03,  2.3711e-02, -2.3336e-02,  1.3298e-02,\n","         1.6929e-02, -7.6259e-03, -2.5192e-02, -1.3810e-02, -1.3114e-02,\n","         5.7672e-03, -2.0410e-02,  1.3049e-03, -2.0924e-02,  1.9192e-02,\n","         6.4911e-03,  1.1237e-02, -2.6982e-02, -1.4590e-02,  1.5074e-02,\n","         1.7755e-02,  6.8836e-05,  2.8960e-02,  6.5492e-03, -6.4526e-03,\n","        -1.7838e-02, -9.7690e-03,  1.6658e-02, -3.4443e-02, -7.4412e-03,\n","        -3.4939e-02,  5.1932e-03,  3.8833e-03,  1.7178e-02,  1.9874e-02,\n","         2.0285e-02,  1.3708e-02,  1.2074e-02, -1.4510e-02,  3.2547e-02,\n","         9.8103e-04,  3.1207e-02,  3.0911e-03,  2.8954e-02,  3.4061e-02,\n","        -1.5131e-02, -3.3309e-02,  1.0564e-03, -2.7906e-02,  3.1918e-02,\n","        -2.7322e-02, -2.1863e-02, -2.1211e-02, -1.8289e-02, -1.4092e-04,\n","        -3.1349e-02, -6.5347e-03,  1.7494e-02,  3.5585e-02, -2.3558e-02,\n","         1.8044e-02,  1.8246e-03,  1.6488e-02,  2.1206e-03,  2.3520e-02,\n","         3.5143e-02,  7.5667e-03, -3.5086e-02, -1.6132e-02,  2.7012e-02,\n","         1.2543e-02, -3.1106e-02, -3.5359e-02,  3.4191e-03,  2.3654e-02,\n","         2.0159e-02,  7.6107e-03, -1.7745e-02, -1.6661e-02, -2.8765e-02,\n","         9.5250e-03,  1.6091e-02, -1.7923e-02,  2.9256e-02,  2.8095e-02,\n","         5.2800e-04,  2.3636e-02, -2.7322e-02, -6.2480e-03, -4.3347e-03,\n","        -1.0311e-02,  1.7093e-03, -2.2769e-02, -3.5393e-02,  9.9167e-03,\n","         2.2256e-02, -1.9395e-02, -2.5735e-02,  2.6626e-02, -3.3112e-02,\n","         3.0576e-02,  2.6292e-02, -2.8800e-02, -1.6690e-02, -3.2146e-02,\n","        -3.6030e-03,  6.4922e-03,  2.6992e-02,  1.8004e-02, -1.9767e-02,\n","        -3.5530e-03,  2.9089e-04,  2.9437e-04, -7.1626e-03, -3.0182e-02,\n","         1.0185e-02, -1.9814e-02, -1.6088e-02,  2.1539e-02,  3.9423e-03,\n","        -1.7734e-02,  1.9036e-02, -2.1679e-02, -2.1672e-03, -2.7933e-02,\n","        -3.4809e-02,  1.4321e-02,  1.0941e-02, -5.8556e-03,  1.8746e-02,\n","        -2.0126e-02, -1.4028e-02,  2.7493e-02,  2.2200e-02, -2.9914e-02,\n","        -6.4312e-03,  6.7892e-03, -2.6286e-02, -3.4793e-02,  3.5271e-02,\n","        -1.7848e-03, -2.5655e-02,  6.2843e-03,  2.1279e-02,  4.5552e-03,\n","        -4.4332e-03,  2.7816e-02,  6.3435e-03, -3.0658e-02,  2.7486e-02,\n","         6.3648e-04, -2.9292e-03, -2.6239e-02,  1.5762e-02, -2.6464e-02,\n","         2.6633e-02, -3.4193e-02,  1.2745e-02, -2.8574e-02,  3.3291e-02,\n","         2.4922e-03,  1.8956e-02,  7.4373e-03,  3.1659e-02, -4.3523e-03,\n","        -3.0179e-02, -1.3443e-03, -9.5118e-04, -2.4038e-02,  1.9001e-02,\n","         1.7315e-02, -3.2907e-02,  2.1788e-02, -7.6555e-03,  1.8424e-02,\n","         3.5684e-03, -4.4144e-03,  2.2026e-03,  1.3002e-02, -1.7966e-03,\n","        -2.2696e-03,  2.7091e-02,  4.0474e-03,  2.5138e-02,  1.3399e-02,\n","         1.7410e-02,  1.2133e-02, -1.2321e-02, -1.9545e-02, -7.9674e-03,\n","         2.0563e-02, -1.5978e-02,  2.3388e-02, -1.9403e-02, -2.5561e-02,\n","        -2.4557e-02, -2.1437e-02,  2.8891e-02,  2.3200e-02,  3.7684e-03,\n","        -1.5712e-02, -2.7450e-02,  1.0863e-02,  1.0099e-02, -1.3729e-02,\n","         7.1165e-03,  6.6021e-03,  2.1306e-02, -2.4786e-02,  1.7868e-02,\n","         8.7222e-03, -1.1271e-02, -1.8776e-02, -1.3188e-02,  8.9272e-03,\n","         3.6319e-03,  1.8686e-03, -2.0596e-02,  2.4475e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.310741\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.778223\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.602814\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.304696\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.227373\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.539758\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.795320\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.198354\n","Best validation accuracy improved from 0 to 0.9271666407585144, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.193116\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.143870\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.142978\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.096580\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.101931\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.475105\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.774082\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.167223\n","Best validation accuracy improved from 0.9271666407585144 to 0.937666654586792, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.177139\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.226806\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.118477\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.036122\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.017962\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.213500\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.562125\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.142166\n","Best validation accuracy improved from 0.937666654586792 to 0.9486666917800903, saving model...\n","\n","Fold number 4 , Valid Accuracy: 0.9486666917800903\n","\n","tensor([-3.1108e-02,  2.3562e-02, -1.3610e-02,  2.1022e-02,  1.3821e-02,\n","        -3.0939e-02,  3.3304e-02,  3.0860e-02,  2.1511e-02,  8.8448e-03,\n","         2.0658e-02, -9.6553e-04,  1.1264e-02, -8.9087e-03,  2.9645e-02,\n","         2.0959e-02, -9.1753e-03, -2.2291e-02, -1.9601e-02, -1.7701e-03,\n","        -6.6580e-03,  1.3585e-02, -4.0529e-03,  3.5579e-02, -1.2637e-03,\n","        -1.4234e-02, -3.7397e-03,  1.6321e-02, -1.4445e-02, -2.8103e-02,\n","         2.8013e-02,  2.0685e-02, -1.4932e-03, -3.2996e-03,  3.4189e-02,\n","        -6.5714e-03, -1.8120e-02, -9.9337e-03,  4.2255e-03,  4.7978e-03,\n","        -1.3997e-02, -1.1449e-03, -1.1023e-02,  2.4119e-02, -2.8557e-02,\n","         7.2616e-03, -6.5016e-03, -2.7686e-02,  8.0898e-03,  2.4436e-02,\n","        -3.0726e-02,  1.2432e-02, -1.0231e-02,  2.7347e-02, -3.2042e-02,\n","         1.3188e-02,  1.8907e-02, -2.3698e-02, -2.0206e-03, -2.5372e-02,\n","        -3.4477e-02,  2.0135e-02,  1.2449e-02,  2.0430e-03,  1.8638e-02,\n","        -8.5880e-03,  1.7140e-02,  7.1184e-03, -9.1825e-03, -4.3100e-03,\n","         3.4261e-03, -2.3917e-02, -1.2106e-02, -1.0867e-03,  1.4593e-02,\n","         1.6603e-03,  1.7173e-02,  2.8741e-02, -3.2021e-02, -1.9180e-04,\n","         7.7470e-03,  2.1300e-02,  7.8060e-03, -2.3635e-02, -1.4178e-02,\n","        -1.1676e-02, -3.2540e-02,  2.7707e-02, -8.5022e-03,  3.1863e-02,\n","        -3.4404e-02, -1.1572e-02, -2.1900e-02,  1.9367e-02,  1.9737e-02,\n","         2.5524e-02, -8.7473e-03,  2.8780e-02,  2.4359e-02, -6.9092e-03,\n","        -4.3599e-03,  1.9465e-02,  3.2965e-02, -2.6606e-02,  2.3262e-02,\n","         1.1251e-03, -2.3204e-02, -2.2275e-02, -1.8088e-02,  8.3276e-03,\n","         3.3410e-03,  2.0782e-03,  2.9487e-02, -4.2639e-03,  2.1908e-02,\n","         1.8294e-02,  3.5594e-02,  6.0315e-03,  5.9471e-03, -3.1772e-02,\n","         1.0458e-02, -2.3988e-02,  1.7502e-02, -7.3774e-03,  3.4989e-02,\n","         3.1809e-02,  3.1702e-02, -6.7919e-03, -2.3305e-02, -7.8322e-03,\n","        -2.9915e-02, -3.2329e-02,  1.1694e-02, -8.4946e-03,  3.5100e-02,\n","         1.3522e-02, -5.0619e-03, -6.1967e-03,  1.5649e-02, -3.5049e-02,\n","        -3.3163e-02, -1.4512e-02,  1.4703e-03, -1.4072e-02,  2.4506e-03,\n","         2.0077e-02, -2.4523e-02, -2.7942e-02, -2.4531e-03,  2.9514e-03,\n","        -3.1328e-02,  2.9858e-02, -2.7193e-02,  1.1742e-03, -1.9484e-03,\n","         2.6516e-02,  2.4714e-02, -2.0983e-02,  3.0581e-02, -1.8198e-02,\n","        -5.1935e-03, -5.2360e-03,  1.2961e-02, -2.3514e-02, -1.2884e-02,\n","        -2.3628e-02, -3.1636e-02, -3.5485e-02, -3.7532e-03,  5.1559e-03,\n","         3.5420e-02, -1.1816e-02,  3.0634e-02,  1.2500e-02,  3.0829e-02,\n","         1.2731e-02, -2.1480e-02, -2.0407e-02,  6.6126e-03, -1.4124e-02,\n","         2.9933e-02, -1.6640e-02,  8.2632e-03, -8.3206e-03,  7.8198e-03,\n","        -2.0932e-02,  1.2071e-02,  1.6597e-03, -1.8342e-02, -3.5629e-02,\n","         3.5667e-02,  1.8433e-02, -1.9436e-02, -2.5018e-03, -5.9386e-03,\n","         9.5387e-03,  3.4445e-02, -1.0506e-02, -1.7918e-02,  1.9254e-02,\n","         2.6210e-02,  2.9034e-02, -3.4959e-02,  3.4201e-02, -1.1839e-02,\n","        -3.0739e-04,  1.0280e-02,  5.0073e-04,  1.9889e-02,  2.0633e-02,\n","        -2.5743e-02, -2.3863e-02, -2.8247e-02, -5.2535e-03,  1.1367e-02,\n","        -1.3596e-02,  2.1466e-02,  1.9084e-02,  6.3256e-03,  9.8533e-03,\n","         7.9822e-03, -1.6440e-02,  2.8640e-02,  1.1714e-02,  5.5387e-03,\n","         3.0800e-02, -1.3671e-02,  1.3649e-02, -1.3083e-02,  1.1843e-02,\n","         1.6142e-02, -1.7152e-02, -2.3917e-02, -1.0103e-02,  1.5654e-02,\n","        -2.3061e-02, -2.6950e-02,  2.1743e-02, -3.2777e-02,  2.1591e-02,\n","         1.8160e-02,  4.0270e-03, -2.6607e-02,  1.6059e-02,  2.1128e-02,\n","        -2.4237e-02, -1.1960e-02,  2.7454e-02, -2.2509e-03, -1.1405e-02,\n","        -3.0731e-02, -2.9233e-02,  5.7814e-03,  4.6998e-03,  2.3831e-04,\n","         1.4176e-02,  1.8754e-02,  2.7220e-03, -8.9005e-03,  1.6792e-02,\n","         3.5149e-02,  1.0942e-02,  1.3230e-02, -1.4788e-02,  2.9122e-02,\n","         1.1616e-03, -6.9433e-03,  3.2809e-02, -2.0309e-02, -1.2029e-02,\n","         1.2572e-02,  2.2130e-02, -2.8379e-02, -6.8678e-03,  3.5000e-02,\n","        -2.6722e-02, -1.0942e-03, -6.6117e-03, -4.6719e-03, -3.7166e-03,\n","         3.8235e-03, -3.4946e-02, -1.0788e-02, -2.6446e-02, -9.3842e-03,\n","         1.4883e-02,  2.6005e-03, -3.5277e-02, -1.0719e-02,  3.1520e-02,\n","         5.9205e-04, -1.4752e-02,  1.2906e-02,  7.7259e-03,  1.9088e-02,\n","        -8.6026e-03, -1.1294e-03, -1.2804e-02, -3.5588e-02,  2.2912e-02,\n","         5.4993e-03, -2.6627e-02, -1.8863e-02, -3.2767e-02,  1.5669e-02,\n","         1.4470e-02, -2.1004e-02,  1.0201e-02, -2.4873e-02,  1.2721e-02,\n","        -2.0890e-02,  2.9335e-02,  8.4590e-03,  3.1612e-02, -1.6410e-02,\n","        -4.3074e-03,  3.5622e-02,  3.4801e-03, -1.2590e-02,  2.1668e-02,\n","         2.0644e-02, -3.4191e-02, -1.5032e-02,  3.8574e-03,  3.5194e-02,\n","        -1.0082e-02, -6.8383e-03,  2.4914e-02,  3.9131e-03,  2.8826e-02,\n","        -2.3657e-02,  1.7922e-02,  2.3088e-02, -3.1920e-02,  1.7088e-04,\n","        -2.6320e-02,  2.9709e-03,  6.6105e-03,  9.6403e-03, -2.2107e-02,\n","         1.8260e-03, -5.4769e-03, -3.6621e-03, -2.8487e-02, -5.6615e-03,\n","        -2.8985e-02,  1.5701e-02,  7.6726e-03, -2.7654e-02, -3.5044e-02,\n","        -2.1235e-03,  5.2059e-03,  6.7579e-03,  1.6026e-02,  3.0274e-02,\n","         2.3207e-02,  7.5014e-03,  3.3570e-02, -1.8227e-02, -4.9395e-03,\n","        -3.4447e-02, -1.6357e-02,  2.2363e-02, -1.2721e-02,  1.6730e-02,\n","        -3.4551e-02, -6.7659e-03,  1.1813e-02, -2.8829e-02, -2.1908e-03,\n","        -1.0931e-02,  3.4059e-02,  2.8418e-02,  6.1077e-03,  9.0828e-03,\n","        -1.5658e-02,  1.1254e-04,  9.6228e-03,  2.5801e-02,  2.2231e-02,\n","         2.1628e-02,  2.9192e-02,  2.7201e-02, -5.3290e-04, -3.2435e-02,\n","         1.1606e-02,  2.0386e-03,  3.0118e-02,  1.8588e-02, -3.0772e-02,\n","         1.8126e-02, -2.2488e-02, -9.5115e-03, -3.4270e-02,  2.9253e-02,\n","        -1.7760e-02, -3.5102e-02,  4.2821e-03, -1.8724e-02,  2.3523e-02,\n","        -1.1340e-02,  1.7731e-02, -3.8702e-03, -1.3534e-02,  8.8067e-04,\n","        -2.9126e-02,  4.9423e-03, -1.6016e-02, -6.0132e-03,  6.4760e-03,\n","         1.3887e-02, -3.5304e-02,  1.2898e-02,  9.4187e-03,  2.3850e-02,\n","        -3.3215e-02,  8.0435e-03, -5.0447e-03, -3.0127e-02,  2.7067e-02,\n","        -2.0843e-02, -9.8425e-03, -4.1286e-03,  2.3763e-02,  1.2256e-02,\n","         8.4853e-03,  1.9214e-02, -3.5586e-02,  1.6501e-02, -2.3770e-03,\n","         2.6716e-02,  2.9882e-02,  2.4439e-02,  1.7453e-02, -2.4469e-02,\n","         1.3200e-02, -1.3409e-02, -2.0289e-02, -2.0192e-02, -1.8638e-02,\n","        -3.2642e-02, -1.0186e-02,  2.3828e-02,  3.0878e-02,  1.1174e-02,\n","        -1.1620e-02, -1.3368e-02, -4.8857e-03, -6.9446e-03, -2.1758e-02,\n","         2.5447e-02, -3.2655e-04, -3.2778e-02, -1.2407e-02, -2.5746e-03,\n","         1.1249e-02,  8.4687e-03,  2.7508e-02,  7.3380e-03, -2.4991e-02,\n","         4.0053e-04,  2.5850e-02, -1.6093e-02,  1.3535e-02,  2.9198e-02,\n","        -3.0009e-02, -6.0037e-03, -3.4853e-02,  9.3856e-03, -4.8135e-03,\n","         3.5677e-02,  5.6428e-03,  9.1762e-03,  2.9572e-02, -3.3303e-02,\n","        -2.9699e-02,  9.8307e-03, -1.9110e-02,  3.1789e-02,  2.1047e-02,\n","        -2.7718e-02,  1.9382e-02, -4.4371e-03,  3.1035e-03,  1.2912e-03,\n","         2.0250e-02, -1.5161e-03, -1.5698e-02, -3.3978e-02, -1.6710e-02,\n","        -1.1362e-03, -3.1087e-02,  7.2027e-03,  3.5034e-02,  6.0924e-03,\n","         2.1756e-02,  3.4730e-02,  8.4615e-03, -3.2396e-02, -2.5138e-02,\n","        -1.5917e-02, -2.4192e-02, -2.4682e-02, -1.0266e-02, -2.8302e-02,\n","        -4.9269e-03, -2.7959e-02, -3.3961e-02, -6.1202e-03,  3.1271e-02,\n","        -1.8146e-02,  2.5531e-02,  2.4184e-02,  2.6373e-02, -3.4721e-02,\n","        -2.3258e-02,  1.5469e-02,  3.3899e-02,  3.4953e-02, -9.5330e-03,\n","        -7.6447e-03, -1.5982e-02,  1.7682e-02,  1.4647e-03,  2.5155e-02,\n","         2.1435e-02, -2.8598e-02,  3.5195e-02, -3.9483e-03,  9.3768e-03,\n","        -1.9650e-02, -2.9577e-02, -2.7503e-02, -8.2164e-03, -3.2761e-03,\n","        -2.9794e-02,  3.3573e-02, -2.5794e-02, -1.7871e-02, -4.3952e-03,\n","         3.0800e-02, -1.0957e-02,  2.4444e-02,  3.1848e-02,  1.5286e-02,\n","         3.1893e-02, -2.9608e-03,  3.1325e-02, -1.1912e-02,  1.5376e-02,\n","        -8.2521e-03,  2.4302e-03, -4.7494e-03,  1.8111e-02, -2.9871e-02,\n","        -2.7772e-02, -2.1450e-02,  8.2388e-03,  2.4070e-02,  1.5319e-02,\n","         1.1052e-02, -1.6135e-02, -1.3371e-02,  7.5438e-04,  2.0630e-02,\n","         1.6169e-03,  6.3129e-03,  2.3711e-02, -2.3336e-02,  1.3298e-02,\n","         1.6929e-02, -7.6259e-03, -2.5192e-02, -1.3810e-02, -1.3114e-02,\n","         5.7672e-03, -2.0410e-02,  1.3049e-03, -2.0924e-02,  1.9192e-02,\n","         6.4911e-03,  1.1237e-02, -2.6982e-02, -1.4590e-02,  1.5074e-02,\n","         1.7755e-02,  6.8836e-05,  2.8960e-02,  6.5492e-03, -6.4526e-03,\n","        -1.7838e-02, -9.7690e-03,  1.6658e-02, -3.4443e-02, -7.4412e-03,\n","        -3.4939e-02,  5.1932e-03,  3.8833e-03,  1.7178e-02,  1.9874e-02,\n","         2.0285e-02,  1.3708e-02,  1.2074e-02, -1.4510e-02,  3.2547e-02,\n","         9.8103e-04,  3.1207e-02,  3.0911e-03,  2.8954e-02,  3.4061e-02,\n","        -1.5131e-02, -3.3309e-02,  1.0564e-03, -2.7906e-02,  3.1918e-02,\n","        -2.7322e-02, -2.1863e-02, -2.1211e-02, -1.8289e-02, -1.4092e-04,\n","        -3.1349e-02, -6.5347e-03,  1.7494e-02,  3.5585e-02, -2.3558e-02,\n","         1.8044e-02,  1.8246e-03,  1.6488e-02,  2.1206e-03,  2.3520e-02,\n","         3.5143e-02,  7.5667e-03, -3.5086e-02, -1.6132e-02,  2.7012e-02,\n","         1.2543e-02, -3.1106e-02, -3.5359e-02,  3.4191e-03,  2.3654e-02,\n","         2.0159e-02,  7.6107e-03, -1.7745e-02, -1.6661e-02, -2.8765e-02,\n","         9.5250e-03,  1.6091e-02, -1.7923e-02,  2.9256e-02,  2.8095e-02,\n","         5.2800e-04,  2.3636e-02, -2.7322e-02, -6.2480e-03, -4.3347e-03,\n","        -1.0311e-02,  1.7093e-03, -2.2769e-02, -3.5393e-02,  9.9167e-03,\n","         2.2256e-02, -1.9395e-02, -2.5735e-02,  2.6626e-02, -3.3112e-02,\n","         3.0576e-02,  2.6292e-02, -2.8800e-02, -1.6690e-02, -3.2146e-02,\n","        -3.6030e-03,  6.4922e-03,  2.6992e-02,  1.8004e-02, -1.9767e-02,\n","        -3.5530e-03,  2.9089e-04,  2.9437e-04, -7.1626e-03, -3.0182e-02,\n","         1.0185e-02, -1.9814e-02, -1.6088e-02,  2.1539e-02,  3.9423e-03,\n","        -1.7734e-02,  1.9036e-02, -2.1679e-02, -2.1672e-03, -2.7933e-02,\n","        -3.4809e-02,  1.4321e-02,  1.0941e-02, -5.8556e-03,  1.8746e-02,\n","        -2.0126e-02, -1.4028e-02,  2.7493e-02,  2.2200e-02, -2.9914e-02,\n","        -6.4312e-03,  6.7892e-03, -2.6286e-02, -3.4793e-02,  3.5271e-02,\n","        -1.7848e-03, -2.5655e-02,  6.2843e-03,  2.1279e-02,  4.5552e-03,\n","        -4.4332e-03,  2.7816e-02,  6.3435e-03, -3.0658e-02,  2.7486e-02,\n","         6.3648e-04, -2.9292e-03, -2.6239e-02,  1.5762e-02, -2.6464e-02,\n","         2.6633e-02, -3.4193e-02,  1.2745e-02, -2.8574e-02,  3.3291e-02,\n","         2.4922e-03,  1.8956e-02,  7.4373e-03,  3.1659e-02, -4.3523e-03,\n","        -3.0179e-02, -1.3443e-03, -9.5118e-04, -2.4038e-02,  1.9001e-02,\n","         1.7315e-02, -3.2907e-02,  2.1788e-02, -7.6555e-03,  1.8424e-02,\n","         3.5684e-03, -4.4144e-03,  2.2026e-03,  1.3002e-02, -1.7966e-03,\n","        -2.2696e-03,  2.7091e-02,  4.0474e-03,  2.5138e-02,  1.3399e-02,\n","         1.7410e-02,  1.2133e-02, -1.2321e-02, -1.9545e-02, -7.9674e-03,\n","         2.0563e-02, -1.5978e-02,  2.3388e-02, -1.9403e-02, -2.5561e-02,\n","        -2.4557e-02, -2.1437e-02,  2.8891e-02,  2.3200e-02,  3.7684e-03,\n","        -1.5712e-02, -2.7450e-02,  1.0863e-02,  1.0099e-02, -1.3729e-02,\n","         7.1165e-03,  6.6021e-03,  2.1306e-02, -2.4786e-02,  1.7868e-02,\n","         8.7222e-03, -1.1271e-02, -1.8776e-02, -1.3188e-02,  8.9272e-03,\n","         3.6319e-03,  1.8686e-03, -2.0596e-02,  2.4475e-02],\n","       grad_fn=<SelectBackward0>)\n","Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.303108\n","Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.811611\n","Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.515743\n","Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.544495\n","Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.481865\n","Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.554434\n","Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.495694\n","Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.581680\n","Best validation accuracy improved from 0 to 0.9359999895095825, saving model...\n","Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.156714\n","Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.311756\n","Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.222101\n","Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.135980\n","Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.131577\n","Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.371704\n","Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.263791\n","Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.171534\n","Best validation accuracy improved from 0.9359999895095825 to 0.9539999961853027, saving model...\n","Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.056837\n","Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.279637\n","Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.222856\n","Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.099871\n","Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.124014\n","Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.183160\n","Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.247457\n","Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.074260\n","Best validation accuracy improved from 0.9539999961853027 to 0.9614999890327454, saving model...\n","\n","Fold number 5 , Valid Accuracy: 0.9614999890327454\n","\n","\n","Final Accuracy Mean: 0.9552333950996399 , Final Accuracy Std: 0.004498567897826433\n","\n"]},{"data":{"text/plain":["(0.9552334, 0.004498568)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model = MLP(dropout_rate = 0.2)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.00066) \n","criterion = nn.CrossEntropyLoss()\n","\n","kwargs = {'optim': optimizer, 'crit': criterion, \n","          'epochs': 3, 'device': device, 'batch_size': 32, 'model_save': False}\n","\n","cross_validation(model, **kwargs)"]},{"cell_type":"markdown","metadata":{"id":"o4KEtFhgobSR"},"source":["Cross validation is finished and now it is time to test the model on the test dataset"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":1107,"status":"ok","timestamp":1599735287231,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"bE30oneCl2Jp","outputId":"c7b9fa0c-e7f1-486c-9f5c-bf16eae4bea9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set: Loss: 0.0071, Accuracy: 2879/3000 (96.0%)\n","\n"]}],"source":["test(model=model, device=device, test_loader=test_loader)\n"]},{"cell_type":"markdown","metadata":{"id":"DJu6BGRjjJa-"},"source":["Since we are dealing with a multi-class classification (10 classes), let us check"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"MLbuRknip0gf"},"outputs":[],"source":["def confusion_matrix_data(model, device, test_loader):\n","    model.eval()\n","\n","    all_preds = np.array([])\n","    all_labels = np.array([])\n","    with torch.no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            output = model(data)\n","            preds = output.argmax(dim=1, keepdim=True) \n","            # transfer to cpu and reshape preds from [[1],[2],[3]] to [1,2,3]\n","            preds = preds.to('cpu').detach().numpy().reshape(-1)\n","            labels = labels.to('cpu').detach().numpy()\n","\n","            all_preds = np.concatenate((all_preds, preds), axis=0)\n","            all_labels = np.concatenate((all_labels, labels), axis=0)\n","\n","    return all_preds, all_labels"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"elapsed":1129,"status":"ok","timestamp":1599752087482,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"x9eqkWGmcvJa","outputId":"0ca34127-7e2e-46d7-a267-351132f06640"},"outputs":[{"data":{"text/plain":["array([[301,   0,   5,   0,   2,   0,   5,   1,   2,   2],\n","       [  0, 350,   0,   0,   1,   0,   1,   6,   1,   2],\n","       [  1,   1, 298,   1,   4,   0,   0,   3,   2,   0],\n","       [  0,   0,   1, 276,   0,   1,   0,   2,   8,   4],\n","       [  0,   0,   1,   0, 271,   0,   2,   1,   1,   1],\n","       [  1,   0,   0,   3,   0, 270,   3,   0,   1,   3],\n","       [  0,   1,   0,   0,   4,   1, 293,   0,   0,   0],\n","       [  0,   1,   4,   1,   0,   0,   0, 265,   0,   1],\n","       [  3,   3,   1,   3,   3,   2,   1,   0, 259,   7],\n","       [  0,   0,   0,   0,   9,   0,   0,   3,   0, 296]])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import metrics\n","\n","preds, labels = confusion_matrix_data(model=model, device=device, test_loader=test_loader)\n","\n","cm = metrics.confusion_matrix(preds, labels)\n","\n","cm"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":596},"executionInfo":{"elapsed":2567,"status":"ok","timestamp":1599752309772,"user":{"displayName":"denocris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrN8dz6PznkXeNveLINUvYAypb09-x8tbG07aJQg=s64","userId":"14225405973604893991"},"user_tz":-120},"id":"WpclKO10k4Uz","outputId":"b09bd774-2875-4e6b-c17b-51a73446a492"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.96       306\n","         1.0       0.97      0.98      0.98       356\n","         2.0       0.96      0.96      0.96       310\n","         3.0       0.95      0.97      0.96       284\n","         4.0       0.98      0.92      0.95       294\n","         5.0       0.96      0.99      0.97       274\n","         6.0       0.98      0.96      0.97       305\n","         7.0       0.97      0.94      0.96       281\n","         8.0       0.92      0.95      0.93       274\n","         9.0       0.96      0.94      0.95       316\n","\n","    accuracy                           0.96      3000\n","   macro avg       0.96      0.96      0.96      3000\n","weighted avg       0.96      0.96      0.96      3000\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import seaborn as sns #library for plotting\n","\n","## Plot multi-class metrics and confusion matrix \n","classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","print(metrics.classification_report(labels, preds))\n","\n","fig, ax = plt.subplots()\n","sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n","            cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n","       yticklabels=classes, title=\"Confusion matrix\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9JXxvRsqRfw"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN8UH7Q4FuNcTzUOmhZEe2z","collapsed_sections":[],"name":"3_mlp_mnist_training.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
