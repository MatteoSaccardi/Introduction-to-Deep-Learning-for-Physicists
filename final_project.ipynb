{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWGOqJQJWU9m"
      },
      "source": [
        "# Deep Learning Course Final Project: The Lottery Ticket Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHspgluAtYSB"
      },
      "source": [
        "### Cristiano De Nobili - My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "<p align=\"center\">\n",
        "\n",
        "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Instagram_logo_2016.svg/2048px-Instagram_logo_2016.svg.png\" width=\"20\">](https://www.instagram.com/denocris/?hl=it)\n",
        "[<img src=\"https://1.bp.blogspot.com/-Rwqcet_SHbk/T8_acMUmlmI/AAAAAAAAGgw/KD_fx__8Q4w/s1600/Twitter+bird.png\" width=\"30\">](https://twitter.com/denocris) \n",
        "[<img src=\"https://loghi-famosi.com/wp-content/uploads/2020/04/Linkedin-Simbolo.png\" width=\"40\">](https://www.linkedin.com/in/cristiano-de-nobili/)     \n",
        "\n",
        "</p>\n",
        "\n",
        "or here (https://denocris.com).\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "All notebooks can be found [here!](https://drive.google.com/drive/folders/1i3cNfzWZTNXfvkFVVIIDXjRDdSa9L9Dv?usp=sharing)\n",
        "\n",
        "Introductory slides [here!](https://www.canva.com/design/DAEa5hLfuWg/-L2EFFfZLVuiDkmg4KiKkQ/view?utm_content=DAEa5hLfuWg&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink)\n",
        "\n",
        "Collection of references: [here!](https://denocris.notion.site/Deep-Learning-References-0c5af2dc5c8d40baba19f1328d596fff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_RG1kQyV-is"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXJwpbSKWU9s"
      },
      "source": [
        "The aim of this exercise is to explore a somewhat mysterious property of deep neural networks \n",
        "(DNN), i.e. the existence of very small subnetworks S of a given network N that are *trainable* \n",
        "and can reach the same performance of N (or even higher). \n",
        "\n",
        "This intriguing observation has been elevated to the rank of an **hypothesis** in the seminal work by Frankle & Carbin [3]\n",
        "\n",
        "https://arxiv.org/abs/1803.03635\n",
        "\n",
        "*Any large network that trains successfully contains a subnetwork that is initialized such that - when trained in isolation - it can match the accuracy of the original network in at most the same number of training iterations*\n",
        "\n",
        "The authors metaphorically called this subnetwork as a *winning ticket*.\n",
        "\n",
        "In some cases these subnetworks are really small. We can find subnetworks with the  $\\approx 1 \\%$ or less of the original connections.\n",
        "\n",
        "Imagine the possible applications of such tiny DNNs. Being small means less memory and less computation required: this can be crucial in developing embedded systems that, given a target performance to be reached, can afford it with lower resources, including energy consumption.\n",
        "\n",
        "From a theoretical viewpoint there is also another interesting angle. If these subnetworks are so small then, shall we think differently about the problem of *overparametrization*?\n",
        "\n",
        "Let us state it more clearly. A very diffuse concern about DNNs is the fact that the number of parameters is often vastly larger then the number of data points. For example we will work in this notebook with a network with $\\approx 400,000$ *trainable* parameters on the MNIST dataset, that has 60,000 training samples. This means that on average we have 7 parameters for each data point.\n",
        "If we compare this situation with polynomial regression of a dataset of 10 points, for example, a polynomial of degree 10 (that has 11 parameters) overfits the data perfectly. But in DNNs overfitting - while still there - is not as severe as we could expect.\n",
        "\n",
        "But now, if we can reduce by two orders of magnitude the number of parameters in DNNs, does this concern still hold?\n",
        "We don't know the answer, and that's *one* of the reasons why we are here.\n",
        "\n",
        "We are PyTorch beginners, but what we saw in the course is enough to approach this *research* problem in DNNs, from an empirical viewpoint. Hopefully, this will give you stimuli to explore more in depth these problems, adding your perspective to it.\n",
        "\n",
        "Another point of this exercise is to make experience of *new strategies in research and knowledge dissemination*. \n",
        "Short communications - going straight to the point - are very effective indeed as a *first* exposition to a new subject. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzjfi7ctWU9v"
      },
      "source": [
        "### Project Evaluation\n",
        "\n",
        "In order to receive the certification you should work on the following assignments\n",
        "\n",
        "- Reading assignments;\n",
        "- Brief answers to the questions in the notebook;\n",
        "- Completion of the PyTorch code in this notebook;\n",
        "- write the conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMDokSErWU9w"
      },
      "source": [
        "# Reading assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inCpiCq3WU9w"
      },
      "source": [
        "The first part of this exercise is a *reading assignment*.\n",
        "\n",
        "You should read\n",
        "\n",
        "- the research account from the Uber Engineering AI team's blog:\n",
        "https://eng.uber.com/deconstructing-lottery-tickets/\n",
        "\n",
        "\n",
        "We suggest if you want to go deeper in this, to read also the original references:\n",
        "\n",
        "- the original paper of Uber AI\n",
        "\n",
        "https://arxiv.org/abs/1905.01067\n",
        "\n",
        "- and the paper by Frankle and Carbin\n",
        "\n",
        "https://arxiv.org/abs/1803.03635\n",
        "\n",
        "After the readings, we will try to reproduce one of their numerous experiments of the Uber AI paper. Take your time for this and enjoy your reading!\n",
        "\n",
        "\n",
        "![](./figs_nb/manuscript.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWaNgmmCWLG_"
      },
      "source": [
        "# Intro to The Lottery Ticket Hypothesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg57YehkWU9z"
      },
      "source": [
        "A recent work by Frankle & Carbin was thus surprising to many researchers when it presented\n",
        "a simple algorithm for finding sparse subnetworks within larger networks that are trainable.\n",
        "\n",
        "Here the key word is *trainable*: it is possible to find sparse subnetworks that perform well but they are difficult \n",
        "to train directly from scratch.\n",
        "\n",
        "\n",
        "You may want to check the research described in the original paper\n",
        "\n",
        "https://arxiv.org/abs/1803.03635\n",
        "\n",
        "and a video presentation of the original research at ICLR conference 2019\n",
        "\n",
        "https://www.youtube.com/watch?v=s7DqRZVvRiQ&t=3s\n",
        "\n",
        "Briefly, their approach for finding these sparse, performant networks is as follows: \n",
        "\n",
        "- train a network\n",
        "- set all weights smaller than some threshold (in absolute value) to zero\n",
        "- prune them\n",
        "- rewind the rest of the weights to their initial configuration\n",
        "- retrain the network from this starting configuration but with the zero weights frozen (not trained)\n",
        "\n",
        "\n",
        "Using this approach, they obtained two intriguing results\n",
        "\n",
        "- the pruned networks performed well\n",
        "- the network trains well only if it is rewound to its initial state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSNMBKmYWU90"
      },
      "source": [
        "**The Lottery Ticket Algorithm**\n",
        "\n",
        "[...]We begin by briefly describing the lottery ticket algorithm (we simplify things a bit with respect\n",
        "to the paper):\n",
        "\n",
        "- Initialize a mask m to all ones (in the PyTorch code this will be a list of tensors of the same shapes of the ones given by model.parameters()). \n",
        "\n",
        "- Randomly initialize the parameters w of a network\n",
        "$f(x;w \\star m)$ ($\\star$ stands for elementwise multiplication), in this case of course the multiplication by the mask does not have any effect.\n",
        "\n",
        "- Train the parameters w of the network $f(x;w \\star m)$ to completion. Denote the initial weights\n",
        "before training wi and the final weights after training wf\n",
        "\n",
        "- Mask Criterion. Use the mask criterion $M(wi; wf)$ to produce a masking score for each\n",
        "currently unmasked weight. \n",
        "\n",
        "Method to create the mask: \n",
        "\n",
        "\n",
        "<span style=\"color:red\">\n",
        "Rank the weights in each layer by their scores, set the mask\n",
        "value for the top $p\\% $ to 1, the bottom $(100 - p) \\% $ to 0. \n",
        "The mask selected weights with large final value corresponding to $M(wi;wf) = |wf|$.\n",
        "</span>   \n",
        "\n",
        "\n",
        "This text is colored in red because this is what is done in the paper, but we will do it *differently*, as we will explain below!\n",
        "\n",
        "3. Mask-1 Action. Take some action with the weights with mask value 1. In [3] these weights\n",
        "were reset to their initial values and marked for training in the next round.\n",
        "\n",
        "4. Mask-0 Action. Take some action with the weights with mask value 0. In [3] these weights\n",
        "were pruned: set to 0 and frozen during any subsequent training [...]\n",
        "\n",
        "\n",
        "We do not consider iterative pruning (as in the original paper by Frankle and Carbin too): we do it just once.\n",
        "\n",
        "Other masking criteria are possible:\n",
        "\n",
        "\n",
        "![](./figs_nb/mask_criteria.png)\n",
        "\n",
        "we will stick for the moment with the original one, which is the large final (LF) mask.\n",
        "\n",
        "\n",
        "We will do experiments on a small convolutional network trained on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdHzPJEWU90"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Here we start with the necessary imports for the PyTorch version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u-FL2tGVWU91"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import os\n",
        "from os.path import join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfM7NmRUWU93"
      },
      "source": [
        "# Exercise 1: the Network\n",
        "\n",
        "Define the network. It should be composed by\n",
        "\n",
        "* A 1st 2d convolutional layer with 1 input channel, 20 output channels, and a squared window size of 5 (stride 1 is fine);\n",
        "* A ReLu activation function;\n",
        "* A Max Pooling layer of square size 2;\n",
        "* A 2nd 2d convolutional layer of the same window size and with 50 output channels (you must understand the input size by youself);\n",
        "* A ReLu activation function;\n",
        "* A Max Pooling layer of square size 2;\n",
        "* A 1st Linear layers with input size equal to the flattened output size of the last convolution and an output size of 500;\n",
        "* A 2nd and last Linear layer with input size 500 and output size 10 (MNIST classes);\n",
        "* A softmax activation function.\n",
        "\n",
        "We suggest you to look at the documentation of the layers that are involved: convolutional and pooling\n",
        "layers.\n",
        "\n",
        "To check your architecture is correct you should see something like this when printing the model:\n",
        "    \n",
        "    model = Net().to(device)\n",
        "    print(model)\n",
        "\n",
        "    Net(\n",
        "    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
        "    (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
        "    (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
        "    (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
        "    )\n",
        "\n",
        "\n",
        "    and summary() should give something like this \n",
        "\n",
        "    summary(model, (1,28,28))\n",
        "    \n",
        "    ----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "    ================================================================\n",
        "                Conv2d-1           [-1, 20, 24, 24]             520\n",
        "                Conv2d-2             [-1, 50, 8, 8]          25,050\n",
        "                Linear-3                  [-1, 500]         400,500\n",
        "                Linear-4                   [-1, 10]           5,010\n",
        "    ================================================================\n",
        "    Total params: 431,080\n",
        "    Trainable params: 431,080\n",
        "    Non-trainable params: 0\n",
        "    ----------------------------------------------------------------\n",
        "    Input size (MB): 0.00\n",
        "    Forward/backward pass size (MB): 0.12\n",
        "    Params size (MB): 1.64\n",
        "    Estimated Total Size (MB): 1.76\n",
        "    ----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yai2X6gHWU94"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1 , 20, kernel_size = 5, stride = 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size = 5, stride = 1)\n",
        "        self.linear1 = torch.nn.Linear(800, 500)\n",
        "        self.linear2 = torch.nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        pooling = nn.MaxPool2d(2)\n",
        "        relu = nn.ReLU()\n",
        "        flatten = torch.nn.Flatten()\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = relu(out)\n",
        "        out = pooling(out)\n",
        "        out = self.conv2(out)\n",
        "        out = relu(out)\n",
        "        out = pooling(out)\n",
        "        out = self.linear1(flatten(out))\n",
        "        out = self.linear2(out)\n",
        "        return F.log_softmax(out, dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aIkczbrWU94"
      },
      "source": [
        "##### Check your Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5k5FMwV6WU94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (linear1): Linear(in_features=800, out_features=500, bias=True)\n",
            "  (linear2): Linear(in_features=500, out_features=10, bias=True)\n",
            ")\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Conv2d: 1-1                            [-1, 20, 24, 24]          520\n",
            "├─Conv2d: 1-2                            [-1, 50, 8, 8]            25,050\n",
            "├─Linear: 1-3                            [-1, 500]                 400,500\n",
            "├─Linear: 1-4                            [-1, 10]                  5,010\n",
            "==========================================================================================\n",
            "Total params: 431,080\n",
            "Trainable params: 431,080\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 2.29\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.12\n",
            "Params size (MB): 1.64\n",
            "Estimated Total Size (MB): 1.76\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Conv2d: 1-1                            [-1, 20, 24, 24]          520\n",
            "├─Conv2d: 1-2                            [-1, 50, 8, 8]            25,050\n",
            "├─Linear: 1-3                            [-1, 500]                 400,500\n",
            "├─Linear: 1-4                            [-1, 10]                  5,010\n",
            "==========================================================================================\n",
            "Total params: 431,080\n",
            "Trainable params: 431,080\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 2.29\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.12\n",
            "Params size (MB): 1.64\n",
            "Estimated Total Size (MB): 1.76\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# instantiate the model\n",
        "model = Net()\n",
        "# put the model on the GPU       \n",
        "model.to(device)\n",
        "    \n",
        "print(model)\n",
        "print(summary(model, (1,28,28)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdcFfCIeWU95"
      },
      "source": [
        "# Exercise 2: Training and test functions\n",
        "\n",
        "Before implementing the LT algorithm there are two changes we have to make to the *standard* \n",
        "training functions\n",
        "\n",
        "- add an optional argument *mask*\n",
        "- write the code that, if a mask is passed, modifies the update in order to freeze the parameters whose mask value is zero\n",
        "\n",
        "Notice that in PyTorch we can set the flag require_grad to *tensors*, but not to their individual elements. Let us recall this once.\n",
        "\n",
        "For tensors we could proceed as follows (we did something like this in the transfer learning, when we froze all the network but the last hidden layer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cb5loZE6WU95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1, 5, 5]) True\n",
            "torch.Size([20]) True\n",
            "torch.Size([50, 20, 5, 5]) True\n",
            "torch.Size([50]) True\n",
            "torch.Size([500, 800]) True\n",
            "torch.Size([500]) True\n",
            "torch.Size([10, 500]) True\n",
            "torch.Size([10]) True\n"
          ]
        }
      ],
      "source": [
        "# show the requires_grad flags\n",
        "for p in model.parameters():\n",
        "    print(p.shape, p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YhEs8C45WU96"
      },
      "outputs": [],
      "source": [
        "# freeze first hidden layer as an example\n",
        "list(model.parameters())[0].requires_grad=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OvIX1kZnWU96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# check that the flag has changed\n",
        "for p in model.parameters():\n",
        "    print(p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tepf6RCNWU97"
      },
      "source": [
        "Let us re-set to True the flag we changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XxpKu-2ZWU97"
      },
      "outputs": [],
      "source": [
        "list(model.parameters())[0].requires_grad=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak8hIm-4WU98"
      },
      "source": [
        "But in order to *freeze* individual elements of our parameter tensors we have to proceed differently:\n",
        "\n",
        "- compute the gradient with respect to all the parameters \n",
        "- set to zero the gradients of the parameters whose mask value is zero\n",
        "- do the normal update\n",
        "\n",
        "\n",
        "Complete the code in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qWZ1Zp1HWU98"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, mask=None):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        \n",
        "        #----------------------------------------------------------------------------------------------\n",
        "        # compute the gradient with respect to all the parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # set to zero the gradients of the freezed parameters if a mask is passed as a parameter        \n",
        "        if mask is not None:\n",
        "            try:\n",
        "                for i,p in enumerate(model.parameters()):\n",
        "                    p.data = p.data * mask[i]\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(\"Exception {} when applying the mask!\".format(e))\n",
        "\n",
        "        \n",
        "        # parameters update\n",
        "        optimizer.step()\n",
        "        #----------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr7e5RBDWU98"
      },
      "source": [
        "Here the test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JOqnuG28WU99"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader,verbose=False):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    if verbose:\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            loss, correct, len(test_loader.dataset), acc))\n",
        "\n",
        "    return loss,acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEDCcMcHWU99"
      },
      "source": [
        "Now we are ready to do the experiment with the LF mask.\n",
        "Before doing that we have to \n",
        "\n",
        "- set our hyperparameters\n",
        "- deal with the data\n",
        "\n",
        "Just evaluate the following two cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4NclbIWU99"
      },
      "source": [
        "# Hyperparameters settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sX-r_-BKWU99"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "\n",
        "#number of epochs \n",
        "epochs=5\n",
        "\n",
        "#learning rate\n",
        "lr=0.01\n",
        "\n",
        "# keep the momentum to 0, otherwise also freezed parameters \n",
        "# will move for the momentum contribution to parameters evolution\n",
        "momentum=0.0\n",
        "\n",
        "seed=1\n",
        "torch.manual_seed(seed)\n",
        "save_model=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J77sH_l1WU9-"
      },
      "source": [
        "# Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IE6tRuwtWU9-"
      },
      "outputs": [],
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYKRQ5dfWU9-"
      },
      "source": [
        "# Exercise 3: First Standard Training (with no mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6vSkCA1Wrbq"
      },
      "source": [
        "During the first standard training you will\n",
        "\n",
        "* initialize the network and send to device;\n",
        "* store the initial weights $w_i$ of the network;\n",
        "* initialize the optimizer;\n",
        "* train the network without any mask (or passing a mask of ones if you prefer);\n",
        "* store the final weights $w_f$;\n",
        "* The final weights will be used later to compute the Large Final (LF) mask.\n",
        "\n",
        "During the training check that your loss is getting smaller and your accuracy on test set higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7zr-pAA2WU9-"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "if not os.path.isdir('./models'):\n",
        "    os.mkdir('./models')\n",
        "\n",
        "torch.save(model.state_dict(), 'models/initial.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mI_E8heOWU9_"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2AsilbECWU9_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, mask=None)\n",
        "    \n",
        "if (save_model):\n",
        "    print('Saving model.')\n",
        "    torch.save(model.state_dict(), './models/final.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6gSt6hZWU-A"
      },
      "source": [
        "# Masks Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1imKH4MAWx_g"
      },
      "source": [
        "We define different masks\n",
        "\n",
        "- the identity mask, which is useful for debugging: it will leave the training unaltered;\n",
        "- the random mask, that we will use to make comparisons;\n",
        "- the Large Final mask (LF mask).\n",
        "\n",
        "The first two are given, the implementation of the LF mask is an exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DawoAJupWU-A"
      },
      "outputs": [],
      "source": [
        "def identity_mask(model):\n",
        "    '''\n",
        "    Returns the identity mask for all parameter tensors in a list\n",
        "    '''\n",
        "    mask = []\n",
        "    for p in model.parameters():\n",
        "        mask.append(torch.ones_like(p))\n",
        "    return mask\n",
        "\n",
        "def random_mask(model, level=0.0):\n",
        "    '''\n",
        "    Construct random mask with a given level of pruning (probability to assign a zero value)\n",
        "    '''\n",
        "    # construct random mask\n",
        "    mask = []\n",
        "    frac = 0\n",
        "    tot = 0\n",
        "    for i,p in enumerate(model.parameters()):\n",
        "        mask.append( (torch.rand_like(p) > level).float().to(device) )\n",
        "        frac += torch.sum( mask[i] ).item() \n",
        "        tot += mask[i].numel()\n",
        "    frac = frac/tot\n",
        "    return mask, frac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpXKxaKKW3HD"
      },
      "source": [
        "# Exercise 4: build the LF Mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Xla3-2WU-A"
      },
      "source": [
        "You will implement here a *variant* of the original LF (which is a bit more sophisticated). \n",
        "\n",
        "The general idea is just to mark with 1 the weights that at the end of the training are larger then a certain threshold. \n",
        "\n",
        "![LF-masks.png](https://drive.google.com/uc?export=view&id=1Xc50-81zo5YPmn3EBu8eGMDnKlFvbAtv)\n",
        "\n",
        "\n",
        "Concretely you will do it as follows. Let us denote with\n",
        "\n",
        "- p a vector that contains all the weights;\n",
        "- |p| its elementwise absolute value;\n",
        "- m the average of |p|;\n",
        "- s the standard deviation of p (notice that now we are considering the original parameters, not their absolute values);\n",
        "- $\\alpha$ is a parameter in the range $[0,2]$;\n",
        "\n",
        "Then a parameter $p_i$ will be considered relevant (mask = 1 for this parameter) if its final value is such that:\n",
        "\n",
        "$$\n",
        "|p_i| > m + \\alpha \\times s\n",
        "$$\n",
        "\n",
        "Your function (LF_mask) will have the following signature:\n",
        "    \n",
        "    input : model,alpha\n",
        "        \n",
        "    output : mask (a list of tensors with the same shape of list(model.parameters()) ) in which to \n",
        "             each parameter is associated a value 1 if relevant, 0 if irrelevant\n",
        "             \n",
        "             frac (a the fraction of parameters whose mask is equal to 1)\n",
        "             \n",
        "             The latter is useful to keep track of the level of parameters'pruning which is \n",
        "             implicitly set by alpha\n",
        "             \n",
        "Then we will test it and see how many parameters we pruned with a certain $\\alpha$.\n",
        "             \n",
        "Hint: Basically you will have to concatenate all the parameters in a long numpy array first, in order to compute the statistics you need. Maybe you will find useful the functions torch.where, torch.ones_like, torch.zeros_like. Furthermore, before acting on torch tensors you will need to detach them from the computational graph (see detach() method), send them to the cpu device, and convert them into numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XUPYwAWmWU-A"
      },
      "outputs": [],
      "source": [
        "def LF_mask(model,alpha):\n",
        "    '''\n",
        "    Construct large final (LF) mask. The threshold for \n",
        "    decision is determined globally.\n",
        "    '''\n",
        "    frac = 0\n",
        "    tot = 0\n",
        "    \n",
        "    \n",
        "    # concatenate all parameters into a numpy array\n",
        "    params = np.array([])\n",
        "    for i, p in enumerate( model.parameters() ):\n",
        "        value = p.to('cpu').detach().numpy().reshape(-1)\n",
        "        params = np.concatenate((params, value), axis=0)\n",
        "          \n",
        "    # compute mean of the absolute values and standard deviation\n",
        "    mean = np.mean(params)\n",
        "    std = np.std(params) \n",
        "        \n",
        "    # compute the mask\n",
        "    mask = []\n",
        "    for i, p in enumerate( model.parameters() ):\n",
        "        condition = np.abs(p) > mean + alpha*std\n",
        "        temp_mask = torch.where(condition,1,0)\n",
        "        \n",
        "        mask.append(temp_mask)\n",
        "        \n",
        "        frac += torch.sum( mask[i] ).item()\n",
        "        tot += mask[i].numel()\n",
        "\n",
        "    frac = frac/tot\n",
        "    \n",
        "    return mask, frac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stlmL57YWU-B"
      },
      "source": [
        "##### Test the LF mask\n",
        "\n",
        "To make a test let us create the mask with the LF function, then print the fraction of weights that received \n",
        "a mask equal to 1.\n",
        "Notice that with $\\alpha=1$ you are already pruning a substantial part of your network ($\\approx 98 \\%$ of the parameters).\n",
        "\n",
        "This means that the LF mask defines an highly *sparse* subnetwork."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YJo_CDmtWU-B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fraction of weights with mask=1:  0.384\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('models/final.pt') )\n",
        "alpha = 1\n",
        "with torch.no_grad():\n",
        "    mask, frac = LF_mask(model,alpha)\n",
        "print('Fraction of weights with mask=1:  {}'.format(np.round(frac,3)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JOiUWv1KKYZ"
      },
      "source": [
        "We can inspect LF mask values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ODyjpvCbKOO-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[1, 1, 1, 0, 1],\n",
            "          [1, 0, 1, 1, 0],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [0, 1, 0, 0, 1],\n",
            "          [1, 0, 1, 1, 1],\n",
            "          [1, 0, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [0, 1, 1, 1, 1],\n",
            "          [1, 0, 1, 0, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 0, 0, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [0, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 0, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0, 1, 1],\n",
            "          [1, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [0, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 0, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0, 1, 0],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [0, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [0, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 0, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 0, 1],\n",
            "          [0, 1, 0, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 0],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1],\n",
            "          [1, 0, 1, 1, 1],\n",
            "          [1, 1, 1, 1, 1]]]])\n"
          ]
        }
      ],
      "source": [
        "for m, p in zip(mask, model.parameters()):\n",
        "  print(m)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4VtDPJneU1y"
      },
      "source": [
        "# Exercise 5: Compare the different trainings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbAy1kLpWU-B"
      },
      "source": [
        "### Rewind to init state and train again: with LF mask\n",
        "\n",
        "Now we will verify that this highly sparse subnetwork is also *trainable* when if start from the original weights.\n",
        "    \n",
        "We will\n",
        "\n",
        "- rewind the network to its initial state;\n",
        "- apply the LF mask *before* training: in this way we will obtain our sparse subnetwork;\n",
        "- retrain just the subnetwork (it is enough to pass the mask to the train function: convince yourself that this is indeed the case);\n",
        "- evaluate it at the end.\n",
        "\n",
        "Verify that the subnetwork is trainable if we start from the original weights (how much accuracy is \n",
        "reached on the test set?)\n",
        "\n",
        "We do the initial step and you will complete the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OGoTmIXdWU-C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [02:34<00:00, 30.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the subnetwork after the training: 98.51\n"
          ]
        }
      ],
      "source": [
        "# rewind to initial state\n",
        "model.load_state_dict(torch.load('models/initial.pt') )\n",
        "\n",
        "# apply mask\n",
        "for i,p in enumerate(model.parameters()):\n",
        "    p.data = p.data * mask[i]\n",
        "    \n",
        "# re-instantiate optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "# train\n",
        "for epoch in tqdm(range(1, epochs + 1)):\n",
        "    train(model, device, train_loader, optimizer, epoch, mask=None)\n",
        "    \n",
        "# evaluate at the end of training\n",
        "_, acc_trained_subnetwork = test(model, device, test_loader)\n",
        "\n",
        "print('Accuracy of the subnetwork after the training: {}'.format(acc_trained_subnetwork) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxub591Weego"
      },
      "source": [
        "### Start from a new random init and train again: with LF mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB-znxmveJc3"
      },
      "source": [
        "What happens if we start from a new initialization? Verify also that the subnetwork is\n",
        "not as good as before in this case (how much accuracy do you reach now? Notice that in this context if a subnetwork reaches *only* the $\\approx 90 \\%$ - that one might think is not a bad result after all - of performance we consider it as *not trainable*, meaning only that the training is not completely effective)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CHyuEflaWU-C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [02:32<00:00, 30.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the subnetwork after the training: 97.85\n"
          ]
        }
      ],
      "source": [
        "# rewind to a random state\n",
        "model = Net().to(device)\n",
        "\n",
        "# apply mask\n",
        "for i,p in enumerate(model.parameters()):\n",
        "    p.data = p.data * mask[i]\n",
        "    \n",
        "# re-instantiate optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "# train\n",
        "for epoch in tqdm(range(1, epochs + 1)):\n",
        "    train(model, device, train_loader, optimizer, epoch, mask=None)\n",
        "    \n",
        "# evaluate at the end of training\n",
        "_, acc_trained_subnetwork = test(model, device, test_loader)\n",
        "\n",
        "print('Accuracy of the subnetwork after the training: {}'.format(acc_trained_subnetwork) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGV2pQ5y_rEN"
      },
      "source": [
        "### LF on randomly initilized net without training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4en-N6Acbd"
      },
      "source": [
        "**Question:** What happens if we do not train the network at all, but simply applying the (learned) mask to the randomly initialized network?\n",
        "\n",
        "It turns out that with a well-chosen mask, an untrained network can already attain a test accuracy far better than chance. This might come as a surprise, because if you use a randomly initialized and untrained network to, say, classify images of handwritten digits from the MNIST dataset, you would expect accuracy to be no better than chance (about $10 \\%$). But now imagine you multiply the network weights by a mask containing only zeros and ones. In this instance, weights are either unchanged or deleted entirely, but the resulting network now achieves nearly 40 percent accuracy at the task! This is strange, but it is exactly what we observe with masks created using the large final criterion.\n",
        "\n",
        "With the randomly initialized network the application of the LF mask sets to zero all the weights that would have vanished whith the training, leaving non-zero only the weights that would have reached a given threshold after training (from the paper: \"the benefit derived from freezing values to zero comes from the fact that those values were moving towards zero anyway\"). So, the LF mask acts as a training by allowing, from the very first initial state, to have a lower cost function than the one of the dense neural network.\n",
        "\n",
        "We will perform these steps:\n",
        "\n",
        "* rewind our network to its initial state\n",
        "* evaluate it once\n",
        "* apply the LF mask\n",
        "* repeat the evaluation on the subnetwork\n",
        "\n",
        "Consider that **we are not re-training** our network but just applying the LF mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQedzOiTk62_"
      },
      "source": [
        "**Rewind to the initial network with random weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQW-LNvI9fm4",
        "outputId": "74e194dd-b595-4c9a-dd05-5b5f6897c663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the randomly initialized network, without mask: 10.78\n"
          ]
        }
      ],
      "source": [
        "# rewind to initial state\n",
        "model.load_state_dict(torch.load('models/initial.pt') )\n",
        "\n",
        "#evaluate the network:\n",
        "_, acc_init = test(model, device, test_loader)\n",
        "    \n",
        "print('Accuracy of the randomly initialized network, without mask: {}'.format(acc_init) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTSTES5flLNG"
      },
      "source": [
        "**Network with random weights + random mask**\n",
        "\n",
        "Apply a random mask to the random initialized net. You should obtain around $10\\%$ of accuracy (the same performance of a random classifier if the number of classes are 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ErmRLzC2lEsw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the randomly initialized network, with a random mask: 17.02\n"
          ]
        }
      ],
      "source": [
        "# compute random mask\n",
        "\n",
        "pruning_fraction = 1 - frac\n",
        "\n",
        "rmask, rfrac = random_mask(model, pruning_fraction)\n",
        "\n",
        "# apply random mask\n",
        "for i,p in enumerate(model.parameters()):\n",
        "    p.data = p.data * rmask[i]\n",
        "    \n",
        "#re-evaluate the network:\n",
        "_, acc_rand_subnetwork = test(model, device, test_loader)\n",
        "\n",
        "print('Accuracy of the randomly initialized network, with a random mask: {}'.format(acc_rand_subnetwork) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqXaCHbClX_O"
      },
      "source": [
        "**Network with random weights + LF mask**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "li69PArHlZI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the randomly initialized subnetwork WITHOUT a LF mask: 10.78\n",
            "Accuracy of the randomly initialized subnetwork with a LF mask: 78.41\n"
          ]
        }
      ],
      "source": [
        "# rewind to random\n",
        "model.load_state_dict(torch.load('models/initial.pt') )\n",
        "\n",
        "#evaluate the network:\n",
        "_, acc_subnetwork = test(model, device, test_loader)\n",
        "\n",
        "print('Accuracy of the randomly initialized subnetwork WITHOUT a LF mask: {}'.format(acc_subnetwork) )\n",
        "\n",
        "# apply LF mask\n",
        "# by setting to zero the weights of the frozen parameters \n",
        "for i,p in enumerate(model.parameters()):\n",
        "    p.data = p.data * mask[i]\n",
        "    \n",
        "#re-evaluate the network:\n",
        "_, acc_subnetwork = test(model, device, test_loader)\n",
        "\n",
        "print('Accuracy of the randomly initialized subnetwork with a LF mask: {}'.format(acc_subnetwork) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWbBKD49ktLr"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I36RJgNPnnzf"
      },
      "source": [
        "Write down conclusions about the exercise as if it were a scientific paper. In particular\n",
        "\n",
        "* Summarise findings;\n",
        "* Rise possible questions;\n",
        "* highlight future developments or next steps.\n",
        "\n",
        "\n",
        "I hope that this exercise was a stimulus for you to delve more into this subject, and more \n",
        "generally into deep learning problems.\n",
        "\n",
        "Good Luck!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ofXcQ2dfWU-C",
        "f1Oeon3-WU-E",
        "YRxLQyGcWU-F",
        "Iyemvk5BWU-F",
        "q6-eLKMpWU-G"
      ],
      "name": "final-project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
