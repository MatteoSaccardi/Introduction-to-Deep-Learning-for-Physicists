{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fz1FL9F5D1t"
      },
      "source": [
        "<h1><center> Denoising Variational Autoencoder </center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qy4qOigfQd_"
      },
      "source": [
        "  <center>  <img src=https://drive.google.com/uc?id=1A6QMi7EGwnyeee8DsD7UJ0V_ssWyCNNE \" width=\"700\">  </center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_fSAUpAZj9E"
      },
      "source": [
        "### Cristiano De Nobili - My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "<p align=\"center\">\n",
        "\n",
        "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Instagram_logo_2016.svg/2048px-Instagram_logo_2016.svg.png\" width=\"20\">](https://www.instagram.com/denocris/?hl=it)\n",
        "[<img src=\"https://1.bp.blogspot.com/-Rwqcet_SHbk/T8_acMUmlmI/AAAAAAAAGgw/KD_fx__8Q4w/s1600/Twitter+bird.png\" width=\"30\">](https://twitter.com/denocris) \n",
        "[<img src=\"https://loghi-famosi.com/wp-content/uploads/2020/04/Linkedin-Simbolo.png\" width=\"40\">](https://www.linkedin.com/in/cristiano-de-nobili/)     \n",
        "\n",
        "</p>\n",
        "\n",
        "or here (https://denocris.com).\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "All notebooks can be found [here!](https://drive.google.com/drive/folders/1i3cNfzWZTNXfvkFVVIIDXjRDdSa9L9Dv?usp=sharing)\n",
        "\n",
        "Introductory slides [here!](https://www.canva.com/design/DAEa5hLfuWg/-L2EFFfZLVuiDkmg4KiKkQ/view?utm_content=DAEa5hLfuWg&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink)\n",
        "\n",
        "Collection of references: [here!](https://denocris.notion.site/Deep-Learning-References-0c5af2dc5c8d40baba19f1328d596fff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p19lrvbvvYs-"
      },
      "source": [
        "### Some Refs\n",
        "\n",
        "* [What is a Variational Autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
        "\n",
        "* [Variational Autoencoder by Jonathan Hui](https://jhui.github.io/2017/03/06/Variational-autoencoders/)\n",
        "\n",
        "* [Variational autoencoders by Jeremy Jordan](https://www.jeremyjordan.me/variational-autoencoders/)\n",
        "\n",
        "* [Understanding Variational Autoencoders (VAEs) by Joseph Rocca](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73#:~:text=In%20variational%20autoencoders%2C%20the%20loss,makes%20the%20latent%20space%20regular)\n",
        "\n",
        "* Original Papers: [Kingma et al.](https://arxiv.org/abs/1312.6114) and [Rezende et al.](https://arxiv.org/abs/1401.4082)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iuCO5ftpX6-w"
      },
      "outputs": [],
      "source": [
        "import math as m\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSLc0ud-wb5t"
      },
      "source": [
        "## <h1><center>  Information Theory Background \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrqdhkMID39"
      },
      "source": [
        "### Self-Information\n",
        "\n",
        "The idea behind self-information is the following\n",
        "\n",
        "* if an event always occurs, we associate it with a smaller amount of information. It will not suprise us!\n",
        "* On the other side, a rare event is associated with a huge amount of information. It will suprise us!\n",
        "\n",
        "I am not surprise to see the sunrise every morning (likely event). Instead,  I would be really suprised if tomorrow the Sun will not rise (unlikely event). This amount of surprise or self-information of the event $x$ is quantified by\n",
        "\n",
        "$$I(x) = - \\log p(x),$$\n",
        "\n",
        "where $p(x)$ is the probability of the event $x$. If $p(x)=1$, then self-info is zero. A rare event instead has a huge surpise factor.\n",
        "\n",
        "### Shannon Entropy \n",
        "\n",
        "In terms of self-info, Shannon Entropy is the average self-information (expected value) over all possible values of X.\n",
        "The entropy for a probability $p(x)$ distribution is\n",
        "\n",
        "$$ S = - \\sum_i p(x_i) \\log p(x_i),$$\n",
        "\n",
        "where we assume we know the probability $p$ for each outcome $i$. If we use $log_2$  for our calculation we can interpret entropy as *the minimum number of bits it would take us to encode our information*.\n",
        "\n",
        "For continous variables, we can use the integral form\n",
        "\n",
        "$$ S = - \\int  p(x) \\log p(x) \\, dx,$$\n",
        "\n",
        "where now $p(x)$ is taking the role of a probability density function (PDF). Take in mind that a broad probability density has higher entropy than a narrowed one (think about Gaussian distribution vs delta Dirac, which has $S=0$).\n",
        "\n",
        "In both discrete and continous formulation, we are computing the expectation (i.e. average) of the negative log-probability (i.e. self-info) which is the theoretical minimum encoding size of the information from the event $x$. The same formula is usually written as\n",
        "\n",
        "$$S = \\mathbb E _{\\, x \\sim p} \\left[ -\\log p(x) \\right],$$\n",
        "\n",
        "where $x \\sim p$ means that we calculate the expectation with the probability distribution $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEow_oxYSje"
      },
      "source": [
        "Let's give an example! \n",
        "\n",
        "<!---\n",
        "  REMIND to change open with uc\n",
        " https://drive.google.com/open?id=1Y52T3Z4dwRU4Rq5L5bEVYh0d3kU0aVB8\n",
        "--->\n",
        "\n",
        "  <center>  <img src=https://drive.google.com/uc?id=1GaAeK8xIZCVDRb-oHQNUzRuoOprFh1eS \" width=\"700\">  </center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5E4znWyYWDT"
      },
      "source": [
        "Let's say we have to pass a message about what drink Cristiano would take during an event. In general, Cristiano loves [Midori Sour](https://drizly.com/midori-sour/r-b972d5282bec6fe8) , Daiquiri, Spritz and Wine.\n",
        "\n",
        "On Monday, Cristiano loves to listen Jazz and the probability distribution of his choice is: \n",
        "\n",
        "$$P(\\text Midori ) =  P(\\text Daiquiri ) = P(\\text Spritz ) = P(\\text Wine ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} = 2$$\n",
        "\n",
        "On Wednesday, he usually meets with some friends after work: \n",
        "\n",
        "$$P(\\text Midori ) = 0.125,\\;  P(\\text Daiquiri ) =0.125,\\;  P(\\text Spritz ) = 0.5,\\; P(\\text  Wine ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} = 1.75$$\n",
        "\n",
        "\n",
        "On Thursday, he often goes to an event where cocktail attire dress code is required\n",
        "\n",
        "$$P(\\text Midori ) = 0.95,\\;  P(\\text Daiquiri ) =0.02,\\;  P(\\text Spritz ) = 0.018,\\; P(\\text  Wine ) = 0.012,$$\n",
        "\n",
        "and the corresponding entropy\n",
        "\n",
        "$$S = - 0.95 \\log 0.95 - 0.02 \\log 0.02 - 0.018 \\log 0.018 - 0.012 \\log 0.012 = 0.364$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KReLhZDvICt1",
        "outputId": "16c26899-e20d-46e7-96bc-9b5635fcbe64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On Monday, high entropy:  2.0\n",
            "On Wednesday, medium entropy:  1.75\n",
            "On Thursday, low entropy:  0.36407300467232967\n"
          ]
        }
      ],
      "source": [
        "# On Monday, all drinks have equal probability to be chose\n",
        "entropy_1 = -0.25*m.log(0.25,2)-0.25*m.log(0.25,2)-0.25*m.log(0.25,2)-0.25*m.log(0.25,2)\n",
        "print('On Monday, high entropy: ', entropy_1)\n",
        "\n",
        "# On Wednesday, some are more probable than others\n",
        "entropy_2 = -0.5*m.log(0.5,2)-0.25*m.log(0.25,2)-0.125*m.log(0.125,2)-0.125*m.log(0.125,2)\n",
        "print('On Wednesday, medium entropy: ', entropy_2)\n",
        "\n",
        "# On Thursday, one drink is by far the most probable\n",
        "entropy_3 = -0.95*m.log(0.95,2)-0.02*m.log(0.02,2)-0.018*m.log(0.018,2)-0.012*m.log(0.012,2)\n",
        "print('On Thursday, low entropy: ', entropy_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZh-58l2Qwvu"
      },
      "source": [
        "If entropy is high (encoding size $log_2 p(x)$ is big on average), it means we have many message types with small and almost equal probabilities. Hence, every time a new message arrives, you would expect a different type than previous messages. You may see it as a disorder or uncertainty or unpredictability.\n",
        "\n",
        "On the contrary, when a message has much smaller probability than other message, it appears as a surprise because on average you would expect other more frequently sent message types. Moreover, a rare message type has more information than more frequent message types because it eliminates a lot of other probabilities and tells us more specific information.\n",
        "\n",
        "In the drink scenario, by sending “Wine” on thursday which happens 1.2% of the times, we are reducing the uncertainty by 98.8% of the probability distribution (“Midori, Daiquiri, Spritz”) provided we had no information before. If we were sending “Midori” (95%) instead, we would be reducing the uncertainty by 5% only.\n",
        "\n",
        "If the entropy is high (ex: fair coin), the average encoding size is significant which means each message tends to have more (specific) information. Again, this is why high entropy is associated with disorder, uncertainty, surprise, unpredictability, amount of information. The more random a message is, the more information will be gained from decoding the message.\n",
        "\n",
        "Low entropy (ex: sunrise) means that most of the times we are receiving the more predictable information which means less disorder, less uncertainty, less surprise, more predictability and less (specific) information. This is the Thursday case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu6ZmWp5UGDa"
      },
      "source": [
        "### Cross Entropy\n",
        "\n",
        "Suppose to have two distributions, the true one $p(x)$ and the estimated $q(x)$. In the language of neural networks, $p(x)$ would be the grond truth (labels in one hot-encoding) and $q(x)$ the outcome of the net, i.e. the one that your machine learning algorithm is trying to match. Cross entropy is a mathematical tool for comparing two probability distributions $p(x)$ and $q(x)$ and it is expressed by the formula \n",
        "\n",
        "$$ H (p,q) = - \\int p(x) \\log q(x)\\,dx.$$\n",
        "\n",
        "If $\\log$ is in base $2$, then cross entropy measures the number of bits you will need encoding symbols from $p$ using the wrong distribution $q$. Subtracting to cross entropy the entropy of $p$, you are counting the cost in terms of bits of using the wrong distribution $q$ (this somehow will be KL-divergence). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60tieif3uc-l"
      },
      "source": [
        "### Kullback-Leibler Divergence\n",
        "\n",
        "KL-divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution $h$ we add into the game our approximating distribution $g$. Then we look at the difference of the log values for each\n",
        "\n",
        "$$D_{KL}(h || g) = - \\sum_i h(x_i) (\\log h(x) - \\log g(x)) = - \\sum_i h(x_i) \\log \\frac{h(x)}{g(x)}$$ \n",
        "\n",
        "KL-divergence is the expectation of the log-difference between the probability of data in the original distribution $h$ with the approximating distribution $g$. Again, if we think in terms of $\\log_2$ we can interpret this as how many bits of information we expect to lose when we choose an approximation $g$ of our original ditribution $h$. \n",
        "\n",
        "In the variational autoencoder loss function, the KL-divergence is used to force the distribution of latent variables $q(z | x)$ to be a normal distribution $n(z)$ so that we can sample latent variables from the normal distribution. As such, the KL-divergence is included in the loss function to improve the similarity between the distribution of latent variables and the normal distribution. More about **KL** can be found [here](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) and about **cross-entropy** [here](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9unYqYhn3bQ"
      },
      "source": [
        "##  Autoencoder vs Variational Autoencoder\n",
        "\n",
        "<center>  \n",
        "<p>      \n",
        "<img src=https://drive.google.com/uc?id=1QdiSESYkbNnigrbH-GutlGtRuC6QbGpw  width=\"800\" height=\"250\">  \n",
        "<img src=https://drive.google.com/uc?id=1IiuzC8YeZ_MXJsYg1BgJV-o3VMoQUGvI width=\"800\" height=\"250\">    \n",
        " <p>\n",
        "</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Generative\" since we sample a distribution and generate an image which has possibly never been generated before.\n",
        "\n",
        "Why Autoencoders?\n",
        "- Map high-dimensional data to low-dimensional\n",
        "- Learn abstract features in an **unsupervised** way so you can apply them to a supervised task; unsupervised (or self unsupervised) algorithms (the label is the image itself!!)\n",
        "- Learn a semantically meaningful representation where you can.\n",
        "\n",
        "Standard Autoencoders are NOT generative models (they do not have any distribution inside), so they don't define a distribution.\n",
        "\n",
        "We can train our variational autoencoder to be an identity, but in the end it's slightly different since it randomly samples from the distributions of the vector of latent attributes, features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTdh_cy0vCgI"
      },
      "source": [
        "## <h1><center> Coding our Denoising VAE\n",
        "\n",
        "### Importing Data and Add some Gaussian Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwaF2dzYyGWX",
        "outputId": "ef52b732-b4e5-4b7c-85c6-da49fa99a17c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc573f9f5b0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set the seed: built-in python, numpy, and pytorch\n",
        "seed = 172\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed) # works for all devices (CPU and GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jucoAyAvyAbD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#clean images\n",
        "train_images = MNIST('./data', train=True, download=True).data.numpy()\n",
        "test_images = MNIST('./data', train=False, download=True).data.numpy()\n",
        "\n",
        "\n",
        "##########################\n",
        "# image pixels in [0,1]\n",
        "#########################\n",
        "\n",
        "# clean images normalization\n",
        "train_images_norm = train_images / 255.0\n",
        "test_images_norm = test_images / 255.0\n",
        "\n",
        "\n",
        "##########################\n",
        "# compute mean and std (you can normalize also wrt to mean and std)\n",
        "#########################\n",
        "\n",
        "print('Clean images values: ')\n",
        "print('Train mean and std: %f  %f' %(train_images_norm.mean(), train_images_norm.std()))\n",
        "print('Test mean and std: %f  %f' %(test_images_norm.mean(), test_images_norm.std()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "R5eUWp3puiQy",
        "outputId": "392ee86d-060a-4933-d7aa-31f4b66cacce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = train_images[0]\n",
        "image = np.array(image, dtype='float')\n",
        "pixels = image.reshape((28, 28))\n",
        "plt.imshow(pixels, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6VVekOGyuvC8",
        "outputId": "068434b8-9d4e-43a9-8351-80a5727f5a86"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnoisy_image = train_images_noisy[0]\\nnoisy_image = np.array(noisy_image, dtype='float')\\npixels = noisy_image.reshape((28, 28))\\nplt.imshow(pixels, cmap='gray')\\nplt.show()\\n\""
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "noisy_image = train_images_noisy[0]\n",
        "noisy_image = np.array(noisy_image, dtype='float')\n",
        "pixels = noisy_image.reshape((28, 28))\n",
        "plt.imshow(pixels, cmap='gray')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUiwN-nKvTgu"
      },
      "source": [
        "### Transforming Data in Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kgrQcee5VxdC"
      },
      "outputs": [],
      "source": [
        "# Build Clean Dataset\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "train_images_norm = torch.tensor(train_images_norm).view(60000, 1, 28, 28)\n",
        "test_images_norm = torch.tensor(test_images_norm).view(10000, 1, 28, 28)\n",
        "\n",
        "\n",
        "# Clean Set\n",
        "# DataLoader: combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "train_loader = DataLoader(train_images_norm, batch_size=batch_size ,shuffle=False)\n",
        "# Splitting the test images (tot 10k) in valid and test set.\n",
        "valid_loader_tmp, test_loader_tmp = random_split(test_images_norm, [7000, 3000]) \n",
        "valid_loader = DataLoader(valid_loader_tmp, batch_size=batch_size , shuffle=False)\n",
        "test_loader = DataLoader(test_loader_tmp, batch_size=batch_size , shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LA0oRnKXoWav"
      },
      "outputs": [],
      "source": [
        "# Build Noisy Dataset\n",
        "\n",
        "batch_size = 128\n",
        "noise_variance = 0.2\n",
        "\n",
        "train_images_norm_noisy = train_images_norm + torch.randn_like(train_images_norm)*noise_variance\n",
        "test_images_norm_noisy = test_images_norm + torch.randn_like(test_images_norm)*noise_variance\n",
        "\n",
        "train_loader_noisy = DataLoader(train_images_norm_noisy, batch_size=batch_size ,shuffle=False)\n",
        "# Splitting the test images (tot 10k) in valid and test set.\n",
        "valid_loader_tmp, test_loader_tmp = random_split(test_images_norm_noisy, [7000, 3000]) \n",
        "valid_loader_noisy = DataLoader(valid_loader_tmp, batch_size=batch_size , shuffle=False)\n",
        "test_loader_noisy = DataLoader(test_loader_tmp, batch_size=batch_size , shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "JU8t8fsBR7Me",
        "outputId": "2a151b79-52ba-480a-9283-cac2fe8c46a1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = train_images_norm[0]\n",
        "plt.imshow(image[0, :, :], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "j-jB9PPPRh86",
        "outputId": "9d570733-e6f0-40dd-c1ce-064df8571daa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "image =train_images_norm_noisy[0]\n",
        "plt.imshow(image[0, :, :], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFj2SCq0fz9"
      },
      "source": [
        "### Using Class to define a Model (From TensorFlow Official Page)\n",
        "\n",
        "   \n",
        "```python\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX65cSON7CtZ"
      },
      "source": [
        "$\\frac{d f (g ( h (w)))}{dw} = \\frac{df}{dg(h)}\\frac{dg(h)}{dh}\\frac{d h(w)}{dw}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i776XcCtxdKe"
      },
      "source": [
        "### Defining our Variational Autoencoder Model\n",
        "<center>  <img src=https://drive.google.com/uc?id=14dNERtYBOiyEq3cPOUFObUe6Ul4cn_r0  \" width=\"800\">  </center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW_i8c3Xr8Un"
      },
      "source": [
        "Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point (as it happens for a standard autoencoder), we encode it as a distribution over the latent space. The model is then trained as follows:\n",
        "\n",
        "* first, the input is encoded as distribution over the latent space\n",
        "* second, a point from the latent space is sampled from that distribution\n",
        "* third, the sampled point is decoded and the reconstruction error can be computed\n",
        "* finally, the reconstruction error is backpropagated through the network\n",
        "\n",
        "In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control).\n",
        "\n",
        "Thus, the loss function that is minimised when training a VAE is composed of a \n",
        "\n",
        "* ***reconstruction term*** (on the final layer), that tends to make the encoding-decoding scheme as performant as possible;\n",
        "* ***regularisation term*** (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. Without this term, VAE will act like a basic autoencoder, which may lead to overfitting, and we won’t have the generative properties that we desire.\n",
        "\n",
        "The **Encoder** defines the *approximate posterior distribution* $q_\\theta(z|x)$ ($\\theta$ are its parameters), which takes as input an observation $x$ and outputs a set of parameters for specifying the conditional distribution of the latent representation $z$. In this example, simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance (is log for numerical stability) parameters of a factorized Gaussian.\n",
        "\n",
        "The **Decoder** defines the conditional distribution of the observation $p_ϕ(x|z)$ ($\\phi$ are its parameters), which takes a latent sample $z$ as input and outputs the parameters for a conditional distribution of the observation. In other words, it is the distribution of possible generated images, thus providing more sense the name \"variational\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CISgscLbjpxq"
      },
      "outputs": [],
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_channels: int, latent_dim: int):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim  # self is used when the object will be used in the methods\n",
        "\n",
        "\n",
        "        hidden_dims = [8, 16, 32]           # [128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        # 3 hidden layers of convolution\n",
        "        # It is not necessary to have a convolution with VAE, but it is efficient for images so we use it\n",
        "        conv2d_1 = nn.Sequential(\n",
        "                   nn.Conv2d(in_channels, out_channels=hidden_dims[0], kernel_size= 3, stride= 2, padding  = 1),\n",
        "                   nn.BatchNorm2d(hidden_dims[0]),\n",
        "                   nn.LeakyReLU()) #LeakyReLU does not go to zero, used to avoid vanishing gradient issues\n",
        "                   #This is a convolutional block\n",
        "        \n",
        "        conv2d_2 = nn.Sequential(\n",
        "                   nn.Conv2d(hidden_dims[0], out_channels=hidden_dims[1], kernel_size= 3, stride= 2, padding  = 1),\n",
        "                   nn.BatchNorm2d(hidden_dims[1]),\n",
        "                   nn.LeakyReLU())\n",
        "        \n",
        "        conv2d_3 = nn.Sequential(\n",
        "                   nn.Conv2d(hidden_dims[1], out_channels=hidden_dims[2], kernel_size= 3, stride= 2, padding  = 1),\n",
        "                   nn.BatchNorm2d(hidden_dims[2]),\n",
        "                   nn.LeakyReLU())\n",
        "        \n",
        "        modules = [conv2d_1, conv2d_2, conv2d_3]\n",
        "\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "\n",
        "        # 4*4 is the final dimension of images after the convolutions, with hidden_dims[-1] channels\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1] * 4 * 4, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dims[-1] * 4 * 4, latent_dim)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4 * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        first_output_padding = 0\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride = 2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=first_output_padding),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            first_output_padding = 1\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                                               hidden_dims[-1],\n",
        "                                               kernel_size=3,\n",
        "                                               stride=2,\n",
        "                                               padding=1,\n",
        "                                               output_padding=1),\n",
        "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "                            nn.LeakyReLU(),\n",
        "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
        "                                      kernel_size= 3, padding= 1),\n",
        "                            nn.Tanh())\n",
        "\n",
        "    def encode(self, input):\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes.\n",
        "        Input tensor to encoder [N x C x H x W]\n",
        "        \"\"\"\n",
        "        result = self.encoder(input)\n",
        "        #print(result.size())\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "        #print(result.size())\n",
        "\n",
        "        # Split the result into mu and var components\n",
        "        # of the latent Gaussian distribution\n",
        "        mu = self.fc_mu(result)\n",
        "        log_var = self.fc_var(result)\n",
        "\n",
        "        return [mu, log_var]\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Maps the given latent codes\n",
        "        onto the image space.\n",
        "        \"\"\"\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 32, 4, 4)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        result = torch.sigmoid(result) #not needed, we used tanh in final_layer\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) from\n",
        "        N(0,1).\n",
        "        mu: mean of the latent Gaussian\n",
        "        log_var: standard deviation of the latent Gaussian\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        # eps \\sim N(0,1)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, input):\n",
        "        mu, log_var = self.encode(input)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return  [self.decode(z), input, mu, log_var]\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples,\n",
        "               current_device):\n",
        "        \"\"\"\n",
        "        Samples from the latent space and return the corresponding\n",
        "        image space map.\n",
        "        num_samples: number of samples\n",
        "        current_device: device to run the model\n",
        "        \"\"\"\n",
        "        z = torch.randn(num_samples,\n",
        "                        self.latent_dim)\n",
        "\n",
        "        #z = z.to(current_device)\n",
        "\n",
        "        samples = self.decode(z)\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiQEdwCr0a_j"
      },
      "source": [
        "##### Reparametrization Trick\n",
        "\n",
        "The overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called ***reparametrisation trick***, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture. The idea is to approximate $z$ using the encoder outputs $\\mu$, $\\sigma$ and another parameter $ϵ$ as follows:\n",
        "\n",
        "$$z = \\mu(x) + \\sigma(x)\\epsilon .$$\n",
        "\n",
        "where $\\mu$ and $\\sigma$ represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the encoder output. The $\\epsilon$ can be thought of as a random noise used to maintain stochasticity of $z$. Generate $\\epsilon$ from a standard normal distribution.\n",
        "\n",
        "The latent variable $z$ is now generated by a function of $\\mu$, $\\sigma$ and $\\epsilon$, which would enable the model to backpropagate gradients in the encoder through $\\mu$ and $\\sigma$ respectively, while maintaining stochasticity through $\\epsilon$.\n",
        "\n",
        "\n",
        "[Here a more detailed explanation!](http://gregorygundersen.com/blog/2018/04/29/reparameterization/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar__cgFtrkTR",
        "outputId": "4033f658-3fc7-464a-edf5-79c6c9e832c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 32, 4, 4]            --\n",
            "|    └─Sequential: 2-1                   [-1, 8, 14, 14]           --\n",
            "|    |    └─Conv2d: 3-1                  [-1, 8, 14, 14]           80\n",
            "|    |    └─BatchNorm2d: 3-2             [-1, 8, 14, 14]           16\n",
            "|    |    └─LeakyReLU: 3-3               [-1, 8, 14, 14]           --\n",
            "|    └─Sequential: 2-2                   [-1, 16, 7, 7]            --\n",
            "|    |    └─Conv2d: 3-4                  [-1, 16, 7, 7]            1,168\n",
            "|    |    └─BatchNorm2d: 3-5             [-1, 16, 7, 7]            32\n",
            "|    |    └─LeakyReLU: 3-6               [-1, 16, 7, 7]            --\n",
            "|    └─Sequential: 2-3                   [-1, 32, 4, 4]            --\n",
            "|    |    └─Conv2d: 3-7                  [-1, 32, 4, 4]            4,640\n",
            "|    |    └─BatchNorm2d: 3-8             [-1, 32, 4, 4]            64\n",
            "|    |    └─LeakyReLU: 3-9               [-1, 32, 4, 4]            --\n",
            "├─Linear: 1-2                            [-1, 50]                  25,650\n",
            "├─Linear: 1-3                            [-1, 50]                  25,650\n",
            "├─Linear: 1-4                            [-1, 512]                 26,112\n",
            "├─Sequential: 1-5                        [-1, 8, 14, 14]           --\n",
            "|    └─Sequential: 2-4                   [-1, 16, 7, 7]            --\n",
            "|    |    └─ConvTranspose2d: 3-10        [-1, 16, 7, 7]            4,624\n",
            "|    |    └─BatchNorm2d: 3-11            [-1, 16, 7, 7]            32\n",
            "|    |    └─LeakyReLU: 3-12              [-1, 16, 7, 7]            --\n",
            "|    └─Sequential: 2-5                   [-1, 8, 14, 14]           --\n",
            "|    |    └─ConvTranspose2d: 3-13        [-1, 8, 14, 14]           1,160\n",
            "|    |    └─BatchNorm2d: 3-14            [-1, 8, 14, 14]           16\n",
            "|    |    └─LeakyReLU: 3-15              [-1, 8, 14, 14]           --\n",
            "├─Sequential: 1-6                        [-1, 1, 28, 28]           --\n",
            "|    └─ConvTranspose2d: 2-6              [-1, 8, 28, 28]           584\n",
            "|    └─BatchNorm2d: 2-7                  [-1, 8, 28, 28]           16\n",
            "|    └─LeakyReLU: 2-8                    [-1, 8, 28, 28]           --\n",
            "|    └─Conv2d: 2-9                       [-1, 1, 28, 28]           73\n",
            "|    └─Tanh: 2-10                        [-1, 1, 28, 28]           --\n",
            "==========================================================================================\n",
            "Total params: 89,917\n",
            "Trainable params: 89,917\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 1.20\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.19\n",
            "Params size (MB): 0.34\n",
            "Estimated Total Size (MB): 0.53\n",
            "==========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 32, 4, 4]            --\n",
              "|    └─Sequential: 2-1                   [-1, 8, 14, 14]           --\n",
              "|    |    └─Conv2d: 3-1                  [-1, 8, 14, 14]           80\n",
              "|    |    └─BatchNorm2d: 3-2             [-1, 8, 14, 14]           16\n",
              "|    |    └─LeakyReLU: 3-3               [-1, 8, 14, 14]           --\n",
              "|    └─Sequential: 2-2                   [-1, 16, 7, 7]            --\n",
              "|    |    └─Conv2d: 3-4                  [-1, 16, 7, 7]            1,168\n",
              "|    |    └─BatchNorm2d: 3-5             [-1, 16, 7, 7]            32\n",
              "|    |    └─LeakyReLU: 3-6               [-1, 16, 7, 7]            --\n",
              "|    └─Sequential: 2-3                   [-1, 32, 4, 4]            --\n",
              "|    |    └─Conv2d: 3-7                  [-1, 32, 4, 4]            4,640\n",
              "|    |    └─BatchNorm2d: 3-8             [-1, 32, 4, 4]            64\n",
              "|    |    └─LeakyReLU: 3-9               [-1, 32, 4, 4]            --\n",
              "├─Linear: 1-2                            [-1, 50]                  25,650\n",
              "├─Linear: 1-3                            [-1, 50]                  25,650\n",
              "├─Linear: 1-4                            [-1, 512]                 26,112\n",
              "├─Sequential: 1-5                        [-1, 8, 14, 14]           --\n",
              "|    └─Sequential: 2-4                   [-1, 16, 7, 7]            --\n",
              "|    |    └─ConvTranspose2d: 3-10        [-1, 16, 7, 7]            4,624\n",
              "|    |    └─BatchNorm2d: 3-11            [-1, 16, 7, 7]            32\n",
              "|    |    └─LeakyReLU: 3-12              [-1, 16, 7, 7]            --\n",
              "|    └─Sequential: 2-5                   [-1, 8, 14, 14]           --\n",
              "|    |    └─ConvTranspose2d: 3-13        [-1, 8, 14, 14]           1,160\n",
              "|    |    └─BatchNorm2d: 3-14            [-1, 8, 14, 14]           16\n",
              "|    |    └─LeakyReLU: 3-15              [-1, 8, 14, 14]           --\n",
              "├─Sequential: 1-6                        [-1, 1, 28, 28]           --\n",
              "|    └─ConvTranspose2d: 2-6              [-1, 8, 28, 28]           584\n",
              "|    └─BatchNorm2d: 2-7                  [-1, 8, 28, 28]           16\n",
              "|    └─LeakyReLU: 2-8                    [-1, 8, 28, 28]           --\n",
              "|    └─Conv2d: 2-9                       [-1, 1, 28, 28]           73\n",
              "|    └─Tanh: 2-10                        [-1, 1, 28, 28]           --\n",
              "==========================================================================================\n",
              "Total params: 89,917\n",
              "Trainable params: 89,917\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.20\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.19\n",
              "Params size (MB): 0.34\n",
              "Estimated Total Size (MB): 0.53\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = VarAutoEncoder(in_channels=1, latent_dim = 50)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#model.cuda() #model.to(device)\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA6194-1XLsy"
      },
      "source": [
        "### Gaussian Distribution and Loss Function\n",
        "\n",
        "The Gaussian distribution is defined by\n",
        "\n",
        "$$ N (x\\mid \\mu ,\\sigma ^{2})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}} .$$\n",
        "\n",
        "\n",
        "Taking its log we obtain\n",
        "\n",
        "$$ \\log N (x\\mid \\mu ,\\sigma ^{2})= - \\frac{1}{2} \\left( \\log2\\pi  + \\log \\sigma ^2  +  \\frac {(x-\\mu )^{2}}{\\sigma ^{2}}\\right) = - \\frac{1}{2} \\left( \\log2\\pi  + \\log\\sigma ^2 +  (x-\\mu )^{2} \\exp (- \\log\\sigma^2)\\right).$$\n",
        "\n",
        "Therefore the KL-Divergence is:\n",
        "\n",
        "$$\\sum_z N (z\\mid \\mu ,\\sigma ^{2})\\;(\\log N (x\\mid \\mu ,\\sigma ^{2})- \\log N(0 ,1)) = \\frac{1}{2}\\sum_z N (z\\mid \\mu ,\\sigma ^{2})\\;(1+\\log \\sigma^2 -\\mu^2 - \\sigma^2) $$\n",
        "\n",
        "The loss function of a VAE is\n",
        "\n",
        "$$l_i(\\theta, \\phi) = - \\mathbb{E}_{\\,z\\sim q_{\\theta}(z | x_i)} \\left[ \\log p_{\\phi}(x_i|z) \\right] + \\mathbb K \\mathbb L ( q_{\\theta}(x_i | z) \\, || \\, N(z) ),$$ \n",
        "\n",
        "where the first term is the reconstruction loss, or expected negative log-likelihood of the $i$-th datapoint, while the second is the KL-divergence. The former encourages the decoder to learn to reconstruct the data. The latter is a regularizer that forces the encoder’s distribution (which is by construction a Gaussian distribution with $N (z\\mid \\mu ,\\sigma ^{2})$) to be as much as possible similar to a Gaussian distribution. The final total loss function is $L =\\frac{1}{N} \\sum_i l_i$.\n",
        "\n",
        "  \n",
        "  <center>  <img src=https://drive.google.com/uc?id=1_UmDN6a2I2bktY4nVgT1N2uC-pFxoXo_ \" width=\"800\">  <center> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZKU0Gtnr9zza"
      },
      "outputs": [],
      "source": [
        "def loss_function(reconstructed_input, target, mu, log_var):\n",
        "    # BCE tries to make our reconstruction as accurate as possible\n",
        "    # KLD tries to push the distributions as close as possible to unit Gaussian\n",
        "    reconstruction_loss = F.binary_cross_entropy(reconstructed_input, target, reduction='sum')\n",
        "    kldiv_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "    loss = reconstruction_loss + kldiv_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww8drbMT1enI"
      },
      "source": [
        "### Defining Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "guH1kqSH44is"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VarAutoEncoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "  )\n",
              "  (fc_mu): Linear(in_features=512, out_features=50, bias=True)\n",
              "  (fc_var): Linear(in_features=512, out_features=50, bias=True)\n",
              "  (decoder_input): Linear(in_features=50, out_features=512, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Sequential(\n",
              "    (0): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 10\n",
        "latent_dim = 50\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model = VarAutoEncoder(in_channels=1, latent_dim = 50)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmw1ATxFuBA9"
      },
      "source": [
        "#### Normal Training (Reconstruct Input Images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fbqGfvsbhRC",
        "outputId": "65cf78ef-40a8-42fe-e48a-5a21c23886ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====> Epoch: 2 Average loss: 318.3994\n",
            "====> Epoch: 4 Average loss: 285.9268\n",
            "====> Epoch: 6 Average loss: 279.1782\n",
            "====> Epoch: 8 Average loss: 276.3413\n",
            "====> Epoch: 10 Average loss: 274.7927\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        input_images = data.to(device, dtype=torch.float)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, _, mu, log_var = model(input_images)\n",
        "        # input_images are the target!\n",
        "        loss = loss_function(recon_batch, input_images , mu, log_var)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 2 == 0:        \n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "            epoch, train_loss / len(train_loader.dataset)))\n",
        "        train_losses.append(train_loss / len(train_loader.dataset))\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    #evaluate(valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizmblSRuHCJ"
      },
      "source": [
        "#### Denoising Training (Clean Input Images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4yMUk4vtHFu",
        "outputId": "1f89bf7f-a7f2-4a18-c0cd-9e002f28dc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====> Epoch: 2 Average loss: 276.4789\n",
            "====> Epoch: 4 Average loss: 275.3659\n",
            "====> Epoch: 6 Average loss: 274.7926\n",
            "====> Epoch: 8 Average loss: 274.3970\n",
            "====> Epoch: 10 Average loss: 274.0983\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(zip(train_loader_noisy, train_loader)):\n",
        "        noisy_images = data[0].to(device, dtype=torch.float)\n",
        "        images = data[1].to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, _, mu, log_var = model(noisy_images)\n",
        "        # input_images are the target!\n",
        "        loss = loss_function(recon_batch, images, mu, log_var)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 2 == 0:        \n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "            epoch, train_loss / len(train_loader.dataset)))\n",
        "        train_losses.append(train_loss / len(train_loader.dataset))\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    #evaluate(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "mFO3S-AZ0L7q",
        "outputId": "0ac7fadd-0f69-4a0d-f1f2-a04550ac469b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def original_image(model, images):\n",
        "  \n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(images.shape[0]):\n",
        "      plt.subplot(2, 2, i+1)\n",
        "      plt.imshow(images[i, 0, :, :], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "images = test_images_norm_noisy[:4]\n",
        "#images = train_images_norm[:4]\n",
        "original_image(model, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "-hIAa0YA5KWf",
        "outputId": "9e101550-de87-4295-ddcb-eb058c0a1654"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10995/3459243677.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_input = torch.tensor(test_input)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def reconstructed_images(model, test_input):\n",
        "    test_input = torch.tensor(test_input)\n",
        "\n",
        "    test_input = test_input.to(device, dtype=torch.float)\n",
        "    rec_images, _, mu, log_var = model(test_input)\n",
        "    #z = model.reparameterize(mu, log_var)\n",
        "    #predictions = model.decode(z, apply_sigmoid=True)\n",
        "    #predictions = model.sample(test_input)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "    rec_images = rec_images.detach().cpu()\n",
        "    for i in range(rec_images.shape[0]):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(rec_images[i, 0, :, :], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "reconstructed_images(model, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "UqgCV4edrkYi",
        "outputId": "81fccb42-73b2-442e-9835-ed707891ba2b"
      },
      "outputs": [
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/matteo/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3259: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "\n",
        "for i in range(0,100, 4):\n",
        "  images = train_images_norm[i:4+i]\n",
        "  #print(image.shape)\n",
        "  original_image(model, images)\n",
        "  reconstructed_images(model, images)\n",
        "  time.sleep(5)\n",
        "  display.clear_output(wait=False)\n",
        "  cnt += 1\n",
        "  if cnt > 10:\n",
        "    sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvhKbr4r6Zz"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "0. Review the full code;\n",
        "1. Define and compute evaluation function evaluate(valid_loader);\n",
        "2. Train the model with different latent_dim = 5, 20, 50, 100\n",
        "3. Modify the code and built a Denoising VAE: \n",
        "\n",
        "    noisy_image -> VAE -> cleaned_image  \n",
        "\n",
        "    noisy_image = image + 0.2 * np.random.randn(shape(image))\n",
        "\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "5_denoising_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
